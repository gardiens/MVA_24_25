{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIrYdl_vUEol"
   },
   "source": [
    "---\n",
    "# <center> Visualization of CNN: Grad-CAM\n",
    "\n",
    "<center> Eya Ghamgui $~~$ eya.ghamgui@telecom-paris.fr\n",
    "<center> Siwar Mhadhbi $~~$ siwar.mhadhbi@telecom-paris.fr\n",
    "<center> Saifeddine Barkia $~~$ saifeddine.barkia@telecom-paris.fr\n",
    "<center> February 02, 2022\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZVV1iCSA14w"
   },
   "source": [
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
    "\n",
    "* **To be submitted within 2 weeks**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xucgOdxkA14z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "import pickle\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from cv2 import cv2\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bh-VnEtlA140"
   },
   "source": [
    "### Download the Model\n",
    "We provide you a pretrained model `ResNet-34` for `ImageNet` classification dataset.\n",
    "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
    "* **ResNet-34**: A deep architecture for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5FlmaqJA141"
   },
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(pretrained=True)\n",
    "resnet34.eval()  # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4xXo4DuA142"
   },
   "outputs": [],
   "source": [
    "classes = pickle.load(\n",
    "    urllib.request.urlopen(\n",
    "        \"https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oow0ytTAA141"
   },
   "source": [
    "![ResNet34](https://miro.medium.com/max/1050/1*Y-u7dH4WC-dXyn9jOG4w0w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iW0Bqo5HA142"
   },
   "source": [
    "### Input Images\n",
    "We provide you 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n",
    "In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfUnf1pvA143"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(dir_path):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    dataset = datasets.ImageFolder(\n",
    "        dir_path,\n",
    "        transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),  # resize the image to 224x224\n",
    "                transforms.ToTensor(),  # convert numpy.array to tensor\n",
    "                normalize,\n",
    "            ]\n",
    "        ),\n",
    "    )  # normalize the tensor\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uF554mXGA143"
   },
   "outputs": [],
   "source": [
    "# The images should be in a *sub*-folder of \"data/\" (ex: data/TP2_images/images.jpg) and *not* directly in \"data/\"!\n",
    "# otherwise the function won't find them\n",
    "\n",
    "import os\n",
    "\n",
    "os.mkdir(\"data\")\n",
    "os.mkdir(\"data/TP2_images\")\n",
    "!cd data/TP2_images && wget \"https://www.lri.fr/~gcharpia/deeppractice/2022/TP2/TP2_images.zip\" && unzip TP2_images.zip\n",
    "dir_path = \"data/\"\n",
    "dataset = preprocess_image(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "oC4g6z2uA144",
    "outputId": "2d7de3b6-dec2-4427-b954-6b6ca72402e6"
   },
   "outputs": [],
   "source": [
    "# show the orignal image\n",
    "index = 5\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert(\"RGB\")\n",
    "plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwlunQXVA144",
    "outputId": "3fe9b872-339b-4508-e475-c93b24770123"
   },
   "outputs": [],
   "source": [
    "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
    "values, indices = torch.topk(output, 3)\n",
    "print(\"Top 3-classes:\", indices[0].numpy(), [classes[x] for x in indices[0].numpy()])\n",
    "print(\"Raw class scores:\", values[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXRmT-YnA145"
   },
   "source": [
    "### Grad-CAM \n",
    "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "\n",
    "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude. \n",
    "\n",
    "\n",
    "* **Hints**: \n",
    " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully. \n",
    " + The pretrained model resnet34 doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly. \n",
    " + The size of feature maps is 7x7, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n",
    " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7y8aXNYA146"
   },
   "source": [
    "![Grad-CAM](https://da2so.github.io/assets/post_img/2020-08-10-GradCAM/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qqkh-4k7A146"
   },
   "outputs": [],
   "source": [
    "################\n",
    "## Grad Cam\n",
    "################\n",
    "def Grad_Cam(image, category):\n",
    "    # Useful functions to extract gradients and features\n",
    "    def _backward_hook(model, grad_input, grad_output):\n",
    "        gradients.append(grad_output[0])\n",
    "\n",
    "    def _forward_hook(model, input, output):\n",
    "        features.append(output.data)\n",
    "\n",
    "    features = []\n",
    "    gradients = []\n",
    "\n",
    "    # Hooks for the gradients and features\n",
    "    hook1 = model.layer4[2].bn2.register_backward_hook(_backward_hook)\n",
    "    hook2 = model.layer4[2].bn2.register_forward_hook(_forward_hook)\n",
    "\n",
    "    # Extract last predicted layer\n",
    "    output = model(image)\n",
    "\n",
    "    # Create signal\n",
    "    signal = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "    signal[0][category] = 1\n",
    "    signal = torch.from_numpy(signal).requires_grad_(True)\n",
    "    signal = torch.sum(signal * output)\n",
    "\n",
    "    # Backpropagate signal\n",
    "    model.zero_grad()\n",
    "    signal.backward(retain_graph=True)\n",
    "\n",
    "    # Extract gradients and features\n",
    "    gradients = gradients[0][-1].numpy()\n",
    "    features = features[0][-1].numpy()\n",
    "\n",
    "    # Compute weights using gradients\n",
    "    Weights = np.mean(gradients, axis=(1, 2))\n",
    "\n",
    "    # Initiate a heatmap\n",
    "    heatmap = np.zeros(features.shape[1:])\n",
    "\n",
    "    # Remove Hooks\n",
    "    hook1.remove()\n",
    "    hook2.remove()\n",
    "\n",
    "    # Compute heatmap\n",
    "    for i in range(Weights.shape[0]):\n",
    "        heatmap += Weights[i] * features[i, :, :]\n",
    "\n",
    "    # ReLU on top of the heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "    # Interpolate values\n",
    "    heatmap = torch.from_numpy(heatmap.reshape(1, 1, 7, 7))\n",
    "    heatmap = F.interpolate(heatmap, scale_factor=32, mode=\"bilinear\")\n",
    "    heatmap = heatmap.numpy()[0, 0, :, :]\n",
    "\n",
    "    # Normalize the heatmap\n",
    "    heatmap = heatmap / np.max(heatmap)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "################\n",
    "# Combine an image with its heatmap\n",
    "################\n",
    "def apply_heatmap(img, map):\n",
    "    # Construct a map\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * map), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "    # Merge the image with its map\n",
    "    merged_image = cv2.addWeighted(heatmap, 0.5, img, 0.5, 0.0)\n",
    "    merged_image = np.uint8(255 * merged_image[:, :, ::-1])\n",
    "\n",
    "    return merged_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LgKWRwaKNGkC",
    "outputId": "a92bfa44-a29e-454e-d764-7157f14a2018"
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    img = np.array(Image.open(dataset.imgs[i][0]).convert(\"RGB\"))\n",
    "    img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
    "    input = dataset[i][0].view(1, 3, 224, 224)\n",
    "\n",
    "    model = models.resnet34(pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    output = model(input)\n",
    "    values, indices = torch.topk(output, 3)\n",
    "\n",
    "    f, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(\"Sample \" + str(i + 1))\n",
    "    for j in range(1, 4):\n",
    "        category = indices[0].numpy()[j - 1]\n",
    "        heatmap = Grad_Cam(input, category)\n",
    "        merged_img = apply_heatmap(img, heatmap)\n",
    "        ax[j].imshow(merged_img)\n",
    "        names = classes[category].split(\",\")\n",
    "        ax[j].set_title(names[0])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96FWgHwTStwG"
   },
   "source": [
    "**Comment:** \n",
    "\n",
    "In the paper [Grad-CAM 2019, Selvaraju et al.], the authors applied this method on the last convolutional layer. Using the ResNet34 architecture, we interpreted the last convolutional block as the last convolutional layer and we applied this method on the last BatchNormalization layer. Indeed, by comparing the ploted heatmaps applied on the last bn2 layer and the last conv2 layer, we found that the results of the bn2 layer are much better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bvDHxyPs1Kp"
   },
   "source": [
    "**Interpretations:**\n",
    "\n",
    "* From the previous results, we can notice that almost in all samples, the first predictions, associated with the first heatmaps, seem to be more correct and relevant to the original image. However, in some particular cases, second or third prediction outweigh the first one.\n",
    "\n",
    "In fact, more precisely,\n",
    "\n",
    "* In the case of samples 1, (3, 5), 6, (9, 18), (12, 7), 15, and (17, 20), all three heatmaps show that the network sees almost exactly the same region for the prediction. For each aforementioned samples, the predicted animals belong to the same family of mammals. That is, they share many of the same characteristics. They indeed belong to the same family of elephants, dogs, cats, mustelids, foxes, felines and monkeys respectively.\n",
    "* In the case of sample 2, even though the network sees almost exactly the same region in the three heatmaps, it predicts “porcupine” and “marmoset” which are relevant predictions for the original image, but “sloth bear” is way different which explains its lower score.\n",
    "* In the case of sample 4, observing the three heatmaps, the network doesn’t focus on the same regions for the different predictions. To predict “Norwegian elkhound” or “German shepherd”, the network focuses on the front region. However, to predict “Cardigan”, it focuses on the back region of the animal.\n",
    "* In the case of sample 8, it is interesting to note that the heatmaps reveal that the first and second predictions are based on the dog in the foreground, while the third prediction is based on the dog in the background.\n",
    "* In the case of sample 10, the first heatmap reveals that the network focuses primarily on the horns, which is why it detects \"ibex\", while in the second and especially the third heatmap, the network sees both the horns and the body, allowing it to detect sheep breeds rather than goats.\n",
    "\n",
    "Few failure prediction noticed:\n",
    "\n",
    "* In the case of sample 11, the network correctly predicts a horse when it focuses on the entire region of the animal. However, it gives incorrect predictions such as ox or dog breeds when it focuses only on the neck region.\n",
    "* In the case of sample 13, the network fails to detect the animal correctly, which is also noticeable through the heatmaps, in all examples the network focuses more on the border regions outside the animals region.\n",
    "* In the case of sample 14, first and second heatmaps reveal that the network sees almost exactly the same region in both cases, and give relevant predictions. However, the third heatmap reveals that the network sees the whole body but wrongly predicts the animal species.\n",
    "* In the case of sample 16, it is interesting to note that when the network sees the entire animal region, it correctly detects a “sea lion”, but when it sees only a portion of the animal region, it directs the detection to unrelated objects, such as a cowboy boot or a balance beam.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzXfSN7LI0hT"
   },
   "source": [
    "**Conclusion:**\n",
    "\n",
    "* To conclude, we can say that Grad_Cam helps us to better understand what is going on inside the network and what it sees for prediction. It also helps us understand the differences in prediction and how they are related to specific parts seen by the network in the input image.\n",
    "* Specifically in our task, Grad_Cam helped us understand how the network differentiates between animal species and even breeds within the same family based on their characteristics that the network focuses on during prediction. \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TP2_GradCAM__Ghamgui_Mhadhbi_Barkia.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
