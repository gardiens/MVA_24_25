{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gardiens/MVA_24_25/blob/main/MVA_SAT/TP7/TP_triangulation_without_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Triangulation and Digital Elevation Models\n",
        "\n",
        "The objective of this practical session is to study the creation and processing of 3D\n",
        "models computed from satellite images.  The extraction of 3D points from\n",
        "image correspondences is called *triangulation*.  The processing of generic\n",
        "3D point clouds is a very general problem, out of the scope of the present\n",
        "course. Here, we will content ourselves with the much simpler 2.5D models\n",
        "that, in the context of geographic imaging, are called *digital elevation\n",
        "models* (D.E.M.).\n",
        "\n",
        "We cover the following topics:\n",
        "* How to find the vertical direction of an image\n",
        "* How to obtain a 3D point from a match between two images\n",
        "* How the precision of the point varies according to the baseline\n",
        "* Computation of a dense point cloud\n",
        "* Computation of a D.E.M. from a point cloud\n",
        "* Visualization, ~~filtering, interpolation, registration~~ and fusion of D.E.M.\n",
        "\n",
        "\n",
        "#### Instructions\n",
        "To solve this practical session, answer the questions below. Then, clear all the output cells using the menu option **Cell->All Output->Clear** and export the notebook with your answers using the menu option **File -> Download as -> Notebook (.ipynb)**. Then [submit the resulting file here](https://forms.gle/k6FSX2BrPix823dx6) by next week. You will receive an automatic acknowledgement of receipt.\n",
        "\n",
        "There are 4 compulsory exercises, plus 3 optional ones:\n",
        "* Exercise 1. Find the vertical direction of an image _(warm-up exercise)_\n",
        "* Exercise 2. Affine triangulation of a match _(head-scratching, requires quite a bit of hand computation)_\n",
        "* Exercise 3. Qualitative evaluation of high vs. low-baseline results _(only conceptual)_\n",
        "* Exercise 4. (given) Obtain a dense point cloud by triangulating a disparity map _(plug existing functions)_\n",
        "* Exercise 5. (given) Projection of a 3D point cloud into a DEM _(easy to do, hard to get 100% right)_\n",
        "* Exercise 6. Merge several DEM into a single one _(easy)_\n",
        "* Exercise 7. (optional) Create a 3D point cloud from a DEM _(easy for pythonists)_\n",
        "* Exercise 8. (optional) Create a complete 3D point cloud _(difficult)_\n",
        "\n",
        "**Note:** A correct solution of exercise 2 is _essential_ because all the following exercises depend on it.\n",
        "\n",
        "<!--\n",
        "## Overview of notations\n",
        "\n",
        "$(x,y)$, $(i,j)$ pixel coordinates in the domain of an image\n",
        "\n",
        "$(\\lambda,\\theta,h)$ latitude, longitude, height of a 3D point\n",
        "\n",
        "$(e,n,h)$ easting, northing, height of a 3D point (the UTM zone is implicit)\n",
        "\n",
        "$A, B, \\ldots$ gray-level images\n",
        "\n",
        "$A(x,y)$ pixel value at coordinates $(x,y)$ of image $A$\n",
        "\n",
        "$P_A(\\lambda,\\theta,h), L_A(x,y,h)$ projection and localization functions\n",
        "of image $A$\n",
        "\n",
        "$u, v, \\ldots$ raster images representing digital elevation models in\n",
        "meters\n",
        "\n",
        "$u(i,j)$ value of $u$ at the pixel $(i,j)$\n",
        "\n",
        "$u(e,n)$ height of the point at geographic coordinates $(e,n)$\n",
        "\n",
        "$u(\\lambda,\\theta)$ height of the point at geographic coordinates\n",
        "$(\\lambda,\\theta)$\n",
        "-->"
      ],
      "metadata": {
        "id": "5oDXzHqnyZ1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup code for the notebook\n",
        "\n",
        "# Execute code 'cells' like this by clicking on the 'Run'\n",
        "# button or by pressing [shift] + [Enter].\n",
        "\n",
        "# This cell only imports some python packages that will be\n",
        "# used below. It doesn't generate any output.\n",
        "\n",
        "# The following lines install the necessary packages in the colab environment\n",
        "try:\n",
        "    from google.colab import files\n",
        "\n",
        "    # download TP data and tools\n",
        "    !wget -q http://boucantrin.ovh.hw.ipol.im/static/facciolo/mvaisat/tp4.zip\n",
        "    !unzip -q -o tp4.zip\n",
        "\n",
        "    # install dependencies\n",
        "    !python -m pip -q install rpcm\n",
        "    !pip install -q 'ad @ git+https://github.com/DapengFeng/ad'\n",
        "    !python -m pip -q install numpy matplotlib scipy geojson pyproj opencv-contrib-python==4.8.0.76 rasterio srtm4 folium numba pypotree\n",
        "\n",
        "except ImportError:\n",
        "    %matplotlib notebook\n",
        "    pass\n",
        "\n",
        "\n",
        "## Setup code for the notebook\n",
        "##\n",
        "# Autoreload external python modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "\n",
        "# general imports\n",
        "import numpy as np                   # numeric linear algebra\n",
        "from scipy import ndimage            # only for ndimage.affine_transform\n",
        "import matplotlib.pyplot as plt      # plotting\n",
        "import rpcm                          # RPC model\n",
        "\n",
        "# imports specific to this course\n",
        "import utils          # IO and conversion tools (from TP-collection)\n",
        "import vistools       # display tools (from TP-collection)\n",
        "import rectification  # rectification tools (from TP-rectification)\n",
        "import stereo         # stereo matching tools (from TP-stereo)\n",
        "import folium\n",
        "\n",
        "# display hacks\n",
        "np.set_printoptions(linewidth=80)"
      ],
      "outputs": [],
      "metadata": {
        "id": "L4ijNChSyZ1o"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Tokyo dataset\n",
        "\n",
        "For this session we use the Tokyo series of 23 Pl√©iades images, acquired\n",
        "during a single orbit.  Notice that the satellite has to rotate very fast\n",
        "and very accurately to point the camera towards the city as it flies all over\n",
        "it in a few seconds."
      ],
      "metadata": {
        "id": "h_yo43QDyZ1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list the tiff images available in the remote folder\n",
        "myimages = utils.find('http://menthe.ovh.hw.ipol.im/IARPA_data/tokyo_20130103/', 'TIF')\n",
        "print(f\"Found {len(myimages)} images\")\n",
        "myimages[0:4]"
      ],
      "outputs": [],
      "metadata": {
        "id": "DycHPNVEyZ1q"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# keep only the panchromatic (P) images, discard the multispectral (MS)\n",
        "myimages = [x for x in myimages if \"_P_\" in x]\n",
        "myimages[0:4]"
      ],
      "outputs": [],
      "metadata": {
        "id": "54vhZnPByZ1q"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the exercises below we propose to use a fixed area of interest around the Skytree tower.  Optionally, you can select a different scene of your choice, using the `foliummap` function from the `vistools` module."
      ],
      "metadata": {
        "id": "KkSTtEseyZ1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a map widget\n",
        "m = vistools.foliummap(zoom_start=10)\n",
        "\n",
        "# display the footprint polygons of each image on this map\n",
        "for k in myimages[0:4]:\n",
        "    footprint = utils.lon_lat_image_footprint(k)\n",
        "    folium.GeoJson(footprint).add_to(m)\n",
        "\n",
        "# center the map on the center of the last footprint\n",
        "m.location = np.mean(footprint['coordinates'][0][:4], axis=0).tolist()[::-1]\n",
        "display(m)"
      ],
      "outputs": [],
      "metadata": {
        "id": "vkro19_VyZ1r"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the vertical direction"
      ],
      "metadata": {
        "id": "RWRE3AdDyZ1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first question is just for *warming up* using the RPC and image extraction\n",
        "functions from the previous sessions.\n",
        "\n",
        "**Exercise 1.** Implement the 'crop_vertical' function.  This function\n",
        "should extract the requested *Area of interest* (AOI) from an image and rotate it so that the vertical direction in space points upwards in the rotated image domain.  You\n",
        "can compute the vertical direction by evaluating the RPC functions on 2\n",
        "points at different heights.  The structure of this function is already given, you only need to complete the 'find_vertical_direction' function.\n",
        "\n",
        "Apply your function to an area around Skytree Tower (coordinates $35.710139, 139.810833$) or Tokyo Tower (coordinates $35.658611, 139.745556$) to verify that the building is indeed vertical."
      ],
      "metadata": {
        "id": "Sy9qB_SkyZ1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coordinates around Sky Tree's base (the second tallest building in the world)\n",
        "aoi_skytree = {'type': \"Polygon\", 'coordinates': [[\n",
        "    [139.808, 35.7119],\n",
        "    [139.807, 35.7078],\n",
        "    [139.815, 35.7078],\n",
        "    [139.815, 35.7123],\n",
        "    [139.808, 35.7119]\n",
        "]]\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "eREglYN3yZ1r"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# crop the region of interest on the first image\n",
        "crop, offset_x, offset_y = utils.crop_aoi(myimages[0], aoi_skytree)\n",
        "rpc = rpcm.rpc_from_geotiff(myimages[0])"
      ],
      "outputs": [],
      "metadata": {
        "id": "TvERQ2TZyZ1s"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# display the obtained crop\n",
        "# Notice that the image is not well oriented and is difficult to interpret\n",
        "vistools.display_image(crop)\n",
        "vistools.display_image(crop/8)\n",
        "vistools.display_image(utils.simplest_color_balance_8bit(crop))"
      ],
      "outputs": [],
      "metadata": {
        "id": "vLbUt6rGyZ1s"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# code for Exercise 1\n",
        "def find_vertical_direction(rpc):\n",
        "    \"\"\"\n",
        "    Return the vertical direction associated to an image.\n",
        "\n",
        "    Arguments:\n",
        "        rpc: RPC function of the image\n",
        "\n",
        "    Return:\n",
        "        p, q : the vertical direction in the image domain (a normalized 2D vector)\n",
        "    \"\"\"\n",
        "\n",
        "    # EXERCISE 1 : WRITE THE CODE FOR THIS FUNCTION\n",
        "    return -0.2, 0.98\n",
        "\n",
        "\n",
        "# build a rotation matrix that sets the 3D vertical direction upwards\n",
        "def build_verticalizing_rotation(rpc, shape):\n",
        "    p, q = find_vertical_direction(rpc)                   # cosine and sine\n",
        "    p, q = -q, p                                          # ndimage convention for rows/cols\n",
        "    x, y = shape[1]/2, shape[0]/2                         # center of rotation (middle of the image domain)\n",
        "    T = np.array([[ 1,  0, -x], [0,  1, -y], [0, 0, 1]])  # translate (x,y) to the origin\n",
        "    R = np.array([[ p, q,  0], [-q,  p,  0], [0, 0, 1]])  # rotate by the requested angle\n",
        "    R = np.linalg.inv(T) @ R @ T                          # full rotation matrix\n",
        "    return R\n",
        "\n",
        "# like crop_aoi, but rotates the image after cropping\n",
        "def crop_vertical(image, aoi, base_h=0):\n",
        "    rpc = rpcm.rpc_from_geotiff(image)\n",
        "    x, y, w, h = utils.bounding_box_of_projected_aoi(rpc, aoi, z=base_h)\n",
        "    R = build_verticalizing_rotation(rpc, (h, w))\n",
        "    R = np.dot(R, utils.matrix_translation(-x, -y))  # compensate for the crop offset\n",
        "    return rectification.affine_crop(image, R, w, h)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bPTs2TA6yZ1s"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the crop of an image with and without the verticalizing rotation\n",
        "base_h = 45\n",
        "a, _, _ = utils.crop_aoi(myimages[0], aoi_skytree, base_h)\n",
        "b    = crop_vertical(myimages[0], aoi_skytree, base_h)\n",
        "\n",
        "qa = utils.simplest_color_balance_8bit(a)\n",
        "qb = utils.simplest_color_balance_8bit(b)\n",
        "vistools.display_gallery([qa, qb])"
      ],
      "outputs": [],
      "metadata": {
        "id": "XHKQ7xGkyZ1s"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is used to test your implementation of vertical crop.  You should obtain a timeseries of images such that the vertical direction always points upwards.  **Note:** to evaluate your answer to Question 1 we will run the code below and check visually if the sequence of vertical crops is, indeed, vertical."
      ],
      "metadata": {
        "id": "hzqIphxAyZ1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# auxiliary function with the same interface as crop_vertical\n",
        "def crop_rectangular(image, aoi, base_h=0):\n",
        "    a, _, _ = utils.crop_aoi(image, aoi, base_h)\n",
        "    return a"
      ],
      "outputs": [],
      "metadata": {
        "id": "UZeIyChByZ1s"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# build a timeseries of crops (without rotation)\n",
        "crops = [crop_rectangular(x, aoi_skytree, base_h) for x in myimages]"
      ],
      "outputs": [],
      "metadata": {
        "id": "uLgLcSXAyZ1s"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# build a timeseries of crops (rotated in the vertical direction)\n",
        "vcrops = [crop_vertical(x, aoi_skytree, base_h) for x in myimages]"
      ],
      "outputs": [],
      "metadata": {
        "id": "C6nd4xpzyZ1t"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# quantize the crops to 8 bits\n",
        "q_crops  = [utils.simplest_color_balance_8bit(x) for x in crops]\n",
        "q_vcrops = [utils.simplest_color_balance_8bit(x) for x in vcrops]"
      ],
      "outputs": [],
      "metadata": {
        "id": "i16aRvtSyZ1t"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# display the original images\n",
        "vistools.display_gallery(q_crops)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Gv60Aj0cyZ1t"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# display the rotated images\n",
        "vistools.display_gallery(q_vcrops)"
      ],
      "outputs": [],
      "metadata": {
        "id": "CIq7hidyyZ1t"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Triangulation of a point\n",
        "\n",
        "Triangulation consists in finding the position of a 3D point from its projection into two images.  It is also called *intersection* because it can be interpreted as the intersection of two rays in space.  If $x$ is a point in the domain of image $A$ and $x'$ is a point in the domain of image $A'$, the intersection is found by solving the following equation for $h$ : $L_A(x,h)=L_{A'}(x',h)$.  Then, the 3D point of intersection is either $L_A(x,h)$ or $L_{A'}(x',h)$.  This is a system of two equations with a single unknown $h$, so in general it will not have a solution.  In that case, we can define the \"solution\" by the value of $h$ that minimizes, for example, the error $e(h)=\\|L_A(x,h)-L_{A'}(x',h)\\|^2$.  Then, we obtain the 3D point by evaluating the localization function $L_A$.  Thus, the solution is a 3D point that, when projected into image $A$, it falls exactly on $x$, but when projected on image $A'$ it may be a bit far from $x'$.  Note that there are other reasonable energies to minimize, leading to (slightly) different results.  For example $e(h)=\\|P_{A'}(L_A(x,h),h)-x'\\|^2$."
      ],
      "metadata": {
        "id": "DTf2dCSVyZ1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2** Implement the `triangulation_affine` function that finds the position of a 3D point given a correspondence between two images.  This function should be reasonably fast, because you will want to apply it to all the pixels of an image to obtain a dense 3D cloud.  We give you a slow, naive implementation based on iterative minimization of the error. You should write a fast implementation based on the affine approximation of the RPC functions developed in the session on rectification. Notice that in this case the triangulation function has a closed, linear form. Verify that your implementation gives the same results as triangulation_iterative (up to a few centimeters).\n",
        "\n",
        "**Note:** Solving this exercise is essential for the rest of the notebook!"
      ],
      "metadata": {
        "id": "oU7kJu6fyZ1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the epipolar line between two images at a value of h\n",
        "def epipolar_correspondence(rpc_A, rpc_B, x, y, h):\n",
        "    lon, lat = rpc_A.localization(x, y, h)\n",
        "    return rpc_B.projection(lon, lat, h)"
      ],
      "outputs": [],
      "metadata": {
        "id": "kZNiOnhkyZ1t"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# slowish implementation of the triangulation, based on iterative approximation\n",
        "def triangulation_iterative(rpc1, rpc2, x1, y1, x2, y2):\n",
        "    \"\"\"\n",
        "    Triangulate a match between two images.\n",
        "\n",
        "    Arguments:\n",
        "        rpc1, rpc2: calibration data for each image\n",
        "        x1, y1: pixel coordinates in the domain of the first image\n",
        "        x2, y2: pixel coordinates in the domain of the second image\n",
        "\n",
        "    Return value: a 4-tuple (lon, lat, h, e)\n",
        "        lon, lat, h, e: coordinates of the triangulated point, reprojection error\n",
        "    \"\"\"\n",
        "    # initial guess for h\n",
        "    h = rpc1.alt_offset\n",
        "    hstep = 1\n",
        "    err = 0\n",
        "\n",
        "    # iteratively improve h to minimize the error\n",
        "    for i in range(10):\n",
        "        # two points on the epipolar curve of (x1, y1)\n",
        "        # are used to approximate it by a straight line\n",
        "        px, py = epipolar_correspondence(rpc1, rpc2, x1, y1, h)\n",
        "        qx, qy = epipolar_correspondence(rpc1, rpc2, x1, y1, h + hstep)\n",
        "\n",
        "        # displacement vectors between these two points and with the target\n",
        "        ax, ay = qx-px, qy-py\n",
        "        bx, by = x2-px, y2-py\n",
        "\n",
        "        # projection of the target into the straight line\n",
        "        l = (ax*bx + ay*by) / (ax*ax + ay*ay)\n",
        "        rx, ry = px+l*ax, py+l*ay\n",
        "\n",
        "        # error of this projection\n",
        "        err = np.hypot(rx - x2, ry - y2)\n",
        "\n",
        "        # new value for h\n",
        "        h = h + l * hstep\n",
        "\n",
        "        # stop if l becomes too small (max 2 or 3 iterations are performed in practice)\n",
        "        if np.all(np.fabs(l) < 1e-3):\n",
        "            break\n",
        "\n",
        "    lon, lat = rpc1.localization(x1, y1, h)\n",
        "    return lon, lat, h, err"
      ],
      "outputs": [],
      "metadata": {
        "id": "ccGoAWn_yZ1t"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# fast implementation of triangulation, based on linearized projection functions\n",
        "def triangulation_affine(PA, PB, x1, y1, x2, y2):\n",
        "    \"\"\"\n",
        "    Triangulate a (list of) match(es) between two images of affine cameras.\n",
        "\n",
        "    Arguments:\n",
        "        PA, PB : affine (projection) camera matrices of the two images\n",
        "        x1, y1 : pixel coordinates in the domain of the first image\n",
        "        x2, y2 : pixel coordinates in the domain of the second image\n",
        "\n",
        "    Return value: a 4-tuple (lon, lat, h, e)\n",
        "        lon, lat, h, e : coordinates of the triangulated point(s), reprojection error\n",
        "    \"\"\"\n",
        "\n",
        "    # EXERCICE 2 : WRITE THE CODE FOR THIS FUNCTION\n",
        "    return lon, lat, h, e\n",
        "\n",
        "# compute the affine approximations and triangulate a list of points\n",
        "def triangulation_affine_rpc(rpc1, rpc2, x1, y1, x2, y2, base_lon, base_lat, base_h):\n",
        "    P_A = rectification.rpc_affine_approximation(rpc1, (base_lon, base_lat, base_h))\n",
        "    P_B = rectification.rpc_affine_approximation(rpc2, (base_lon, base_lat, base_h))\n",
        "    return triangulation_affine(P_A, P_B, x1, y1, x2, y2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BXoblxg-yZ1t"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the triangulation function, we can produce points in 3D space.  Let us compute the height of the Skytree tower.  https://en.wikipedia.org/wiki/Tokyo_Skytree"
      ],
      "metadata": {
        "id": "3K5ETq1eyZ1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the rpcs of all images\n",
        "myrpcs = [rpcm.rpc_from_geotiff(x) for x in myimages]"
      ],
      "outputs": [],
      "metadata": {
        "id": "t5vI1Ip8yZ1u"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# check whether the localization function gives reasonable results\n",
        "myrpcs[3].localization(1000, 1000, 10)"
      ],
      "outputs": [],
      "metadata": {
        "id": "GtzriwXjyZ1u"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# let us work with images 9 and 14\n",
        "idx_a = 9\n",
        "idx_b = 14"
      ],
      "outputs": [],
      "metadata": {
        "id": "AL9Dk3ryyZ1u"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# extract a crop of each image and SAVE THE CROP OFFSETS FOR LATER\n",
        "crop_a, offx_a, offy_a = utils.crop_aoi(myimages[idx_a], aoi_skytree)\n",
        "crop_b, offx_b, offy_b = utils.crop_aoi(myimages[idx_b], aoi_skytree)\n",
        "print(f\"x0_a, y0_a = {offx_a}, {offy_a}\")\n",
        "print(f\"x0_b, y0_b = {offx_b}, {offy_b}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "ohUGbSLHyZ1u"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# show the two extracted crops\n",
        "vistools.display_gallery([utils.simplest_color_balance_8bit(x) for x in [crop_a, crop_b]])"
      ],
      "outputs": [],
      "metadata": {
        "id": "UDi94d5GyZ1u"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# display the two crops, and a manually selected point\n",
        "p = [807, 861]\n",
        "q = [944, 38]\n",
        "_, f = plt.subplots(2, 1, figsize=(7,9))\n",
        "f[0].imshow(utils.simplest_color_balance_8bit(crop_a, 0.1), cmap=\"gray\")\n",
        "f[1].imshow(utils.simplest_color_balance_8bit(crop_b, 0.1), cmap=\"gray\")\n",
        "f[0].plot(*p, \"ro\")\n",
        "f[1].plot(*q, \"ro\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZHAeL0zSyZ1u"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# triangulate this single match to find (longitude, latitude, height, error_in_pixels)\n",
        "x = triangulation_iterative(myrpcs[idx_a], myrpcs[idx_b],\n",
        "                            p[0] + offx_a, p[1] + offy_a, q[0] + offx_b, q[1] + offy_b)\n",
        "x"
      ],
      "outputs": [],
      "metadata": {
        "id": "MBMmgveoyZ1u"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# observe that the iterative triangulation is rather slow\n",
        "for i in range(100):\n",
        "    triangulation_iterative(myrpcs[idx_a], myrpcs[idx_b],\n",
        "                            p[0] + offx_a, p[1] + offy_a, q[0] + offx_b, q[1] + offy_b)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Y7X5mNucyZ1u"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# choose a base point for the affine approximation\n",
        "base_of_tower = [x[0], x[1], 40]"
      ],
      "outputs": [],
      "metadata": {
        "id": "iL66-rsGyZ1v"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# triangulate this match using the affine approximation\n",
        "triangulation_affine_rpc(myrpcs[idx_a], myrpcs[idx_b],\n",
        "                         p[0] + offx_a, p[1] + offy_a, q[0] + offx_b, q[1] + offy_b,\n",
        "                         *base_of_tower)"
      ],
      "outputs": [],
      "metadata": {
        "id": "YWiYeKHOyZ1v"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** Notice that the exact and the affine projection models give essentially the same 3D point.  The affine approximation turns out to be much faster.  In the following, we will study the precision of the affine approximation."
      ],
      "metadata": {
        "id": "YMOZ2u_iyZ1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Triangulation of a track\n",
        "\n",
        "The following array contains the image coordinates at the top of the Skytree tower (carefully picked by hand), for each of the 23 images in the series.  The Skytree tower has a height of $634m$ above the ground, which at this location is $45m$ above the surface of the WGS84 ellipsoid."
      ],
      "metadata": {
        "id": "ZVnCTak8yZ1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# position of the top of the antenna, selected by hand using an interactive image viewer\n",
        "top_of_skytree = np.array([\n",
        "    [26542, 6688 ], [27004, 6733 ], [27472, 6792 ], [27959, 6898 ], [28484, 7609 ],\n",
        "    [29012, 8122 ], [29524, 8501 ], [30045, 9211 ], [30501, 9782 ], [30829, 10125],\n",
        "    [31043, 10296], [31094, 10321], [31001, 10448], [30751, 10933], [30372, 11198],\n",
        "    [29902, 11236], [29377, 11445], [28828, 11390], [28297, 11096], [27581, 10991],\n",
        "    [27072, 10838], [26600, 10742], [26157, 10605]\n",
        "])"
      ],
      "outputs": [],
      "metadata": {
        "id": "FcRdfn0IyZ1z"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using these correspondences, we will fill-in a $23\\times23$ matrix (minus the diagonal) with the height of the top of the tower, as computed each possible pairs of images.  We display the matrix in color and explore if it has a visible pattern.  We also display the matrix for the reprojection errors."
      ],
      "metadata": {
        "id": "PniryqnyyZ1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this function is used to fill the matrix of heights\n",
        "def top_of_skytree_height_from_pair_exact(i, j):\n",
        "    if (i == j):\n",
        "        return 0, 0\n",
        "    else:\n",
        "        pi = top_of_skytree[i]\n",
        "        pj = top_of_skytree[j]\n",
        "        lon, lat, h,e = triangulation_iterative(myrpcs[i], myrpcs[j], *pi, *pj)\n",
        "        return h,e"
      ],
      "outputs": [],
      "metadata": {
        "id": "9ZOOPWAiyZ1z"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def skytree_top_matrices_exact():\n",
        "    \"\"\"\n",
        "    Compute the height of the skytree tower top using all possible pairs of images\n",
        "\n",
        "    Arguments:\n",
        "        none, use the global variables \"top_of_skytree\" and \"myrpcs\"\n",
        "\n",
        "    Return value:\n",
        "        Mh, Me : matrices of size 23x23 containing the height and the reprojection error for each pair\n",
        "    \"\"\"\n",
        "    Mh = np.zeros((23,23))\n",
        "    Me = np.zeros((23,23))\n",
        "    for j in range (23):\n",
        "        for i in range(23):\n",
        "            h,e = top_of_skytree_height_from_pair_exact(i,j)\n",
        "            #print((i,j,h))\n",
        "            Mh[i,j] = h\n",
        "            Me[i,j] = e\n",
        "    return Mh,Me"
      ],
      "outputs": [],
      "metadata": {
        "id": "qEpJOMNfyZ1z"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# build the matrix of heights using the EXACT triangulation    # THIS CAN TAKE UP TO A MINUTE\n",
        "ex_Mh, ex_Me = skytree_top_matrices_exact()"
      ],
      "outputs": [],
      "metadata": {
        "id": "u6T_jo4IyZ1z"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# display the matrices in color to interpret the results\n",
        "vistools.display_imshow(ex_Mh, [650, 720], cmap=\"jet\", title=\"Height computed for each pair\", inline=True)\n",
        "vistools.display_imshow(ex_Me, cmap=\"jet\", title=\"Epipolar error of each pair\", inline=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ccgYDJQ4yZ10"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def skytree_top_matrices_affine():\n",
        "    \"\"\"\n",
        "    Compute the height of the skytree tower top using all possible pairs of images\n",
        "\n",
        "    Arguments:\n",
        "        none, use the global variables \"top_of_skytree\" and \"myrpcs\"\n",
        "\n",
        "    Return value:\n",
        "        Mh, Me : matrices of size 23x23 containing the height and the reprojection error for each pair\n",
        "    \"\"\"\n",
        "\n",
        "    # first compute a base point for the approximations\n",
        "    p10 = top_of_skytree[10]\n",
        "    p11 = top_of_skytree[11]\n",
        "    base_lon, base_lat, base_h, _ = triangulation_iterative(myrpcs[10], myrpcs[11], *p10, *p11)\n",
        "\n",
        "    # we move the base point a bit\n",
        "    base_lat += 0.005 # about 500m\n",
        "    base_lon += 0.005\n",
        "    base_h = 0\n",
        "\n",
        "    # compute approximate projection matrices for each image\n",
        "    P = [\n",
        "        rectification.rpc_affine_approximation(myrpcs[i], (base_lon, base_lat, base_h))\n",
        "        for i in range(23)\n",
        "    ]\n",
        "\n",
        "    # fill height and error matrices\n",
        "    Mh = np.zeros((23,23))\n",
        "    Me = np.zeros((23,23))\n",
        "    for j in range (23):\n",
        "        for i in range(23):\n",
        "            pi = top_of_skytree[i]\n",
        "            pj = top_of_skytree[j]\n",
        "            if (i != j):\n",
        "                _, _, h, e = triangulation_affine(P[i], P[j], *pi, *pj)\n",
        "                Mh[i,j] = h\n",
        "                Me[i,j] = e\n",
        "    return Mh,Me"
      ],
      "outputs": [],
      "metadata": {
        "id": "4MCeuGEDyZ10"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# build the matrix of heights using the AFFINE APPROXIMATED triangulation\n",
        "ar_Mh, ar_Me = skytree_top_matrices_affine()"
      ],
      "outputs": [],
      "metadata": {
        "id": "K1Vwpc1yyZ10"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# display the matrices in color to interpret the results\n",
        "vistools.display_imshow(ar_Mh, [650, 720], cmap=\"jet\", title=\"Height computed for each pair\", inline=True)\n",
        "vistools.display_imshow(np.sqrt(ar_Me), cmap=\"jet\", title=\"Epipolar error for each pair\", inline=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "AiIlxwzVyZ10"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# show the difference between the exact and the approximate heights\n",
        "vistools.display_imshow(np.fmin(0.5,np.abs(ar_Mh - ex_Mh)),\n",
        "                        inline=True,\n",
        "                        title=\"Height difference between exact and\\naffine-approximated triangulation,\\nin meters\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "eAUxM5_ayZ10"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precision of the triangulation depending on the baseline\n",
        "\n",
        "Notice that the affine triangulation is essentially instantaneous.  This will allow to triangulate a dense set of matches.  But first, we study the stability and precision of the triangulation depending on the pair of images (that have different baselines).\n",
        "\n",
        "**~~Exercise~~** Repeat the previous experiment adding gaussian noise of $\\sigma=2\\mathrm{pixels}$ to the positions of the input points.  The experiment should be run a large number of times (say, one hundred), and the variance of the measure acquired for each image pair must be plotted in matrix form.  Can you identify the pairs of images where the measure is more precise?"
      ],
      "metadata": {
        "id": "sxEMyT0xyZ10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skytree_top_variances():\n",
        "    \"\"\"\n",
        "    Compute the height of the skytree tower top using all possible pairs of images\n",
        "\n",
        "    Arguments:\n",
        "        none, use the global variables \"top_of_skytree\" and \"myrpcs\"\n",
        "\n",
        "    Return value:\n",
        "        Mvar : Mvar[i,j] = estimated variance of height errors with respect to Mh\n",
        "    \"\"\"\n",
        "\n",
        "    n = 100 # number of tests\n",
        "    sigma = 1 # error in pixels\n",
        "\n",
        "    Mvar = np.zeros((23,23))\n",
        "\n",
        "    # first compute a base point for the approximations\n",
        "    p10 = top_of_skytree[10]\n",
        "    p11 = top_of_skytree[11]\n",
        "    base_lon, base_lat, base_h, _ = triangulation_iterative(myrpcs[10], myrpcs[11], *p10, *p11)\n",
        "\n",
        "    # compute approximate projection matrices for each image\n",
        "    P = [\n",
        "        rectification.rpc_affine_approximation(myrpcs[i], (base_lon, base_lat, base_h))\n",
        "        for i in range(23)\n",
        "    ]\n",
        "\n",
        "    # add gaussian noise to the points before computing the matrices,\n",
        "    Mh_all = np.zeros((n,23,23))\n",
        "    for k in range(n):\n",
        "        Mh = np.zeros((23,23))\n",
        "        for j in range (23):\n",
        "            for i in range(23):\n",
        "                pi = top_of_skytree[i] + sigma * np.random.randn(2)\n",
        "                pj = top_of_skytree[j] + sigma * np.random.randn(2)\n",
        "                if (i != j):\n",
        "                    _, _, h, _ = triangulation_affine(P[i], P[j], *pi, *pj)\n",
        "                    Mh[i,j] = h\n",
        "        Mh_all[k,:,:] = Mh\n",
        "\n",
        "\n",
        "    return Mh_all.std(axis=0)"
      ],
      "outputs": [],
      "metadata": {
        "id": "TpSXmO84yZ10"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the matrix of variances for each image pair\n",
        "MM = skytree_top_variances()"
      ],
      "outputs": [],
      "metadata": {
        "id": "74DjTQgvyZ10"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# show the precision matrix in color\n",
        "vistools.display_imshow(MM,\n",
        "                        cmap='jet', inline=True,\n",
        "                        title=\"Mean-square error of heights computed\\nfrom noisy pairs, in meters\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "Kq8Mg_5OyZ11"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense triangulation\n",
        "\n",
        "Until now, we have been triangulating a *single* point!\n",
        "To compute a dense 3D point cloud, we will apply the triangulation function to matches computed using the stereo matching algorithms seen on `TP-stereo`.  For that, we need to rectify the images using the techniques seen on `TP-rectification`."
      ],
      "metadata": {
        "id": "mQbk9bcVyZ11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preparation : select two images and an area of interest\n",
        "\n",
        "# select two central images\n",
        "idx_a = 11\n",
        "idx_b = 12\n",
        "\n",
        "# coordinates around the Meji Memorial Picture Museum (too large, do not use as it will be very slow),\n",
        "# or Skytree Tower\n",
        "aoi_meji = {'type': 'Polygon', 'coordinates': [[[139.806985, 35.707857],\n",
        "     [139.806985, 35.712143],\n",
        "     [139.815306, 35.712143],\n",
        "     [139.815306, 35.707857],\n",
        "     [139.806985, 35.707857]]]}"
      ],
      "outputs": [],
      "metadata": {
        "id": "SK2tv0kgyZ11"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "m = vistools.foliummap(zoom_start=15)\n",
        "folium.GeoJson(aoi_meji).add_to(m)\n",
        "\n",
        "# center the map on the center of the last footprint\n",
        "m.location = np.mean(aoi_meji['coordinates'][0], axis=0).tolist()[::-1]\n",
        "display(m)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-V5pwS5-yZ11"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the images before rectification\n",
        "a, offx_a, offy_a = utils.crop_aoi(myimages[idx_a], aoi_meji)\n",
        "b, offx_b, offy_b = utils.crop_aoi(myimages[idx_b], aoi_meji)\n",
        "vistools.display_gallery([utils.simplest_color_balance_8bit(x) for x in [a,b]])"
      ],
      "outputs": [],
      "metadata": {
        "id": "gb91HyqZyZ11"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# rectify the images using the techniques of TP-rectification\n",
        "rect1, rect2, S1, S2, dmin, dmax, PA, PB = rectification.rectify_aoi(myimages[idx_a],\n",
        "                                                                     myimages[idx_b],\n",
        "                                                                     aoi_meji)\n",
        "S1, S2, dmin, dmax"
      ],
      "outputs": [],
      "metadata": {
        "id": "GCfovSLsyZ11"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# show the rectified images\n",
        "vistools.display_gallery([utils.simplest_color_balance_8bit(x) for x in [rect1, rect2]])"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZCsaFxinyZ11"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have small, rectified images, we can feed them to any standard stereo matching algorithms to find a dense set of matches.  Here, we use the implementation of Semi Global Matching seen in the stereo matching practical session, with standard filtering options."
      ],
      "metadata": {
        "id": "9dKhVTEfyZ11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the size of the rectified images\n",
        "#\n",
        "# NOTE: it should not exceed too much 1000x1000,\n",
        "# otherwise the cost volumes for stereo will be huge and the SGM will fail.\n",
        "rect1.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "GANAqiQ4yZ11"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# compute a disparity map between two rectified images (should take less than 1 minute)\n",
        "LRS, _, _ = stereo.compute_disparity_map(rect1, rect2, dmin-50, dmax+10)"
      ],
      "outputs": [],
      "metadata": {
        "id": "LVd3xUFEyZ12"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# display the resulting disparity map\n",
        "vistools.display_imshow(-LRS, cmap='jet', inline=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "WFGrwpPVyZ12"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3.** Compute a few disparity maps between a central image and other images in the sequence.  How does a high baseline affect the quality of the result? (in terms of precision of the matches and density of valid points).\n",
        "\n",
        "**Answer.**  ..."
      ],
      "metadata": {
        "id": "T8iE9YbIyZ12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4.** ~~Write a function that transforms a disparty map into 3D point cloud.~~  For that, you have to invert the rectifying transformations to go back to the coordinates of the original (uncropped) image domain, and then triangulate these correspondences.  If your tringulation function admits vectorial inputs, then this step can be be blazingly fast!\n",
        "(NOTE: it was sunny this morning, so we decided to give you an efficient answer to this exercise)"
      ],
      "metadata": {
        "id": "FiNR_1h0yZ12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for exercise 4\n",
        "def triangulate_disparities(dmap, S1, S2, PA, PB):\n",
        "    \"\"\"\n",
        "    Triangulate a disparity map.\n",
        "\n",
        "    Arguments:\n",
        "        dmap : a disparity map between two rectified images\n",
        "        S1, S2 : rectifying affine maps (from the domain of the original, full-size images)\n",
        "        PA, PB : the affine approximations of the projection functions of each image\n",
        "\n",
        "    Return:\n",
        "        xyz : a matrix of size Nx3 (where N is the number of finite disparites in dmap)\n",
        "              this matrix contains the coordinates of the 3d points in \"lon,lat,h\" or \"e,n,h\"\n",
        "    \"\"\"\n",
        "\n",
        "    # WRITE YOUR CODE HERE\n",
        "    # suggested organization:\n",
        "    # 1. unroll all the valid (finite) disparities of dmap into a vector\n",
        "    # 2. for each disparity\n",
        "    # 2.1. produce a pair of points in the original image domain by composing with S1 and S2\n",
        "    # 2.2. triangulate the pair of image points to find a 3D point (in UTM coordinates)\n",
        "    # 2.3. add this point to the output list\n",
        "\n",
        "    # 1. unroll all the valid (finite) disparities of dmap into a vector\n",
        "    m = np.isfinite(dmap.flatten())\n",
        "    x = np.argwhere(np.isfinite(dmap))[:,1]    # attention to order of the indices\n",
        "    y = np.argwhere(np.isfinite(dmap))[:,0]\n",
        "    d = dmap.flatten()[m]\n",
        "\n",
        "    # 2. for each disparity\n",
        "    # 2.1. produce a pair of points in the original image domain by composing with S1 and S2\n",
        "    p = np.linalg.inv(S1) @ np.vstack( (x+0, y, np.ones(len(d))) )\n",
        "    q = np.linalg.inv(S2) @ np.vstack( (x+d, y, np.ones(len(d))) )\n",
        "    # 2.2. triangulate the pair of image points to find a 3D point (in UTM coordinates)\n",
        "    lon, lat, h, e = triangulation_affine(PA, PB, p[0,:], p[1,:], q[0,:], q[1,:])\n",
        "    east, north, zone = utils.lonlat_to_utm(lon, lat)\n",
        "    # 2.3. add this point to the output list\n",
        "    xyz = np.vstack((east, north, h)).T\n",
        "\n",
        "    # map of triangulation errors\n",
        "    err = dmap.copy()\n",
        "    err.flat[m] = e\n",
        "\n",
        "\n",
        "\n",
        "    return xyz, err"
      ],
      "outputs": [],
      "metadata": {
        "id": "PYLNjxOvyZ12"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# triangulate the disparities of the previously computed case\n",
        "xyz, err = triangulate_disparities(LRS, S1, S2, PA, PB)\n",
        "xyz.shape  # this should be a matrix of size (npoints , 3)"
      ],
      "outputs": [],
      "metadata": {
        "id": "w941k8nQyZ12"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the UTM bounding box of the obtained 3D points\n",
        "min_e, min_n = np.min(xyz,axis=0)[0:2]\n",
        "max_e, max_n = np.max(xyz,axis=0)[0:2]\n",
        "\n",
        "print((min_e, max_e, max_e-min_e))\n",
        "print((min_n, max_n, max_n-min_n))"
      ],
      "outputs": [],
      "metadata": {
        "id": "ou12RUeoyZ12"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if all went well, you should be able to display a 3D point cloud using the `display_cloud` function below"
      ],
      "metadata": {
        "id": "TfbjXh09yZ12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute a point cloud in POTREE format (stored on disk)\n",
        "import pypotree\n",
        "cloud = pypotree.generate_cloud_for_display(xyz)"
      ],
      "outputs": [],
      "metadata": {
        "id": "pL4n-qzYyZ12"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# display the POTREE inside the notebook\n",
        "pypotree.display_cloud_colab(cloud)"
      ],
      "outputs": [],
      "metadata": {
        "id": "yX-QpxQ4yZ12"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation.** With the default parameters, you do not recover the whole height of the skytree tower.  The disparities are just too large.  You can try to enlarge the disparity ranges given to SMG to find the top of the tower."
      ],
      "metadata": {
        "id": "lCDkbWqFyZ13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation of a D.E.M.\n",
        "\n",
        "Point clouds in 3D are beautiful, but in a geographical context it is often useful and easier to work with 2.5D models, called digital elevation models (DEM).  We can build a DEM by projecting a point cloud into a fixed square grid in UTM coordinates and accumulate into each square all the 3D points that fall into it.\n",
        "\n",
        "\n",
        "**Exercise 5.**  ~~Write a function that projects a 3D point cloud into a DEM.  This function receives as input the desired resolution of the DEM.~~ (it was sunny this morning, so we decided to give you an efficient answer to this exercise)"
      ],
      "metadata": {
        "id": "C5Fo7KMByZ13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first, define a helper function that will aid to project the points efficiently\n",
        "\n",
        "from numba import jit\n",
        "\n",
        "# this function accumulates all the 3D points into a 2.5D grid\n",
        "# it computes the maximum height (or the average height, by a small change)\n",
        "@jit\n",
        "def reducemax( w,h,  ix, iy, z ):\n",
        "    D_sum = -np.ones((h,w))*np.inf\n",
        "    D_cnt = np.zeros((h,w))\n",
        "    for t in range(len(ix)):\n",
        "        ty = iy[t]\n",
        "        tx = ix[t]\n",
        "        if tx >=0 and ty >= 0 and tx < w and ty < h:\n",
        "            D_sum[ty,tx] = max(D_sum[ty,tx], z[t])\n",
        "            D_cnt[ty,tx] += 1\n",
        "#    D_sum /= D_cnt  # needed for computing average\n",
        "\n",
        "    return D_sum\n",
        "\n",
        "def project_cloud_into_utm_grid(xyz, emin, emax, nmin, nmax, resolution=1):\n",
        "    \"\"\"\n",
        "    Project a point cloud into an utm grid to produce a DEM\n",
        "    The algorithm is the simplest possible: just average all the points that fall into each square of the grid.\n",
        "\n",
        "    Arguments:\n",
        "        xyz : a Nx3 matrix representing a point cloud in (easting,northing,h) coordinates\n",
        "        emin,emax,nmin,nmax : a bounding box in UTM coordinates\n",
        "        resolution : the target resolution in meters (by default, 1 meter)\n",
        "\n",
        "    Return:\n",
        "        dem : a 2D array of heights in meters\n",
        "    \"\"\"\n",
        "\n",
        "    # width and height of the image domain\n",
        "    w = int(np.ceil((emax - emin)/resolution))\n",
        "    h = int(np.ceil((nmax - nmin)/resolution))\n",
        "\n",
        "    # extract and quantize columns\n",
        "    x = xyz[:,0]\n",
        "    y = xyz[:,1]\n",
        "    z = xyz[:,2]\n",
        "\n",
        "    ix = np.asarray((x - emin)/resolution, dtype=\"int\")\n",
        "    iy = np.asarray((nmax - y)/resolution, dtype=\"int\")\n",
        "\n",
        "    dem = reducemax (w,h,  ix, iy, z )\n",
        "\n",
        "    return dem\n",
        "\n",
        "def dem_from_xyz(xyz, resolution=1):\n",
        "    min_e, min_n = np.min(xyz,axis=0)[0:2] - resolution\n",
        "    max_e, max_n = np.max(xyz,axis=0)[0:2] + resolution\n",
        "    return project_cloud_into_utm_grid(xyz, min_e, max_e, min_n, max_n, resolution)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RZtXtuasyZ13"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the DEM from the 3D point cloud obtained above\n",
        "dem = dem_from_xyz(xyz, resolution=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gvoWKAxEyZ13"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the DEM using a color palette\n",
        "# see other palettes here: https://matplotlib.org/examples/color/colormaps_reference.html\n",
        "vistools.display_imshow(dem, [25,100], cmap=\"terrain\", inline=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "kHYlCKxiyZ13"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have reached this point, you have produced a high-resolution D.E.M. from a pair of satellite images.  This is no small feat!  You have repeated in a single day a program that took us many years!\n",
        "\n",
        "# Multi-view stereo (by DEM fusion)\n",
        "\n",
        "Now let us see how can we merge the D.E.M. computed from several pairs of images."
      ],
      "metadata": {
        "id": "ZK5KxzjSyZ13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function that computes a D.E.M. from a pair of images\n",
        "def dem_from_pair(img_a, img_b, aoi, resolution=1):\n",
        "\n",
        "    # run the whole pipeline\n",
        "    R1, R2, S1, S2, dmin, dmax, PA, PB = rectification.rectify_aoi(img_a, img_b, aoi)\n",
        "    print(f\"dmin,dmax = {dmin},{dmax}\")\n",
        "    LRS, _, _ = stereo.compute_disparity_map(R1, R2, dmin-10, dmax+10, cost=\"census\")\n",
        "    print(f\"done computing disparity map\")\n",
        "    xyz, _ = triangulate_disparities(LRS, S1, S2, PA, PB)\n",
        "    emin, emax, nmin, nmax = utils.utm_bounding_box_from_lonlat_aoi(aoi)\n",
        "    dem = project_cloud_into_utm_grid(xyz, emin, emax, nmin, nmax, resolution)\n",
        "\n",
        "    return dem"
      ],
      "outputs": [],
      "metadata": {
        "id": "e3Z72MEjyZ13"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# compute DEM from images 11 and 12\n",
        "dem1 = dem_from_pair(myimages[11], myimages[12], aoi_meji)"
      ],
      "outputs": [],
      "metadata": {
        "id": "AjUGmAXIyZ13"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# compute DEM from images 12 and 13\n",
        "dem2 = dem_from_pair(myimages[12], myimages[13], aoi_meji)"
      ],
      "outputs": [],
      "metadata": {
        "id": "IqFYKAxnyZ13"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# compute DEM from images 13 and 14\n",
        "dem3 = dem_from_pair(myimages[13], myimages[14], aoi_meji)"
      ],
      "outputs": [],
      "metadata": {
        "id": "b7fT6dk8yZ13"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the computed DEMs\n",
        "vistools.display_imshow(dem1, cmap=\"terrain\", range=[20,150])\n",
        "vistools.display_imshow(dem2, cmap=\"terrain\", range=[20,150])\n",
        "vistools.display_imshow(dem3, cmap=\"terrain\", range=[20,150])"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZKK-o2IhyZ13"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the average of the three\n",
        "dem123 = (dem1 + dem2 + dem3)/3"
      ],
      "outputs": [],
      "metadata": {
        "id": "wbfAgc_FyZ14"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# view the three DEM and their average\n",
        "vistools.display_gallery([2*dem1, 2*dem2, 2*dem3, 2*dem123])"
      ],
      "outputs": [],
      "metadata": {
        "id": "y5mJ9YmdyZ14"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the average of the three DEM is only computed at the points where all the three DEMs are defined.  Thus, by merging more and more images we will obtain less and less dense DEM.\n",
        "\n",
        "**Exercise 6 .** Write a function that merges several DEMs into a single one, by computing the average (or the median) of the all the _avaliable_ heights at each point.  Verify that by merging more and more images you obtain denser and denser DEMs."
      ],
      "metadata": {
        "id": "0Od57e8eyZ14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dem_fusion(dems):\n",
        "    \"\"\"\n",
        "    Merge a list of several DEMs into a single one\n",
        "\n",
        "    Arguments:\n",
        "        dems: a list of 2D arrays of the same size\n",
        "\n",
        "    Return:\n",
        "        dem : a 2D array\n",
        "    \"\"\"\n",
        "\n",
        "    # WRITE THE CODE OF EXERCICE 6 HERE\n",
        "    # suggestion: use functions like numpy.nanmax or nanmedian\n",
        "\n",
        "    return dems[0]"
      ],
      "outputs": [],
      "metadata": {
        "id": "TxvQgSj_yZ14"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 7. (BONUS)**  Write a function that \"elevates\" a DEM into a 3D point cloud, and visualize the result of your dem_fusion as a 3D point cloud."
      ],
      "metadata": {
        "id": "FMh-sycfyZ14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dem_elevate_to_3d(dem, emin, emax, nmin, nmax):\n",
        "    \"\"\"\n",
        "    Elevate a DEM in UTM coordinates into a 3D point cloud\n",
        "\n",
        "    Arguments:\n",
        "        dem: a 2D array representing a DEM\n",
        "        emin,emax,nmin,mnax: the corresponding UTM bounding box\n",
        "\n",
        "    Return:\n",
        "        xyz : a 2D array of size Nx3, representing a 3D point cloud\n",
        "    \"\"\"\n",
        "\n",
        "    # WRITE THE CODE OF QUESTION 3 HERE\n",
        "    return xyz"
      ],
      "outputs": [],
      "metadata": {
        "id": "nLYLFlweyZ14"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL: test the \"elevation\" code\n",
        "emin, emax, nmin, nmax = utils.utm_bounding_box_from_lonlat_aoi(aoi_meji)\n",
        "cloud = dem_elevate_to_3d(dem123, emin, emax, nmin, nmax)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZPFlwvzoyZ14"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the computed cloud using the POTREE viewer\n",
        "pypotree.display_cloud_colab(pypotree.generate_cloud_for_display(cloud))"
      ],
      "outputs": [],
      "metadata": {
        "id": "VHAkMrfQyZ14"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpolation of Digital Elevation Maps\n",
        "\n",
        "All the DEM that we are creating have a lot of holes (missing data).  As we have seen in the course, we can interpolate them easily using linear second-order PDE."
      ],
      "metadata": {
        "id": "GUgfKhWMyZ14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import demtk  # small toolkit for DEM interpolation"
      ],
      "outputs": [],
      "metadata": {
        "id": "1BNN6z3RyZ14"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# interpolate by Laplace equation using Dirichlet boundary conditions\n",
        "dem1_laplace = demtk.fill_nans_by_laplace_equation(dem1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bzGoSuHjyZ14"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# interpolate by Laplace equation with Neumann condition on high jumps\n",
        "dem1_neumann = demtk.descending_neumann_interpolation(dem1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UM2Gx_i_yZ15"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "vistools.display_gallery([2*dem1, 2*dem1_laplace, 2*dem1_neumann])"
      ],
      "outputs": [],
      "metadata": {
        "id": "LFfgvTfxyZ15"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 8. (OPTIONAL, DIFFICULT)** Build a 3D model of the Skytree tower without holes, complete up to the antenna spike.\n",
        "\n",
        "This exercise is not for the faint of heart.  While it is conceptually simple, it requires a considerable time investiment and a comprehensive understanding of all the steps of the pipeline.  Most notably, you have to\n",
        "\n",
        "1. Select a few appropriate pairs of images\n",
        "2. For each pair, compute the disparity maps with a suitable disparity range so that the antenna spike is visible\n",
        "3. Merge the resulting DEM so that only small holes remain\n",
        "4. Interpolate the remaining holes\n",
        "5. Elevate the complete DEM into a 3D point cloud"
      ],
      "metadata": {
        "id": "o3228KWTyZ15"
      }
    }
  ]
}