{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gardiens/MVA_24_25/blob/main/MVA_SAT/project/clean_up_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oDXzHqnyZ1l"
      },
      "source": [
        "# Triangulation and Digital Elevation Models\n",
        "\n",
        "The objective of this practical session is to study the creation and processing of 3D\n",
        "models computed from satellite images.  The extraction of 3D points from\n",
        "image correspondences is called *triangulation*.  The processing of generic\n",
        "3D point clouds is a very general problem, out of the scope of the present\n",
        "course. Here, we will content ourselves with the much simpler 2.5D models\n",
        "that, in the context of geographic imaging, are called *digital elevation\n",
        "models* (D.E.M.).\n",
        "\n",
        "We cover the following topics:\n",
        "* How to find the vertical direction of an image\n",
        "* How to obtain a 3D point from a match between two images\n",
        "* How the precision of the point varies according to the baseline\n",
        "* Computation of a dense point cloud\n",
        "* Computation of a D.E.M. from a point cloud\n",
        "* Visualization, ~~filtering, interpolation, registration~~ and fusion of D.E.M.\n",
        "\n",
        "\n",
        "#### Instructions\n",
        "To solve this practical session, answer the questions below. Then, clear all the output cells using the menu option **Cell->All Output->Clear** and export the notebook with your answers using the menu option **File -> Download as -> Notebook (.ipynb)**. Then [submit the resulting file here](https://forms.gle/k6FSX2BrPix823dx6) by next week. You will receive an automatic acknowledgement of receipt.\n",
        "\n",
        "There are 4 compulsory exercises, plus 3 optional ones:\n",
        "* Exercise 1. Find the vertical direction of an image _(warm-up exercise)_\n",
        "* Exercise 2. Affine triangulation of a match _(head-scratching, requires quite a bit of hand computation)_\n",
        "* Exercise 3. Qualitative evaluation of high vs. low-baseline results _(only conceptual)_\n",
        "* Exercise 4. (given) Obtain a dense point cloud by triangulating a disparity map _(plug existing functions)_\n",
        "* Exercise 5. (given) Projection of a 3D point cloud into a DEM _(easy to do, hard to get 100% right)_\n",
        "* Exercise 6. Merge several DEM into a single one _(easy)_\n",
        "* Exercise 7. (optional) Create a 3D point cloud from a DEM _(easy for pythonists)_\n",
        "* Exercise 8. (optional) Create a complete 3D point cloud _(difficult)_\n",
        "\n",
        "**Note:** A correct solution of exercise 2 is _essential_ because all the following exercises depend on it.\n",
        "\n",
        "<!--\n",
        "## Overview of notations\n",
        "\n",
        "$(x,y)$, $(i,j)$ pixel coordinates in the domain of an image\n",
        "\n",
        "$(\\lambda,\\theta,h)$ latitude, longitude, height of a 3D point\n",
        "\n",
        "$(e,n,h)$ easting, northing, height of a 3D point (the UTM zone is implicit)\n",
        "\n",
        "$A, B, \\ldots$ gray-level images\n",
        "\n",
        "$A(x,y)$ pixel value at coordinates $(x,y)$ of image $A$\n",
        "\n",
        "$P_A(\\lambda,\\theta,h), L_A(x,y,h)$ projection and localization functions\n",
        "of image $A$\n",
        "\n",
        "$u, v, \\ldots$ raster images representing digital elevation models in\n",
        "meters\n",
        "\n",
        "$u(i,j)$ value of $u$ at the pixel $(i,j)$\n",
        "\n",
        "$u(e,n)$ height of the point at geographic coordinates $(e,n)$\n",
        "\n",
        "$u(\\lambda,\\theta)$ height of the point at geographic coordinates\n",
        "$(\\lambda,\\theta)$\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "L4ijNChSyZ1o",
        "outputId": "f13a2952-40e1-4556-d487-39849986d3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/2.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for srtm4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.7/147.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypotree (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Setup code for the notebook\n",
        "\n",
        "# Execute code 'cells' like this by clicking on the 'Run'\n",
        "# button or by pressing [shift] + [Enter].\n",
        "\n",
        "# This cell only imports some python packages that will be\n",
        "# used below. It doesn't generate any output.\n",
        "\n",
        "# The following lines install the necessary packages in the colab environment\n",
        "try:\n",
        "    from google.colab import files\n",
        "\n",
        "    # download TP data and tools\n",
        "    !wget -q http://boucantrin.ovh.hw.ipol.im/static/facciolo/mvaisat/tp4.zip\n",
        "    !unzip -q -o tp4.zip\n",
        "\n",
        "    # install dependencies\n",
        "    !python -m pip -q install rpcm\n",
        "    !pip install -q 'ad @ git+https://github.com/DapengFeng/ad'\n",
        "    !python -m pip -q install numpy matplotlib scipy geojson pyproj opencv-contrib-python==4.8.0.76 rasterio srtm4 folium numba pypotree\n",
        "\n",
        "except ImportError:\n",
        "    %matplotlib notebook\n",
        "    pass\n",
        "\n",
        "\n",
        "## Setup code for the notebook\n",
        "##\n",
        "# Autoreload external python modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "\n",
        "# general imports\n",
        "import numpy as np                   # numeric linear algebra\n",
        "from scipy import ndimage            # only for ndimage.affine_transform\n",
        "import matplotlib.pyplot as plt      # plotting\n",
        "import rpcm                          # RPC model\n",
        "\n",
        "# imports specific to this course\n",
        "import utils          # IO and conversion tools (from TP-collection)\n",
        "import vistools       # display tools (from TP-collection)\n",
        "import rectification  # rectification tools (from TP-rectification)\n",
        "import stereo         # stereo matching tools (from TP-stereo)\n",
        "import folium\n",
        "\n",
        "# display hacks\n",
        "np.set_printoptions(linewidth=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_yo43QDyZ1p"
      },
      "source": [
        "## The Tokyo dataset\n",
        "\n",
        "For this session we use the Tokyo series of 23 Pléiades images, acquired\n",
        "during a single orbit.  Notice that the satellite has to rotate very fast\n",
        "and very accurately to point the camera towards the city as it flies all over\n",
        "it in a few seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DycHPNVEyZ1q",
        "outputId": "3ca16cce-207c-42d7-ae55-d9f9d8588abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 23 images\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['http://menthe.ovh.hw.ipol.im/IARPA_data/tokyo_20130103/0137454_PHR1B_P/IMG_PHR1B_P_201301030137454_SEN_IPU_20141022_6527-003_R1C1.JP2.TIF',\n",
              " 'http://menthe.ovh.hw.ipol.im/IARPA_data/tokyo_20130103/0137571_PHR1B_P/IMG_PHR1B_P_201301030137571_SEN_IPU_20141022_6532-003_R1C1.JP2.TIF',\n",
              " 'http://menthe.ovh.hw.ipol.im/IARPA_data/tokyo_20130103/0138084_PHR1B_P/IMG_PHR1B_P_201301030138084_SEN_IPU_20141022_6544-003_R1C1.JP2.TIF',\n",
              " 'http://menthe.ovh.hw.ipol.im/IARPA_data/tokyo_20130103/0138195_PHR1B_P/IMG_PHR1B_P_201301030138195_SEN_IPU_20141022_6554-003_R1C1.JP2.TIF']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# list the tiff images available in the remote folder\n",
        "myimages = utils.find('http://menthe.ovh.hw.ipol.im/IARPA_data/tokyo_20130103/', 'TIF')\n",
        "print(f\"Found {len(myimages)} images\")\n",
        "myimages[0:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54vhZnPByZ1q",
        "outputId": "2c9fd2e4-7c65-4deb-aea2-5efbef54783a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['http://menthe.ovh.hw.ipol.im/IARPA_data/tokyo_20130103/0137454_PHR1B_P/IMG_PHR1B_P_201301030137454_SEN_IPU_20141022_6527-003_R1C1.JP2.TIF',\n",
              " 'http://menthe.ovh.hw.ipol.im/IARPA_data/tokyo_20130103/0137571_PHR1B_P/IMG_PHR1B_P_201301030137571_SEN_IPU_20141022_6532-003_R1C1.JP2.TIF',\n",
              " 'http://menthe.ovh.hw.ipol.im/IARPA_data/tokyo_20130103/0138084_PHR1B_P/IMG_PHR1B_P_201301030138084_SEN_IPU_20141022_6544-003_R1C1.JP2.TIF',\n",
              " 'http://menthe.ovh.hw.ipol.im/IARPA_data/tokyo_20130103/0138195_PHR1B_P/IMG_PHR1B_P_201301030138195_SEN_IPU_20141022_6554-003_R1C1.JP2.TIF']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# keep only the panchromatic (P) images, discard the multispectral (MS)\n",
        "myimages = [x for x in myimages if \"_P_\" in x]\n",
        "myimages[0:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkSTtEseyZ1q"
      },
      "source": [
        "In the exercises below we propose to use a fixed area of interest around the Skytree tower.  Optionally, you can select a different scene of your choice, using the `foliummap` function from the `vistools` module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWRE3AdDyZ1r"
      },
      "source": [
        "# Finding the vertical direction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy9qB_SkyZ1r"
      },
      "source": [
        "The first question is just for *warming up* using the RPC and image extraction\n",
        "functions from the previous sessions.\n",
        "\n",
        "**Exercise 1.** Implement the 'crop_vertical' function.  This function\n",
        "should extract the requested *Area of interest* (AOI) from an image and rotate it so that the vertical direction in space points upwards in the rotated image domain.  You\n",
        "can compute the vertical direction by evaluating the RPC functions on 2\n",
        "points at different heights.  The structure of this function is already given, you only need to complete the 'find_vertical_direction' function.\n",
        "\n",
        "Apply your function to an area around Skytree Tower (coordinates $35.710139, 139.810833$) or Tokyo Tower (coordinates $35.658611, 139.745556$) to verify that the building is indeed vertical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eREglYN3yZ1r"
      },
      "outputs": [],
      "source": [
        "# coordinates around Sky Tree's base (the second tallest building in the world)\n",
        "aoi_skytree = {'type': \"Polygon\", 'coordinates': [[\n",
        "    [139.808, 35.7119],\n",
        "    [139.807, 35.7078],\n",
        "    [139.815, 35.7078],\n",
        "    [139.815, 35.7123],\n",
        "    [139.808, 35.7119]\n",
        "]]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TvERQ2TZyZ1s"
      },
      "outputs": [],
      "source": [
        "# crop the region of interest on the first image\n",
        "crop, offset_x, offset_y = utils.crop_aoi(myimages[0], aoi_skytree)\n",
        "rpc = rpcm.rpc_from_geotiff(myimages[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bPTs2TA6yZ1s"
      },
      "outputs": [],
      "source": [
        "# code for Exercise 1\n",
        "def find_vertical_direction(rpc):\n",
        "    \"\"\"\n",
        "    Return the vertical direction associated to an image.\n",
        "\n",
        "    Arguments:\n",
        "        rpc: RPC function of the image\n",
        "\n",
        "    Return:\n",
        "        p, q : the vertical direction in the image domain (a normalized 2D vector)\n",
        "    \"\"\"\n",
        "\n",
        "    # EXERCISE 1 : WRITE THE CODE FOR THIS FUNCTION\n",
        "    p,q=rpc.localization(0,0,10)\n",
        "    p2,q2=rpc.localization(0,0,100)\n",
        "\n",
        "    # project into the image space\n",
        "    p,q=rpc.projection(p,q,100)\n",
        "    p2,q2=rpc.projection(p2,q2,100)\n",
        "    # return the projected\n",
        "    return (p-p2)/np.linalg.norm([p-p2,q-q2]),(q-q2)/np.linalg.norm([p-p2,q-q2])\n",
        "\n",
        "\n",
        "\n",
        "# build a rotation matrix that sets the 3D vertical direction upwards\n",
        "def build_verticalizing_rotation(rpc, shape):\n",
        "    p, q = find_vertical_direction(rpc)                   # cosine and sine\n",
        "    p, q = -q, p                                          # ndimage convention for rows/cols\n",
        "    x, y = shape[1]/2, shape[0]/2                         # center of rotation (middle of the image domain)\n",
        "    T = np.array([[ 1,  0, -x], [0,  1, -y], [0, 0, 1]])  # translate (x,y) to the origin\n",
        "    R = np.array([[ p, q,  0], [-q,  p,  0], [0, 0, 1]])  # rotate by the requested angle\n",
        "    R = np.linalg.inv(T) @ R @ T                          # full rotation matrix\n",
        "    return R\n",
        "\n",
        "# like crop_aoi, but rotates the image after cropping\n",
        "def crop_vertical(image, aoi, base_h=0):\n",
        "    rpc = rpcm.rpc_from_geotiff(image)\n",
        "    x, y, w, h = utils.bounding_box_of_projected_aoi(rpc, aoi, z=base_h)\n",
        "    R = build_verticalizing_rotation(rpc, (h, w))\n",
        "    R = np.dot(R, utils.matrix_translation(-x, -y))  # compensate for the crop offset\n",
        "    return rectification.affine_crop(image, R, w, h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XHKQ7xGkyZ1s"
      },
      "outputs": [],
      "source": [
        "# compare the crop of an image with and without the verticalizing rotation\n",
        "base_h = 45\n",
        "a, _, _ = utils.crop_aoi(myimages[0], aoi_skytree, base_h)\n",
        "b    = crop_vertical(myimages[0], aoi_skytree, base_h)\n",
        "\n",
        "qa = utils.simplest_color_balance_8bit(a)\n",
        "qb = utils.simplest_color_balance_8bit(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzqIphxAyZ1s"
      },
      "source": [
        "The following code is used to test your implementation of vertical crop.  You should obtain a timeseries of images such that the vertical direction always points upwards.  **Note:** to evaluate your answer to Question 1 we will run the code below and check visually if the sequence of vertical crops is, indeed, vertical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UZeIyChByZ1s"
      },
      "outputs": [],
      "source": [
        "# auxiliary function with the same interface as crop_vertical\n",
        "def crop_rectangular(image, aoi, base_h=0):\n",
        "    a, _, _ = utils.crop_aoi(image, aoi, base_h)\n",
        "    return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uLgLcSXAyZ1s"
      },
      "outputs": [],
      "source": [
        "# build a timeseries of crops (without rotation)\n",
        "crops = [crop_rectangular(x, aoi_skytree, base_h) for x in myimages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C6nd4xpzyZ1t"
      },
      "outputs": [],
      "source": [
        "# build a timeseries of crops (rotated in the vertical direction)\n",
        "vcrops = [crop_vertical(x, aoi_skytree, base_h) for x in myimages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i16aRvtSyZ1t"
      },
      "outputs": [],
      "source": [
        "# quantize the crops to 8 bits\n",
        "q_crops  = [utils.simplest_color_balance_8bit(x) for x in crops]\n",
        "q_vcrops = [utils.simplest_color_balance_8bit(x) for x in vcrops]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTf2dCSVyZ1t"
      },
      "source": [
        "# Triangulation of a point\n",
        "\n",
        "Triangulation consists in finding the position of a 3D point from its projection into two images.  It is also called *intersection* because it can be interpreted as the intersection of two rays in space.  If $x$ is a point in the domain of image $A$ and $x'$ is a point in the domain of image $A'$, the intersection is found by solving the following equation for $h$ : $L_A(x,h)=L_{A'}(x',h)$.  Then, the 3D point of intersection is either $L_A(x,h)$ or $L_{A'}(x',h)$.  This is a system of two equations with a single unknown $h$, so in general it will not have a solution.  In that case, we can define the \"solution\" by the value of $h$ that minimizes, for example, the error $e(h)=\\|L_A(x,h)-L_{A'}(x',h)\\|^2$.  Then, we obtain the 3D point by evaluating the localization function $L_A$.  Thus, the solution is a 3D point that, when projected into image $A$, it falls exactly on $x$, but when projected on image $A'$ it may be a bit far from $x'$.  Note that there are other reasonable energies to minimize, leading to (slightly) different results.  For example $e(h)=\\|P_{A'}(L_A(x,h),h)-x'\\|^2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU7kJu6fyZ1t"
      },
      "source": [
        "**Exercise 2** Implement the `triangulation_affine` function that finds the position of a 3D point given a correspondence between two images.  This function should be reasonably fast, because you will want to apply it to all the pixels of an image to obtain a dense 3D cloud.  We give you a slow, naive implementation based on iterative minimization of the error. You should write a fast implementation based on the affine approximation of the RPC functions developed in the session on rectification. Notice that in this case the triangulation function has a closed, linear form. Verify that your implementation gives the same results as triangulation_iterative (up to a few centimeters).\n",
        "\n",
        "**Note:** Solving this exercise is essential for the rest of the notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kZNiOnhkyZ1t"
      },
      "outputs": [],
      "source": [
        "# evaluate the epipolar line between two images at a value of h\n",
        "def epipolar_correspondence(rpc_A, rpc_B, x, y, h):\n",
        "    lon, lat = rpc_A.localization(x, y, h)\n",
        "    return rpc_B.projection(lon, lat, h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ccGoAWn_yZ1t"
      },
      "outputs": [],
      "source": [
        "# slowish implementation of the triangulation, based on iterative approximation\n",
        "def triangulation_iterative(rpc1, rpc2, x1, y1, x2, y2):\n",
        "    \"\"\"\n",
        "    Triangulate a match between two images.\n",
        "\n",
        "    Arguments:\n",
        "        rpc1, rpc2: calibration data for each image\n",
        "        x1, y1: pixel coordinates in the domain of the first image\n",
        "        x2, y2: pixel coordinates in the domain of the second image\n",
        "\n",
        "    Return value: a 4-tuple (lon, lat, h, e)\n",
        "        lon, lat, h, e: coordinates of the triangulated point, reprojection error\n",
        "    \"\"\"\n",
        "    # initial guess for h\n",
        "    h = rpc1.alt_offset\n",
        "    hstep = 1\n",
        "    err = 0\n",
        "\n",
        "    # iteratively improve h to minimize the error\n",
        "    for i in range(10):\n",
        "        # two points on the epipolar curve of (x1, y1)\n",
        "        # are used to approximate it by a straight line\n",
        "        px, py = epipolar_correspondence(rpc1, rpc2, x1, y1, h)\n",
        "        qx, qy = epipolar_correspondence(rpc1, rpc2, x1, y1, h + hstep)\n",
        "\n",
        "        # displacement vectors between these two points and with the target\n",
        "        ax, ay = qx-px, qy-py\n",
        "        bx, by = x2-px, y2-py\n",
        "\n",
        "        # projection of the target into the straight line\n",
        "        l = (ax*bx + ay*by) / (ax*ax + ay*ay)\n",
        "        rx, ry = px+l*ax, py+l*ay\n",
        "\n",
        "        # error of this projection\n",
        "        err = np.hypot(rx - x2, ry - y2)\n",
        "\n",
        "        # new value for h\n",
        "        h = h + l * hstep\n",
        "\n",
        "        # stop if l becomes too small (max 2 or 3 iterations are performed in practice)\n",
        "        if np.all(np.fabs(l) < 1e-3):\n",
        "            break\n",
        "\n",
        "    lon, lat = rpc1.localization(x1, y1, h)\n",
        "    return lon, lat, h, err"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BXoblxg-yZ1t"
      },
      "outputs": [],
      "source": [
        "\n",
        "def triangulation_affine(PA, PB, x1, y1, x2, y2):\n",
        "    \"\"\"\n",
        "    Better version vectorized\n",
        "    Vectorized triangulation of matches between two images of affine cameras.\n",
        "\n",
        "    Arguments:\n",
        "        PA, PB : affine (projection) camera matrices of the two images\n",
        "        x1, y1 : pixel coordinates in the domain of the first image (A)\n",
        "        x2, y2 : pixel coordinates in the domain of the second image (B)\n",
        "\n",
        "    Return value:\n",
        "        lon, lat, h, e : coordinates of the triangulated point(s), reprojection error\n",
        "    \"\"\"\n",
        "    # Ensure input arrays are 1D\n",
        "    x1, y1, x2, y2 = map(np.atleast_1d, (x1, y1, x2, y2))\n",
        "\n",
        "    # Convert PA and PB to 4x4 matrices\n",
        "    PA_ext = np.vstack([PA[:2], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "    PB_ext = np.vstack([PB[:2], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "\n",
        "    # Compute inverse of PA\n",
        "    LA = np.linalg.inv(PA_ext)\n",
        "\n",
        "    # Compute the epipolar transformation\n",
        "    epi = PB_ext @ LA\n",
        "    A = epi[:2, 2][:, np.newaxis]  # Ensure A is column vector for broadcasting\n",
        "\n",
        "    # Compute b in a vectorized manner\n",
        "    test2 = epi[:2, :2] @ np.vstack([x1, y1]) + epi[:2, 3, np.newaxis]\n",
        "    b = np.vstack([x2, y2]) - test2\n",
        "\n",
        "    # Solve for h using dot product\n",
        "    h = np.sum(b * A, axis=0) / np.sum(A**2)\n",
        "\n",
        "    # Compute lon, lat\n",
        "    lon = LA[0, 0] * x1 + LA[0, 1] * y1 + LA[0, 2] * h + LA[0, 3]\n",
        "    lat = LA[1, 0] * x1 + LA[1, 1] * y1 + LA[1, 2] * h + LA[1, 3]\n",
        "\n",
        "    # Compute reprojection error\n",
        "    ex = epi[0, 0] * x1 + epi[0, 1] * y1 + epi[0, 2] * h + epi[0, 3] - x2\n",
        "    ey = epi[1, 0] * x1 + epi[1, 1] * y1 + epi[1, 2] * h + epi[1, 3] - y2\n",
        "    e = ex**2 + ey**2\n",
        "\n",
        "    return lon, lat, h, e\n",
        "\n",
        "\n",
        "# compute the affine approximations and triangulate a list of points\n",
        "def triangulation_affine_rpc(rpc1, rpc2, x1, y1, x2, y2, base_lon, base_lat, base_h):\n",
        "    P_A = rectification.rpc_affine_approximation(rpc1, (base_lon, base_lat, base_h))\n",
        "    P_B = rectification.rpc_affine_approximation(rpc2, (base_lon, base_lat, base_h))\n",
        "    # add base_h to t0\n",
        "    return triangulation_affine(P_A, P_B, x1, y1, x2, y2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the rpcs of all images\n",
        "myrpcs = [rpcm.rpc_from_geotiff(x) for x in myimages]\n",
        "myrpcs[3].localization(1000, 1000, 10)\n",
        "idx_a = 9\n",
        "idx_b = 14\n",
        "# display the two crops, and a manually selected point\n",
        "p = [807, 861]\n",
        "q = [944, 38]\n",
        "# extract a crop of each image and SAVE THE CROP OFFSETS FOR LATER\n",
        "crop_a, offx_a, offy_a = utils.crop_aoi(myimages[idx_a], aoi_skytree)\n",
        "crop_b, offx_b, offy_b = utils.crop_aoi(myimages[idx_b], aoi_skytree)\n",
        "print(f\"x0_a, y0_a = {offx_a}, {offy_a}\")\n",
        "print(f\"x0_b, y0_b = {offx_b}, {offy_b}\")\n",
        "# choose a base point for the affine approximation\n",
        "# triangulate this single match to find (longitude, latitude, height, error_in_pixels)\n",
        "x = triangulation_iterative(myrpcs[idx_a], myrpcs[idx_b],\n",
        "                            p[0] + offx_a, p[1] + offy_a, q[0] + offx_b, q[1] + offy_b)\n",
        "x\n",
        "base_of_tower = [x[0], x[1], 40]"
      ],
      "metadata": {
        "id": "rk9oh7YmGfae",
        "outputId": "93f43d3b-27f2-4a3c-aa24-d64c9a0b91fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x0_a, y0_a = 30022, 9264\n",
            "x0_b, y0_b = 29428, 11160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# triangulate this match using the affine approximation\n",
        "x1=p[0] + offx_a\n",
        "print(\"x1\",x1)\n",
        "x2= q[0] + offx_b\n",
        "y1=p[1] + offy_a\n",
        "y2= q[1] + offy_b\n",
        "print(x1.ndim)\n",
        "triangulation_affine_rpc(myrpcs[idx_a], myrpcs[idx_b],\n",
        "\n",
        "                         x1=x1,x2=x2,y1=y1,y2=y2,\n",
        "                         base_lon=base_of_tower[0],base_lat=base_of_tower[1],base_h=base_of_tower[2])"
      ],
      "metadata": {
        "id": "pSKcPEgLGmi5",
        "outputId": "562370fb-cb52-419c-e1a1-6d0cba8e74a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1 30829\n",
            "0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([139.81072764]),\n",
              " array([35.7099937]),\n",
              " array([672.57789419]),\n",
              " array([0.03220839]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def return_color(img1,img2,x1,y1,x2,y2):\n",
        "    return (img1[y1,x1]+img2[y2,x2])/2\n",
        "\n",
        "img1=crop_a\n",
        "img2=crop_b\n",
        "return_color(img1,img2,p[0],p[1],q[0],q[1])\n"
      ],
      "metadata": {
        "id": "0_w56gSgHFXw",
        "outputId": "4e2c5571-b2fd-4bba-8828-8ff19f51ed37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1209.5"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K5ETq1eyZ1t"
      },
      "source": [
        "Now that we have the triangulation function, we can produce points in 3D space.  Let us compute the height of the Skytree tower.  https://en.wikipedia.org/wiki/Tokyo_Skytree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "t5vI1Ip8yZ1u"
      },
      "outputs": [],
      "source": [
        "# extract the rpcs of all images\n",
        "myrpcs = [rpcm.rpc_from_geotiff(x) for x in myimages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVnCTak8yZ1v"
      },
      "source": [
        "# Triangulation of a track\n",
        "\n",
        "The following array contains the image coordinates at the top of the Skytree tower (carefully picked by hand), for each of the 23 images in the series.  The Skytree tower has a height of $634m$ above the ground, which at this location is $45m$ above the surface of the WGS84 ellipsoid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FcRdfn0IyZ1z"
      },
      "outputs": [],
      "source": [
        "# position of the top of the antenna, selected by hand using an interactive image viewer\n",
        "top_of_skytree = np.array([\n",
        "    [26542, 6688 ], [27004, 6733 ], [27472, 6792 ], [27959, 6898 ], [28484, 7609 ],\n",
        "    [29012, 8122 ], [29524, 8501 ], [30045, 9211 ], [30501, 9782 ], [30829, 10125],\n",
        "    [31043, 10296], [31094, 10321], [31001, 10448], [30751, 10933], [30372, 11198],\n",
        "    [29902, 11236], [29377, 11445], [28828, 11390], [28297, 11096], [27581, 10991],\n",
        "    [27072, 10838], [26600, 10742], [26157, 10605]\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQbk9bcVyZ11"
      },
      "source": [
        "# Dense triangulation\n",
        "\n",
        "Until now, we have been triangulating a *single* point!\n",
        "To compute a dense 3D point cloud, we will apply the triangulation function to matches computed using the stereo matching algorithms seen on `TP-stereo`.  For that, we need to rectify the images using the techniques seen on `TP-rectification`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SK2tv0kgyZ11"
      },
      "outputs": [],
      "source": [
        "# preparation : select two images and an area of interest\n",
        "\n",
        "# select two central images\n",
        "idx_a = 11\n",
        "idx_b = 12\n",
        "\n",
        "# coordinates around the Meji Memorial Picture Museum (too large, do not use as it will be very slow),\n",
        "# or Skytree Tower\n",
        "aoi_meji = {'type': 'Polygon', 'coordinates': [[[139.806985, 35.707857],\n",
        "     [139.806985, 35.712143],\n",
        "     [139.815306, 35.712143],\n",
        "     [139.815306, 35.707857],\n",
        "     [139.806985, 35.707857]]]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gb91HyqZyZ11"
      },
      "outputs": [],
      "source": [
        "# look at the images before rectification\n",
        "a, offx_a, offy_a = utils.crop_aoi(myimages[idx_a], aoi_meji)\n",
        "b, offx_b, offy_b = utils.crop_aoi(myimages[idx_b], aoi_meji)\n",
        "# vistools.display_gallery([utils.simplest_color_balance_8bit(x) for x in [a,b]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCfovSLsyZ11",
        "outputId": "2c312653-9932-4a74-b2f3-35f7f6eb0dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'srtm.csi.cgiar.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "Downloading: /root/.srtm/srtm_64_05.zip Bytes: 18749810\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-2.36034663e-01,  9.67990821e-01, -1.99900000e+03],\n",
              "        [-9.67990821e-01, -2.36034663e-01,  3.32148786e+04],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00]]),\n",
              " array([[-2.60678957e-01,  9.77383067e-01, -1.63035096e+03],\n",
              "        [-9.74886334e-01, -2.38603129e-01,  3.33951214e+04],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00]]),\n",
              " -7.390496493035425,\n",
              " 0.7206055094513429)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# rectify the images using the techniques of TP-rectification\n",
        "rect1, rect2, S1, S2, dmin, dmax, PA, PB = rectification.rectify_aoi(myimages[idx_a],\n",
        "                                                                     myimages[idx_b],\n",
        "                                                                     aoi_meji)\n",
        "S1, S2, dmin, dmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCsaFxinyZ11"
      },
      "outputs": [],
      "source": [
        "# show the rectified images\n",
        "vistools.display_gallery([utils.simplest_color_balance_8bit(x) for x in [rect1, rect2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dKhVTEfyZ11"
      },
      "source": [
        "Now that we have small, rectified images, we can feed them to any standard stereo matching algorithms to find a dense set of matches.  Here, we use the implementation of Semi Global Matching seen in the stereo matching practical session, with standard filtering options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVd3xUFEyZ12",
        "outputId": "d81665a8-80cc-4989-ce19-d8b61482e63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t=4.8986 done building CV\n",
            "t=16.8611 done sgmfilter\n",
            "t=17.8386 done aggregation and WTA refinement\n",
            "t=4.6933 done building CV\n",
            "t=11.8920 done sgmfilter\n",
            "t=12.5847 done aggregation and WTA refinement\n"
          ]
        }
      ],
      "source": [
        "# compute a disparity map between two rectified images (should take less than 1 minute)\n",
        "LRS, _, _ = stereo.compute_disparity_map(rect1, rect2, dmin-50, dmax+10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFGrwpPVyZ12"
      },
      "outputs": [],
      "source": [
        "# display the resulting disparity map\n",
        "# vistools.display_imshow(-LRS, cmap='jet', inline=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8iE9YbIyZ12"
      },
      "source": [
        "**Exercise 3.** Compute a few disparity maps between a central image and other images in the sequence.  How does a high baseline affect the quality of the result? (in terms of precision of the matches and density of valid points).\n",
        "\n",
        "**Answer.**  \n",
        "We selected the 11th image as the central reference and computed disparity maps for offsets ranging from 1 to 3. Initially, the number of valid points was 1M8. As the temporal distance between images increased, the number of valid points decreased (from 1M8 to 1M5). The quality of the matching degrades visually, as indicated by less consistent disparity indices. For instance, the top of the skytree tower doesn't have any matches anymore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiNR_1h0yZ12"
      },
      "source": [
        "**Exercise 4.** ~~Write a function that transforms a disparty map into 3D point cloud.~~  For that, you have to invert the rectifying transformations to go back to the coordinates of the original (uncropped) image domain, and then triangulate these correspondences.  If your tringulation function admits vectorial inputs, then this step can be be blazingly fast!\n",
        "(NOTE: it was sunny this morning, so we decided to give you an efficient answer to this exercise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PYLNjxOvyZ12"
      },
      "outputs": [],
      "source": [
        "# code for exercise 4\n",
        "def triangulate_disparities(dmap, S1, S2, PA, PB):\n",
        "    \"\"\"\n",
        "    Triangulate a disparity map.\n",
        "\n",
        "    Arguments:\n",
        "        dmap : a disparity map between two rectified images\n",
        "        S1, S2 : rectifying affine maps (from the domain of the original, full-size images)\n",
        "        PA, PB : the affine approximations of the projection functions of each image\n",
        "\n",
        "    Return:\n",
        "        xyz : a matrix of size Nx3 (where N is the number of finite disparites in dmap)\n",
        "              this matrix contains the coordinates of the 3d points in \"lon,lat,h\" or \"e,n,h\"\n",
        "    \"\"\"\n",
        "\n",
        "    # WRITE YOUR CODE HERE\n",
        "    # suggested organization:\n",
        "    # 1. unroll all the valid (finite) disparities of dmap into a vector\n",
        "    # 2. for each disparity\n",
        "    # 2.1. produce a pair of points in the original image domain by composing with S1 and S2\n",
        "    # 2.2. triangulate the pair of image points to find a 3D point (in UTM coordinates)\n",
        "    # 2.3. add this point to the output list\n",
        "\n",
        "    # 1. unroll all the valid (finite) disparities of dmap into a vector\n",
        "    m = np.isfinite(dmap.flatten())\n",
        "    x = np.argwhere(np.isfinite(dmap))[:,1]    # attention to order of the indices\n",
        "    y = np.argwhere(np.isfinite(dmap))[:,0]\n",
        "    d = dmap.flatten()[m]\n",
        "\n",
        "    # 2. for each disparity\n",
        "    # 2.1. produce a pair of points in the original image domain by composing with S1 and S2\n",
        "    p = np.linalg.inv(S1) @ np.vstack( (x+0, y, np.ones(len(d))) )\n",
        "    q = np.linalg.inv(S2) @ np.vstack( (x+d, y, np.ones(len(d))) )\n",
        "    # 2.2. triangulate the pair of image points to find a 3D point (in UTM coordinates)\n",
        "\n",
        "    lon, lat, h, e = triangulation_affine(PA, PB, p[0,:], p[1,:], q[0,:], q[1,:])\n",
        "    east, north, zone = utils.lonlat_to_utm(lon, lat)\n",
        "    # 2.3. add this point to the output list\n",
        "    xyz = np.vstack((east, north, h)).T\n",
        "\n",
        "    # map of triangulation errors\n",
        "    err = dmap.copy()\n",
        "    err.flat[m] = e\n",
        "\n",
        "\n",
        "\n",
        "    return xyz, err"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w941k8nQyZ12",
        "outputId": "15be5e42-d377-4779-d7d3-5bb9b5ce50ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1808886, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# triangulate the disparities of the previously computed case\n",
        "xyz, err = triangulate_disparities(LRS, S1, S2, PA, PB)\n",
        "xyz.shape  # this should be a matrix of size (npoints , 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou12RUeoyZ12"
      },
      "outputs": [],
      "source": [
        "# compute the UTM bounding box of the obtained 3D points\n",
        "min_e, min_n = np.min(xyz,axis=0)[0:2]\n",
        "max_e, max_n = np.max(xyz,axis=0)[0:2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfbjXh09yZ12"
      },
      "source": [
        "Now, if all went well, you should be able to display a 3D point cloud using the `display_cloud` function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yksDaEflz0Ht"
      },
      "outputs": [],
      "source": [
        "# save the point clouds\n",
        "np.savez(\"xyz.npz\", xyz=xyz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL4n-qzYyZ12"
      },
      "outputs": [],
      "source": [
        "# compute a point cloud in POTREE format (stored on disk)\n",
        "import pypotree\n",
        "cloud = pypotree.generate_cloud_for_display(xyz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX-QpxQ4yZ12"
      },
      "outputs": [],
      "source": [
        "# display the POTREE inside the notebook\n",
        "# pypotree.display_cloud_colab(cloud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCDkbWqFyZ13"
      },
      "source": [
        "**Observation.** With the default parameters, you do not recover the whole height of the skytree tower.  The disparities are just too large.  You can try to enlarge the disparity ranges given to SMG to find the top of the tower."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Fo7KMByZ13"
      },
      "source": [
        "# Creation of a D.E.M.\n",
        "\n",
        "Point clouds in 3D are beautiful, but in a geographical context it is often useful and easier to work with 2.5D models, called digital elevation models (DEM).  We can build a DEM by projecting a point cloud into a fixed square grid in UTM coordinates and accumulate into each square all the 3D points that fall into it.\n",
        "\n",
        "\n",
        "**Exercise 5.**  ~~Write a function that projects a 3D point cloud into a DEM.  This function receives as input the desired resolution of the DEM.~~ (it was sunny this morning, so we decided to give you an efficient answer to this exercise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZtXtuasyZ13"
      },
      "outputs": [],
      "source": [
        "# first, define a helper function that will aid to project the points efficiently\n",
        "\n",
        "from numba import jit\n",
        "\n",
        "# this function accumulates all the 3D points into a 2.5D grid\n",
        "# it computes the maximum height (or the average height, by a small change)\n",
        "@jit\n",
        "def reducemax( w,h,  ix, iy, z ):\n",
        "    D_sum = -np.ones((h,w))*np.inf\n",
        "    D_cnt = np.zeros((h,w))\n",
        "    for t in range(len(ix)):\n",
        "        ty = iy[t]\n",
        "        tx = ix[t]\n",
        "        if tx >=0 and ty >= 0 and tx < w and ty < h:\n",
        "            D_sum[ty,tx] = max(D_sum[ty,tx], z[t])\n",
        "            D_cnt[ty,tx] += 1\n",
        "#    D_sum /= D_cnt  # needed for computing average\n",
        "\n",
        "    return D_sum\n",
        "\n",
        "def project_cloud_into_utm_grid(xyz, emin, emax, nmin, nmax, resolution=1):\n",
        "    \"\"\"\n",
        "    Project a point cloud into an utm grid to produce a DEM\n",
        "    The algorithm is the simplest possible: just average all the points that fall into each square of the grid.\n",
        "\n",
        "    Arguments:\n",
        "        xyz : a Nx3 matrix representing a point cloud in (easting,northing,h) coordinates\n",
        "        emin,emax,nmin,nmax : a bounding box in UTM coordinates\n",
        "        resolution : the target resolution in meters (by default, 1 meter)\n",
        "\n",
        "    Return:\n",
        "        dem : a 2D array of heights in meters\n",
        "    \"\"\"\n",
        "\n",
        "    # width and height of the image domain\n",
        "    w = int(np.ceil((emax - emin)/resolution))\n",
        "    h = int(np.ceil((nmax - nmin)/resolution))\n",
        "\n",
        "    # extract and quantize columns\n",
        "    x = xyz[:,0]\n",
        "    y = xyz[:,1]\n",
        "    z = xyz[:,2]\n",
        "\n",
        "    ix = np.asarray((x - emin)/resolution, dtype=\"int\")\n",
        "    iy = np.asarray((nmax - y)/resolution, dtype=\"int\")\n",
        "\n",
        "    dem = reducemax (w,h,  ix, iy, z )\n",
        "\n",
        "    return dem\n",
        "\n",
        "def dem_from_xyz(xyz, resolution=1):\n",
        "    min_e, min_n = np.min(xyz,axis=0)[0:2] - resolution\n",
        "    max_e, max_n = np.max(xyz,axis=0)[0:2] + resolution\n",
        "    return project_cloud_into_utm_grid(xyz, min_e, max_e, min_n, max_n, resolution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvoWKAxEyZ13"
      },
      "outputs": [],
      "source": [
        "# compute the DEM from the 3D point cloud obtained above\n",
        "dem = dem_from_xyz(xyz, resolution=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHYlCKxiyZ13"
      },
      "outputs": [],
      "source": [
        "# visualize the DEM using a color palette\n",
        "# see other palettes here: https://matplotlib.org/examples/color/colormaps_reference.html\n",
        "vistools.display_imshow(dem, [25,100], cmap=\"terrain\", inline=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK5KxzjSyZ13"
      },
      "source": [
        "If you have reached this point, you have produced a high-resolution D.E.M. from a pair of satellite images.  This is no small feat!  You have repeated in a single day a program that took us many years!\n",
        "\n",
        "# Multi-view stereo (by DEM fusion)\n",
        "\n",
        "Now let us see how can we merge the D.E.M. computed from several pairs of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3Z72MEjyZ13"
      },
      "outputs": [],
      "source": [
        "# function that computes a D.E.M. from a pair of images\n",
        "def dem_from_pair(img_a, img_b, aoi, resolution=1):\n",
        "\n",
        "    # run the whole pipeline\n",
        "    R1, R2, S1, S2, dmin, dmax, PA, PB = rectification.rectify_aoi(img_a, img_b, aoi)\n",
        "    print(f\"dmin,dmax = {dmin},{dmax}\")\n",
        "    LRS, _, _ = stereo.compute_disparity_map(R1, R2, dmin-10, dmax+10, cost=\"census\")\n",
        "    print(f\"done computing disparity map\")\n",
        "    xyz, _ = triangulate_disparities(LRS, S1, S2, PA, PB)\n",
        "    emin, emax, nmin, nmax = utils.utm_bounding_box_from_lonlat_aoi(aoi)\n",
        "    dem = project_cloud_into_utm_grid(xyz, emin, emax, nmin, nmax, resolution)\n",
        "\n",
        "    return dem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjUGmAXIyZ13"
      },
      "outputs": [],
      "source": [
        "# compute DEM from images 11 and 12\n",
        "dem1 = dem_from_pair(myimages[11], myimages[12], aoi_meji)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqFYKAxnyZ13"
      },
      "outputs": [],
      "source": [
        "# compute DEM from images 12 and 13\n",
        "dem2 = dem_from_pair(myimages[12], myimages[13], aoi_meji)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7fT6dk8yZ13"
      },
      "outputs": [],
      "source": [
        "# compute DEM from images 13 and 14\n",
        "dem3 = dem_from_pair(myimages[13], myimages[14], aoi_meji)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKK-o2IhyZ13"
      },
      "outputs": [],
      "source": [
        "# visualize the computed DEMs\n",
        "vistools.display_imshow(dem1, cmap=\"terrain\", range=[20,150])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbfAgc_FyZ14"
      },
      "outputs": [],
      "source": [
        "# compute the average of the three\n",
        "dem123 = (dem1 + dem2 + dem3)/3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5mJ9YmdyZ14"
      },
      "outputs": [],
      "source": [
        "# view the three DEM and their average\n",
        "vistools.display_gallery([2*dem1, 2*dem2, 2*dem3, 2*dem123])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Od57e8eyZ14"
      },
      "source": [
        "Notice that the average of the three DEM is only computed at the points where all the three DEMs are defined.  Thus, by merging more and more images we will obtain less and less dense DEM.\n",
        "\n",
        "**Exercise 6 .** Write a function that merges several DEMs into a single one, by computing the average (or the median) of the all the _avaliable_ heights at each point.  Verify that by merging more and more images you obtain denser and denser DEMs.\n",
        "\n",
        "\n",
        "__answer__: we couldn't push because of RAM issue.  The more image we added, the more dense is the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxvQgSj_yZ14"
      },
      "outputs": [],
      "source": [
        "\n",
        "def dem_fusion(dems):\n",
        "    \"\"\"\n",
        "    Merge a list of several DEMs into a single one\n",
        "\n",
        "    Arguments:\n",
        "        dems: a list of 2D arrays of the same size\n",
        "\n",
        "    Return:\n",
        "        dem : a 2D array\n",
        "    \"\"\"\n",
        "    # dems[dems == -np.inf] = np.nan  # Replace -inf with NaN\n",
        "    for dem in dems:\n",
        "      dem[dem==-np.inf]=np.nan\n",
        "    return np.nanmean(dems, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceVy3kjAG8Le"
      },
      "outputs": [],
      "source": [
        "# result=[dem1,dem2,dem3]\n",
        "# start=11\n",
        "\n",
        "# for i in range(1,3):\n",
        "#   result1=dem_fusion(result[:i])\n",
        "\n",
        "#   vistools.display_imshow(result1, cmap=\"terrain\", range=[20,150],title=f\" Output DEM with {i} input img\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMh-sycfyZ14"
      },
      "source": [
        "**Exercise 7. (BONUS)**  Write a function that \"elevates\" a DEM into a 3D point cloud, and visualize the result of your dem_fusion as a 3D point cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLYLFlweyZ14"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def dem_elevate_to_3d(dem, emin, emax, nmin, nmax):\n",
        "    \"\"\"\n",
        "    Elevate a DEM in UTM coordinates into a 3D point cloud\n",
        "\n",
        "    Arguments:\n",
        "        dem: a 2D array representing a DEM\n",
        "        emin,emax,nmin,mnax: the corresponding UTM bounding box\n",
        "\n",
        "    Return:\n",
        "        xyz : a 2D array of size Nx3, representing a 3D point cloud\n",
        "    \"\"\"\n",
        "    xyz = []\n",
        "    # WRITE THE CODE OF QUESTION 3 HERE\n",
        "    w,h = dem.shape\n",
        "    for i in range(w):\n",
        "        for j in range(h):\n",
        "            if math.isfinite(dem[i,j]):\n",
        "                x = i + emin\n",
        "                y = nmax - j\n",
        "                xyz.append([x, y, dem[i,j]])\n",
        "    return xyz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPFlwvzoyZ14"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: test the \"elevation\" code\n",
        "emin, emax, nmin, nmax = utils.utm_bounding_box_from_lonlat_aoi(aoi_meji)\n",
        "cloud = dem_elevate_to_3d(dem123, emin, emax, nmin, nmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHAkMrfQyZ14"
      },
      "outputs": [],
      "source": [
        "# visualize the computed cloud using the POTREE viewer\n",
        "pypotree.display_cloud_colab(pypotree.generate_cloud_for_display(cloud))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUgfKhWMyZ14"
      },
      "source": [
        "# Interpolation of Digital Elevation Maps\n",
        "\n",
        "All the DEM that we are creating have a lot of holes (missing data).  As we have seen in the course, we can interpolate them easily using linear second-order PDE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BNN6z3RyZ14"
      },
      "outputs": [],
      "source": [
        "import demtk  # small toolkit for DEM interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzGoSuHjyZ14"
      },
      "outputs": [],
      "source": [
        "# interpolate by Laplace equation using Dirichlet boundary conditions\n",
        "dem1_laplace = demtk.fill_nans_by_laplace_equation(dem1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM2Gx_i_yZ15"
      },
      "outputs": [],
      "source": [
        "# interpolate by Laplace equation with Neumann condition on high jumps\n",
        "dem1_neumann = demtk.descending_neumann_interpolation(dem1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFfgvTfxyZ15"
      },
      "outputs": [],
      "source": [
        "vistools.display_gallery([2*dem1, 2*dem1_laplace, 2*dem1_neumann])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3228KWTyZ15"
      },
      "source": [
        "**Exercise 8. (OPTIONAL, DIFFICULT)** Build a 3D model of the Skytree tower without holes, complete up to the antenna spike.\n",
        "\n",
        "This exercise is not for the faint of heart.  While it is conceptually simple, it requires a considerable time investiment and a comprehensive understanding of all the steps of the pipeline.  Most notably, you have to\n",
        "\n",
        "1. Select a few appropriate pairs of images\n",
        "2. For each pair, compute the disparity maps with a suitable disparity range so that the antenna spike is visible\n",
        "3. Merge the resulting DEM so that only small holes remain\n",
        "4. Interpolate the remaining holes\n",
        "5. Elevate the complete DEM into a 3D point cloud"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}