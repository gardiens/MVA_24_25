{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "36095417-3681-4329-9c25-b71583a3e3f6"
   },
   "source": [
    "# Implicit representation for mesh reconstruction with Point Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "a5d63547-7405-43ad-8997-b28f05ece806"
   },
   "source": [
    "In this lab work we will reconstruct shapes from point sets with and without their normal information.\n",
    "Each network/method will output the distance or signed distance and one can extract the surface with Marching\n",
    "cubes, following these steps :\n",
    "- Use the trained network to compute the values of the signed distance on a grid\n",
    "- Extract the 0 levelset (marching_cubes method of the mcubes library)\n",
    "- Save/visualize the mesh (export_obj method of the mcubes library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter server list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JVRLjO_fVErO",
    "outputId": "0b3fe686-8d46-42cf-f455-6045587bcf09"
   },
   "outputs": [],
   "source": [
    "!pip install potpourri3d\n",
    "!pip install git+https://github.com/skoch9/meshplot.git\n",
    "!pip install pythreejs\n",
    "!pip install pymcubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLP07KHSWRb4",
    "outputId": "94f19940-0cee-495a-e4a4-717469d3a23b"
   },
   "outputs": [],
   "source": [
    "!wget https://www.lix.polytechnique.fr/~pierson/cours/tp_sdf_material.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWAxPUK2VLj0",
    "outputId": "602d9c89-77dd-43c3-c4cd-238fabd8e771"
   },
   "outputs": [],
   "source": [
    "!unzip -o tp_sdf_material.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMCubes==0.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "e0c895ab-6aac-49b9-bf78-026b2c1b4740"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import mcubes\n",
    "import plot_utils as plu\n",
    "from mesh_utils.mesh import TriMesh\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "adb369d0"
   },
   "source": [
    "# Traditional reconstruction approach\n",
    "This method is a \"historical\" method (<a href=\"https://dl.acm.org/doi/abs/10.1145/133994.134011\">link</a>) for reconstructing a surface from a set of points. It consists in taking an oriented point cloud $(x_i , n_i )$, and estimating for any arbitrary point $x$ in the ambient space a signed distance function as : $u(x) = ± min_i ∥x_i − x∥$\n",
    "\n",
    "The sign is given by the sign of the scalar product $\\langle x - x_i, n_i \\rangle$.\n",
    "\n",
    "The original method starts with unoriented point clouds and devises a clever way to estimate the normal direction and their orientation. Here, for simplicity, we start with oriented points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "545ae05c-32fb-4d95-9053-28472362e908"
   },
   "outputs": [],
   "source": [
    "def get_pc(path):\n",
    "    ## Load the oriented point set. You can use the function np.loadtxt\n",
    "    xyz=np.loadtxt(path)\n",
    "    point_cloud=xyz[:,:3]\n",
    "    normals=xyz[:,3:]\n",
    "    return point_cloud, normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "96a97e2b-5b8b-4b42-b1cb-69d46aa13a46"
   },
   "outputs": [],
   "source": [
    "pc, normals = get_pc(\"armadillo_sub.xyz\")\n",
    "normals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978,
     "referenced_widgets": [
      "a0d23a848b0149c7a86c7241181fee9b",
      "21cd72c937be45ffa1aa1ecd66565055",
      "85b7741de559492ea0b0ee731b808ee8",
      "e18fcaef8e8f439f829be72913b09838",
      "a40304a01ddc452e89133e17c21c09ab",
      "ba57353533c047878b8d58e10d940aa3",
      "fb0e1201299146d0907726ba0a1671b8",
      "bc4b6d21503f492c9f8d785886144744",
      "7c6c112f72ad4c05b603165b08978430",
      "049c3d700d6e44a58fa1fb5198eb475b",
      "1c81f0a67a3c42ada4b4f7ba51004c20",
      "bf4c052432734190aaafdf37332d6075",
      "32fcb6d5aae14d708c9257951e5b6b57"
     ]
    },
    "id": "c69237d8-6698-4181-8ca8-bd59348dbd93",
    "outputId": "2f4d5da9-2211-4ac7-bf53-172e5d3efa3a"
   },
   "outputs": [],
   "source": [
    "plu.plot_pc(pc, point_size=2,cmap=normals) #You can put cmap = normals to see normals orientation as color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "1d672be2-d96c-4a6f-a3f6-ea4d8d0ddfba"
   },
   "source": [
    "The second step is to compute the sdf based on the set of points. You will first need to build a grid of points (using e.g. np.meshgrid), and then to compute the sdf to the set of points. Don't forget to adapt the limits of the grid to the size of the point cloud! For the distance, use an efficient way to compute the distance (look at solutions of previous labs to get an idea)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "def Query_CPU(xyz_query, xyz_search, K):\n",
    "    # Use scipy Kdtree to perform the query\n",
    "    # XYZ MUST BE OF dimension 2 not 3\n",
    "    kdtree = cKDTree(xyz_search)\n",
    "    # TODO: you may need to tweak the number of num_workers, 1 is really long but -1 is can OOM\n",
    "    distances, neighbors = kdtree.query(\n",
    "        xyz_query, k=K, workers=-1\n",
    "    )\n",
    "    return distances, neighbors\n",
    "\n",
    "def compute_nearest_neighbor(X, Y):\n",
    "    \"\"\"\n",
    "    Compute the nearest neighbor in Y for each point in X\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : (n, d) array of points\n",
    "    Y : (m, d) array of points\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    nearst_neighbor : (n,) array of indices of the nearest neighbor in Y for X\n",
    "    \"\"\"\n",
    "    return Query_CPU(xyz_query=X,xyz_search=Y,K=1)[1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "7b34c02e-3384-4b88-beb6-4a05fc42b369"
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy import spatial\n",
    "def compute_sdf(point_cloud, normals, points_query):\n",
    "    ## Compute SDF on points_query from the shape defined by point_cloud and normals\n",
    "    print(\"point clouds\",point_cloud.shape)\n",
    "    print(\"query\",points_query.shape)\n",
    "    distances,neighbors=Query_CPU(xyz_query=points_query,xyz_search=point_cloud,K=1)\n",
    "    print(\"point_clouds neighbors\",point_cloud[neighbors].shape)\n",
    "    # print(\"test\",np.dot(points_query-point_cloud[neighbors],normals[neighbors]))\n",
    "    print(\"normals\",normals[neighbors].shape)\n",
    "    affine_vector=points_query-point_cloud[neighbors]\n",
    "    #np.tensordot(affine_vector,normals[neighbors],axes=0)\n",
    "    return distances*np.sign(\n",
    "       np.sum(affine_vector*normals[neighbors],axis=1))    \n",
    "\n",
    "def compute_sdf_grid(point_cloud, normals, grid_size=40):\n",
    "    ## Compute SDF on a XYZ grid. First generate the grid (it has to enclose the point cloud)\n",
    "    ## Then compute the sdf\n",
    "    #compute the enclosing grid\n",
    "    max_x=np.max(point_cloud,axis=0)\n",
    "    min_x=np.min(point_cloud,axis=0)\n",
    "    liste_grid=[]\n",
    "    for i in range(len(min_x)):\n",
    "        liste_grid.append(np.linspace(min_x[i]+1,max_x[i]+1,grid_size))\n",
    "    grid_x,grid_y,grid_z=np.meshgrid(liste_grid[0],liste_grid[1],liste_grid[2])\n",
    "    # points_query=grid[0].shape\n",
    "    points_query=np.array((grid_x.ravel(), grid_y.ravel(),grid_z.ravel())).T\n",
    "    sdf=compute_sdf(point_cloud=point_cloud,normals=normals,points_query=points_query)\n",
    "\n",
    "    \n",
    "    return sdf # shape (grid_size, grid_size, grid_size)\n",
    "sdf = compute_sdf_grid(pc, normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "7501d31d-6301-4f66-a8ae-645208384e4c"
   },
   "outputs": [],
   "source": [
    "grid_size=40\n",
    "sdf = compute_sdf_grid(pc, normals,grid_size=grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.reshape(grid_size,grid_size,grid_size).shape\n",
    "sdf=sdf.reshape(grid_size,grid_size,grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978,
     "referenced_widgets": [
      "391b74dbf597484d894f4849f0cc0373",
      "73da99c3a634410489b3217005de4267",
      "09afb5addb254c8eaa57ba17c4a17f6c",
      "58878c7d0deb48ffa0ed8ffa8c7efb76",
      "dbd12d44ada7410581f4f9a0532556ae",
      "2eb48b41ecb94ef18aba4c9dce9fc3cd",
      "260d4b9064bf4e7f8940f6fdb2db7e60",
      "8884a15509704f418e3baf747c978f21",
      "af00878bc86e4811880053a056956e85",
      "7291c1b7d2ca4391958376561a2e8d51",
      "c2b1122c8cd640fcb0e9c4ca80674e8f",
      "4d0f56e67ff14eba9aad0fc176f4e719",
      "f6ddac9617d14a3995ef687f28439061",
      "6bc9b2fd29294397bfaa45ef111de96f"
     ]
    },
    "id": "31442fcb-2a9a-42ab-a1d9-41e29ce462b6",
    "outputId": "a32839a4-e5a1-4c3d-8bc6-4a94be4afdad"
   },
   "outputs": [],
   "source": [
    "vertices, triangles = mcubes.marching_cubes(sdf,0)\n",
    "mesh = TriMesh(vertices, triangles)\n",
    "print(\"mesh\",mesh)\n",
    "#mcubes.export_obj(vertices, triangles, 'result_hoppe.obj')\n",
    "plu.plot(mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "4af67d5b-d9fa-4685-ad42-ab15d01086e2"
   },
   "source": [
    "You can try different grid sizes, but do not increase too much its size to avoid memory issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "b497b547-ce5b-4fdf-bff6-11765cba573a"
   },
   "source": [
    "# DeepSDF\n",
    "\n",
    "This method (see <a href=\"https://arxiv.org/pdf/1901.05103\">link</a>) consists of representing the SDF as a function (x, y, z) -> sdf, parameterized by a neural network.\n",
    "\n",
    "We first build the network according to the following figure\n",
    "\n",
    "![title](img/TD5_sdf.png)\n",
    "\n",
    "The activations are ReLUs, except for the last one, defined as $\\phi(a) = \\text{tanh}(a)$.\n",
    "\n",
    "Moreover, the networks have specific initialization (**except the last one**): the weights of size $n \\times n$ are initialized according to the following $\\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n}}\\right)$ law, and the bias are initalized to 0 (except for the last linear layer). You can access to a Linear layer weight, and bias via layer..weight.data, and layer.bias.data, or use <a href=\"https://pytorch.org/docs/stable/nn.init.html\">nn.init<a> on layer.weight, layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "ed7c8a12-cdc3-4220-9491-71600785abcc"
   },
   "outputs": [],
   "source": [
    "class SDFNet(nn.Module):\n",
    "    def __init__(self, ninputchannels, dropout=0.2, gamma=0, sal_init=False, eik=False):\n",
    "        super(SDFNet, self).__init__()\n",
    "        ## Prepare the layers\n",
    "        ## Don't forget to initialize your weights correctly.\n",
    "\n",
    "        ## gamma, sal_init, eik are for later\n",
    "        self.gamma=gamma\n",
    "        self.eik = eik\n",
    "        \n",
    "\n",
    "\n",
    "        #custom weights init\n",
    "\n",
    "    def forward(self,x):\n",
    "        ## Logic of the neural network\n",
    "        ## You can add dropout if you want\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "b0b32746-6a2c-4c25-93b1-24fbac4f7690"
   },
   "source": [
    "### Loss function\n",
    "\n",
    "The loss is computed by sampling random points in the ambient space (set X), computing their ground truth SDF (using part one), and computing the distance between computed and ground truth sdf:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}_{x \\sim X} [|\\text{clamp}(u_\\theta(x), \\delta) - \\text{clamp}(\\text{SDF}_{\\text{gt}}(x), \\delta)|]\n",
    "$$\n",
    "\n",
    "where $\\text{clamp}(x, \\delta) := \\min(\\delta, \\max(−\\delta, x))$ (you can use torch.clamp). To understand the signification of parameter $\\delta$, read carefully paragraph 3 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "ebda59ec-e59e-4b50-a2a3-bda8a3d8221a"
   },
   "outputs": [],
   "source": [
    "def evaluate_loss(net, pts_gt, sdf_gt, device, lpc, batch_size=2000, delta = 0.1):\n",
    "    ## For this function, you need to sample batch_size number of points\n",
    "    ## From pts_gt. Evaluate the sdf at those points and compute the loss\n",
    "    ## compared to sdf_gt (be careful to select the same points between pts_gt and sdf_gt)\n",
    "\n",
    "    # Select points\n",
    "    \n",
    "\n",
    "    # compute and store the losses\n",
    "    loss = \n",
    "\n",
    "    # append all the losses\n",
    "    lpc.append(float(loss.item()))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "db4de69e-a85d-4e32-b3e4-993e45ce6263"
   },
   "source": [
    "### Training the SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "50183176-a41a-43a8-94fb-71f058cad8c8"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_normalized_pointcloud(point_cloud, margin=0.05):\n",
    "    ## Return the same point cloud, scaled such that \n",
    "    ## x,y,z values are between -1+margin and 1-margin\n",
    "    #compute the enclosing grid\n",
    "\n",
    "    #normalize the points\n",
    "    return pc_normed\n",
    "\n",
    "def compute_gt_sdf(point_cloud, normals, n_points=1000000):\n",
    "    ## Sample a n_points points in with XYZ coordinates between -1 and 1\n",
    "    ## Then use compute_sdf to get sdf_gt\n",
    "\n",
    "    p_norm = get_normalized_pointcloud(point_cloud, margin=0.00001)\n",
    "     #preparing gt points:\n",
    "\n",
    "    gtp = \n",
    "    sdf_gt = compute_sdf(p_norm, normals, gtp)\n",
    "    return sdf_gt, gtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "8bdd38cf-77d9-473b-9411-b221b81351b7"
   },
   "outputs": [],
   "source": [
    "n_points = 100000\n",
    "sdf_gt, gtp = compute_gt_sdf(pc, normals, n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "105ecb10-da59-4719-947f-67e6088d4cee",
    "outputId": "fd0d150a-a54f-404c-d1d1-94c40bb2fa83"
   },
   "outputs": [],
   "source": [
    "print(device)\n",
    "print(sdf_gt.shape, gtp.shape) ## Should be same shape\n",
    "print(np.isclose(gtp.max(), 1, 1e-3), np.isclose(gtp.min(), -1, 1e-3)) ## Should be equal to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "5d388ce8-3c03-4923-bd97-ebb2e6325a6a"
   },
   "outputs": [],
   "source": [
    "def training_sdf(sdf_gt, gtp):\n",
    "    geomnet = SDFNet(3)\n",
    "    geomnet.to(device)\n",
    "    gtpoints = torch.from_numpy(gtp).float().to(device)\n",
    "    gtsdf = torch.from_numpy(sdf_gt).float().to(device)\n",
    "\n",
    "    lpc = []\n",
    "\n",
    "    optim = torch.optim.Adam(params = geomnet.parameters(), lr=1e-5)\n",
    "\n",
    "    nepochs=10000\n",
    "    pbar = tqdm(total=nepochs,\n",
    "                desc=\"Training\")\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        loss = evaluate_loss(geomnet, gtpoints, gtsdf, device, lpc, delta = 0.1, batch_size=2500)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if epoch % 100 == 0:\n",
    "        #     print(f\"Epoch {epoch}/{nepochs} - loss : {loss.item()}\")\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        pbar.update(1)\n",
    "    return lpc, geomnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8ce06bf5a67840f79c0d6bde734f2ed4",
      "c4b4dfc8c710402bbcdbb44e506f7823",
      "29255944e05e486e97462a0f2df09fd1",
      "3038d1154fae492e8627438f9688abec",
      "aa3d93019f41449a96ca8b627bba409f",
      "94f637204a8847d5aa63c6cbba1d9584",
      "c83dae32f44448d187d3f27406846808",
      "5b63cd9c94534986a87d072788c6a3f5",
      "43661a3d263e48b0a10aab2c375fbb50",
      "99f4e4ad068e4cb3b50a11aa6de8318a",
      "122eff20624a459a9878a4d05b198504"
     ]
    },
    "id": "2b3d25b5-98f2-431d-b865-4d26b179537e",
    "outputId": "8c359775-45b3-489b-f631-ca82c000930b"
   },
   "outputs": [],
   "source": [
    "loss_, net_sdf = training_sdf(sdf_gt, gtp)\n",
    "## If the training is slow (hours), change you execution environment to GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "befa5482-c1c5-40f3-80ea-2986cea7b1c7",
    "outputId": "4dc96f82-ae31-43f4-80fa-1cb899830f95"
   },
   "outputs": [],
   "source": [
    "# Check that the network learned something\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.yscale('log')\n",
    "plt.plot(loss_, label = 'Point cloud loss ({:.2f})'.format(loss_[-1]))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Reconstruct the shape\n",
    "Code the function compute_deepsdf that compute sdf on a grid using a trained sdf network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "e1f1f2ff-9737-42bc-8a1d-f247acea17e1"
   },
   "outputs": [],
   "source": [
    "def compute_deepsdf(net, grid_size=40):\n",
    "    net.eval()\n",
    "    \n",
    "\n",
    "    v = # point cloud definition (more than one_line, reshape it to (something, 3))\n",
    "    queries = torch.from_numpy(v).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        distance = net(queries).detach().cpu().numpy()\n",
    "    u = np.reshape(distance,(grid_size,grid_size,grid_size))\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978,
     "referenced_widgets": [
      "e78d9c9c6024421d8ebf899dfe9ed253",
      "fd86e89f2ae34840930e97b98257ee2d",
      "00eb5a564a8142fd8aaa5a1bfe279867",
      "2cf64c8ba4f446b38cc2b1f92922cb60",
      "c6340c75041b460c8440f20699d7b6cb",
      "334daf4803474cf6a3d88c5c2fefdb49",
      "07b60692e5db42c4b77a5bc9e5696c6b",
      "6aa4f2df540342328a880e5761a52850",
      "c6be0df512eb4da7af5e218b862fd1ae",
      "6b8801d6968a4dc8a599e286bad8e156",
      "815fe0bfb3334b5ca394a067456dcc23",
      "9e390bb2a45045a0bea292300d4422b5",
      "8a8db73d67154ce5bd44bbeb846d7359",
      "927def9a13a14f00a008e7765fb9dd78"
     ]
    },
    "id": "79cec4a2-2347-4368-869d-a4797a63a8fb",
    "outputId": "2ae3cbf6-56c9-48f1-8c68-ddaa909076cd"
   },
   "outputs": [],
   "source": [
    "u = compute_deepsdf(net_sdf, 40)\n",
    "vertices, triangles = mcubes.marching_cubes(u,0)\n",
    "mesh = TriMesh(vertices, triangles)\n",
    "mcubes.export_obj(vertices, triangles, 'result_deepsdf.obj')\n",
    "plu.plot(mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "d9be6066-adbf-4ef8-9cec-103a933d58de"
   },
   "source": [
    "# Unsigned Distance Function\n",
    "\n",
    "In this case (paper <a href=\"https://arxiv.org/pdf/1911.10414\">link</a>), the objective is to learn directly on raw point clouds, without pre-processing to predicts normals/orientation of the shape. To reach this objective, the authors notice the following:\n",
    "- Using the usigned distance function (absolute value of the predicted SDF) is then necessary\n",
    "- Carefully choosing the points where to predict the distance is crucial\n",
    "- Weights initialization is to be changed\n",
    "\n",
    "The modification to SDF is simple : the loss is now computed by sampling points around each data point $x_i$ , following a centered Gaussian distribution of variance $\\sigma²$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\sum_i \\mathbb{E}_{x \\sim \\mathcal{N}(x_i, \\sigma^2)}[(|u_{\\theta}(x)| - |\\text{SDF}_{\\text{GT}}](x)|)^2)]\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is a parameter that you can play with, and $\\text{SDF}_{\\text{GT}}(x)$ is simply $\\text{dist}(x_i, x)$.\n",
    "\n",
    "The last linear layer is now initialized too, with weights following $\\mathcal{N}\\left(0, 2\\sqrt{\\pi}\\right)$ law and bias initialized to -1. The last layer activation is now $\\phi(a) = \\text{tanh}(a) + \\gamma a$. Gamma parameter now equals to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "re66I8b9bSVG"
   },
   "source": [
    "## Neural network\n",
    "- Modify SDFNet with sal_init, such that when sal_init=True, the last layer is initialized properly\n",
    "- Take in account gamma parameter in the network logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Loss function \n",
    "Implement the SAL loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "id": "ef5cedb8-b629-434b-8180-d7d713bb79fe"
   },
   "outputs": [],
   "source": [
    " def evaluate_loss_sal(net, p, sigma, device, losses,batch_size=5000):\n",
    "    ## Sample batch_size points, and then sample a random point around each point\n",
    "\n",
    "    #sample points  around each of the samples\n",
    "\n",
    "    \n",
    "\n",
    "    # evaluate distances and compute the loss\n",
    "    # compute and store the losses\n",
    "    loss = \n",
    "    losses.append(loss.item())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "id": "8a0cec68-0e84-4bdb-99bd-3b031fee9af1"
   },
   "outputs": [],
   "source": [
    "def training_sal(point_cloud, loss_function, sigma=0.02):\n",
    "    geomnet = SDFNet(3, gamma=0.5, sal_init=True)\n",
    "    geomnet.to(device)\n",
    "\n",
    "    pc_norm = get_normalized_pointcloud(point_cloud)\n",
    "    points_torch = torch.from_numpy(pc_norm).float().to(device)\n",
    "\n",
    "    lpc = []\n",
    "\n",
    "    optim = torch.optim.Adam(params = geomnet.parameters(), lr=1e-4)\n",
    "\n",
    "    nepochs=5000\n",
    "    pbar = tqdm(total=nepochs,\n",
    "                desc=\"Training\")\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        loss = loss_function(geomnet, points_torch, sigma, device, lpc, batch_size=5000)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if epoch % 100 == 0:\n",
    "        #     print(f\"Epoch {epoch}/{nepochs} - loss : {loss.item()}\")\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "    return lpc, geomnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "7595d1d2d56640ad8a2b028d0af270f9",
      "ef4250deb7be4c5db17a28dc05566477",
      "580bfdbf3d91487593d7075061a2dd85",
      "de9fe104a3bf4d968e0b2a14fec64d3f",
      "4a39316fe18743639eae95ae2512c320",
      "1c626e98e1a0446e8d70b53d74e0958f",
      "d71b0598ce5f4be1bf75021d13826ff1",
      "c6afaf3678374b0fb25ff3dfd9b9b9f5",
      "5adb54f552ee4641bc9a8005dcc6331b",
      "1533304550944d8d95be6c339fc5f52c",
      "26658e529d884939a63a46946fb3b6ed"
     ]
    },
    "id": "85f7c7ba-6728-43e8-a800-4189075c512d",
    "outputId": "935b7880-663c-4988-d9c2-fe091a8a9d4e"
   },
   "outputs": [],
   "source": [
    "# If you get an error: did you modify SDFNet according to the instructions?\n",
    "loss_sal, net_sal = training_sal(pc, evaluate_loss_sal, sigma=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "1cfb9856-af01-407e-b387-2ad404d4fbe6",
    "outputId": "b3bd12ed-a60f-4615-af5b-403521714259"
   },
   "outputs": [],
   "source": [
    "# Check that the network learned something\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.yscale('log')\n",
    "plt.plot(loss_sal, label = 'Point cloud loss ({:.2f})'.format(loss_[-1]))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978,
     "referenced_widgets": [
      "816c6ca394f945e99612d29c4504b9d6",
      "44ac3e738dd14856b0c4ebac02c4a6d8",
      "2488a84fc9504a1aa25132ae8f0da227",
      "c399d60303c341e8bb6325c860ab4959",
      "21d7ca720cbd4564a2a5570807da4f53",
      "d33f6725b97c40b594d90b8470126b85",
      "e341313ad08d4122b0165faec7573670",
      "4b9f628316c9498fb32a6fc5adaa4def",
      "cbd0be1818c14d56b3653a1ead09fcf5",
      "28e2959b55e843a6888ef9b6237fb6c0",
      "b5c05ae02b624062839e62062271caba",
      "474056ac15dd4b3687dfd435a52ceca7",
      "48a99ce4d6e84123a293a091d5dc9bce",
      "0bd798a3dd084cb585a969b1fd106644"
     ]
    },
    "id": "a178da71-9471-4bff-b4ad-879a79a6ec8c",
    "outputId": "43685019-2aa5-4a72-ddfe-897f8a3385fc"
   },
   "outputs": [],
   "source": [
    "u = compute_deepsdf(net_sal, 40)\n",
    "vertices, triangles = mcubes.marching_cubes(u,0)\n",
    "mesh = TriMesh(vertices, triangles)\n",
    "plu.plot(mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "id": "a09fe661-0a7e-49d7-91c8-e1bf752f0ac6"
   },
   "source": [
    "The produced signed distances using the proposed are too smooth and can't overfit a single shape. Therefore, the authors propose to learn the $L_0$ unsigned distance, by minimizing:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\sum_i \\mathbb{E}_{x \\sim \\mathcal{N}(x_i, \\sigma^2)}[||u_{\\theta}(x)| - 1|] + \\mathbb{E}_{x \\in \\mathcal{X}}[|u_{\\theta}(x)|],\n",
    "$$\n",
    "\n",
    "i.e. we want the distance to be $1$ outside of the surface, and $0$ on the surface.\n",
    "\n",
    "Write the function evaluate_loss_sal_l0 below accordingly, and launch a new training to see the effects on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "id": "e1f1f3a3-f102-4bb8-942e-cd15145e7357"
   },
   "outputs": [],
   "source": [
    "def evaluate_loss_sal_l0(net, p, sigma, device, losses,batch_size=5000):\n",
    "    ## Do the sampling and evaluations\n",
    "\n",
    "    # compute and store the losses\n",
    "    loss =  \n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "72d26aede05b4fe4bea5f10ff85dc31f",
      "8c004aa288324b8c8617fbc423480388",
      "e6c32731ffd94a6f995b0ed0b6b1ffd0",
      "1c090379b8b444bc939ad461faae49d6",
      "1c434f0793934a709b4d5233440a170b",
      "529ddc48de0941bbbfff3d70615ff8d3",
      "6bc487dc19884862a2d7412d87d04e54",
      "95271048ab31446b9402bf0aeda5b24d",
      "492ff3d421e346a08c14ffcfe5936ed4",
      "1f878730ef9b47ae8928f4e0263ce5d2",
      "b5ab6562d2a74fecac72001b39875c2e"
     ]
    },
    "id": "fc552b1e-10b9-47ff-a4e3-2a17443e5886",
    "outputId": "703136c7-8583-4dbb-8726-3f31581a4f30"
   },
   "outputs": [],
   "source": [
    "loss_sal_0, net_sal_0 = training_sal(pc, evaluate_loss_sal_l0, sigma=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "8906da72-95e6-496f-a613-3280125d0727",
    "outputId": "38c37046-af55-4896-9b42-20768871cc7b"
   },
   "outputs": [],
   "source": [
    "# Check that the network learned something\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.yscale('log')\n",
    "plt.plot(loss_sal_0, label = 'Point cloud loss ({:.2f})'.format(loss_sal_0[-1]))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978,
     "referenced_widgets": [
      "32dc21c1ea6e460597628818270577d0",
      "f96246e78f1241d4a3bd704c5bce4122",
      "d20d3179ed2146b5896cff8e44d6a8e8",
      "e7411a12b6e74aa38b8993650bb01f9a",
      "15da18c8b43446e6858cb894cf0f9bca",
      "1ad24d722d9e4bee94b8f27d3db788c8",
      "49da82c6bb59404ab91092e0d87c12d7",
      "0ef5fa1fc9d94389bf544de1d184ffcb",
      "e5b98d80b35c4ea4afc5a349a33dd33d",
      "5cf9d8a7f8244396b95182325f948b20",
      "96642999f3d84e48b06360efda7b7d53",
      "31793eea58c6499a9cf5c2e546b99100",
      "e214829b89954685acf6c145ed2f2ca0",
      "096cb49fc65448e7bef6e949e98071a3"
     ]
    },
    "id": "be1e41ee-1efd-4f15-a209-89820e62334b",
    "outputId": "f68ae747-8fb1-46b9-d19a-dde027b24d1f"
   },
   "outputs": [],
   "source": [
    "u = compute_deepsdf(net_sal_0, 40)\n",
    "vertices, triangles = mcubes.marching_cubes(u,0)\n",
    "mesh = TriMesh(vertices, triangles)\n",
    "plu.plot(mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "id": "1675226e-0287-4d89-9c43-372190fdcfc4"
   },
   "source": [
    "You can play with the sigma parameter to improve the results (see the paper to choose it wisely)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
