{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "7e3b4c18"
   },
   "source": [
    "In this lab, we will build a model to perform segmentation of RNA molecules, seen as surfaces.\n",
    "\n",
    "![](https://www.lix.polytechnique.fr/Labo/Robin.Magnet/INF631/rna_image.png)\n",
    "\n",
    "For this, we will first build a network able to process these surfaces, and then train it on the dataset.\n",
    "We will additionnally try some small variations and check their effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "R34bkvYoDFGe"
   },
   "source": [
    "# Environment and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "iF1oxQPdDKAD"
   },
   "source": [
    "Let's download the dataset and additional necessary packages.\n",
    "\n",
    "Please use a GPU for this Lab. You can obtain one with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "dueiL2hoowVF"
   },
   "outputs": [],
   "source": [
    "!python --version\n",
    "!pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "lPtegTJLDBKi"
   },
   "outputs": [],
   "source": [
    "!pip install potpourri3d\n",
    "!pip install git+https://github.com/skoch9/meshplot.git\n",
    "!pip install pythreejs\n",
    "\n",
    "!wget https://www.lix.polytechnique.fr/Labo/Robin.Magnet/INF631/RNADataset.zip\n",
    "!wget https://www.lix.polytechnique.fr/Labo/Robin.Magnet/INF631/material_TD3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "AAnyCzTqqvTk"
   },
   "outputs": [],
   "source": [
    "!unzip -qq RNADataset\n",
    "!unzip -qq material_TD3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "cetELQFGD8fx"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import diffusion_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "8d5f2279"
   },
   "source": [
    "# Part I - Implementing Diffusion DiffusionNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "ea693dcc"
   },
   "source": [
    "## Goal\n",
    "\n",
    "In this assignment we will focus on building [DiffusionNet](https://arxiv.org/pdf/2012.00888.pdf), which was presented during the lecture.\n",
    "This network uses differential geometry tools to compute per-vertex features which can later be used for several tasks such as classification, segmentation or matching.\n",
    "\n",
    "Recall that the DiffusionNet architechture can be visualized as follows :\n",
    "\n",
    "![](https://www.lix.polytechnique.fr/Labo/Robin.Magnet/INF631/TD3/DiffusionNet.PNG)\n",
    "\n",
    "\n",
    "Therefore, following this image, we will build the following objects using PyTorch:\n",
    "1. **Precomputed operators**. Most of them have been introduced in TD1, the gradient matrix is provided in the `diffusion_utils.py` file provided with this TD.\n",
    "2. **Spatial Diffusion**. Using again TD1 and in particular results on spectral diffusion.\n",
    "3. **Spatial Gradient features**. Using gradient matrices\n",
    "4. **Per-vertex MLP**. Basic PyTorch\n",
    "5. **DiffusionNet block** and **DiffusionNet** which consists in assembling the previous 4 parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "e3677cc6"
   },
   "source": [
    "## Reminders on PyTorch\n",
    "[Pytorch](https://pytorch.org/docs/stable/index.html) is a standard library to build and train neural networks. Two essential components are auto-differentiation and GPU support with parallelization of operations.\n",
    "\n",
    "A [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) object is very similar to a `numpy.array`, and most existing operations in numpy have identical equivalent in PyTorch. However behaviors can be slightly different so always check the documentation ! Using built-in PyTorch functions instead of numpy enables automatic gradient computation.\n",
    "\n",
    "Note that PyTorch operations are built to be significantly accelerated on GPU by using parallel computing. This implies that the first dimension of a `torch.Tensor` is most often interpreted as a batch size and operations are applied *in parallel* to each element in the batch **independently**.\n",
    "For instance, given a tensor `A` of size `[M,N]` and a tensor `X` of size `[B,N,P]`, the output of `A @ X` will be of size `[B,M,P]`, where the matrix multiplication by `A` is applied to each of the `X[i]` in parallel.\n",
    "\n",
    "In DiffusionNet, some blocks (*eg* the diffusion block) can be applied to multiple surfaces in parallel so batches are made of surfaces. On the contrary the MLP or gradient features blocks are applied to each vertex independantly so the batches are made of vertices. This will require some particular care when coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "13555438"
   },
   "source": [
    "## 1 - Implementing Diffusion Net\n",
    "\n",
    "In this section, we seek to implement a version of DiffusionNet which only handles oriented triangle meshes.\n",
    "\n",
    "It will be impossible for you to test the ouptut of the functions before the complete pipeline is built. Since shapes are provided in the description of each function, you should at least ensure each module outputs a tensor of the right shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "b506dc73"
   },
   "source": [
    "## 1.1 - Some utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "6a6e35a0"
   },
   "source": [
    "We here code 2 utility functions used in the diffusion block, which allow to project  (resp. unproject) functions to (resp. from) the spectral basis **taking the batch dimension** into account.\n",
    "\n",
    "For simplicity, we use a batch size = $1$ in this TD as it's tricky to combine functions defined on a mesh with $n_1$ vertices with functions defined on another mesh with $n_2$ vertices.\n",
    "\n",
    "\n",
    "**Projection**: Recall from TD1, that given functions on a mesh stored in a matrix $\\mathbf{f}\\in\\mathbb{R}^{n\\times p}$, the projection into the spectral basis of size $K$ can be done via the matrix multiplication $\\Phi^\\top A \\mathbf{f}$, where $\\Phi\\in\\mathbb{R}^{n\\times K}$ are the eigenvectors of the laplacian of the shape and A the area matrix of the shape.\n",
    "\n",
    "**Reverse projection**: Recall from TD1, that given spectral coefficients of functions on a mesh stored in a matrix $\\alpha\\in\\mathbb{R}^{K\\times p}$, the corresponding functions on the mesh are defined as $\\Phi \\alpha \\in\\mathbb{R}^{n\\times p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "f75cdc70"
   },
   "source": [
    "### Question 1\n",
    "**Compute the projection and reverse projection functions, which take batch of functions**\n",
    "\n",
    "**Tips**:\n",
    "1. If `A` is to torch Tensor of shape `(B,N,M)`, then `A.mT` does batch-wise matrix transpose and has shape `(B,M,N)`\n",
    "2. Given a diagonal matrix $A$ and a function $f\\in\\mathbb{R}^n$, then $Af = \\left(A_{ii}f_i\\right)$, where $A_{ii}$ are the diagonal values of $A$ (vertex areas in our case). We therefore directly use the diagonal values as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "414ef071"
   },
   "outputs": [],
   "source": [
    "def project_to_basis(x, evecs, vertex_areas):\n",
    "    \"\"\"\n",
    "    Project an input sinal x to the spectral basis.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    -------------------\n",
    "    x            : (B, n, p) Tensor of input\n",
    "    evecs        : (B, n, K) Tensor of eigenvectors\n",
    "    vertex_areas : (B, n,) vertex areax\n",
    "\n",
    "    Output\n",
    "    -------------------\n",
    "    projected_values : (B, K, p) Tensor of coefficients in the basis\n",
    "    \"\"\"\n",
    "    evecs_trans = evecs * vertex_areas[:, :, None]\n",
    "    res = torch.einsum(\"ijk, ijl -> ikl\", evecs_trans, x)  ## TODO\n",
    "    return res\n",
    "\n",
    "\n",
    "def unproject_from_basis(coeffs, evecs):\n",
    "    \"\"\"\n",
    "    Transform input coefficients in basis into a signal on the complete shape.\n",
    "\n",
    "    Parameters\n",
    "    -------------------\n",
    "    coefs : (B, K, p) Tensor of coefficients in the spectral basis\n",
    "    evecs : (B, n, K) Tensor of eigenvectors\n",
    "\n",
    "    Output\n",
    "    -------------------\n",
    "    decoded_values : (B, n, p) values on each vertex\n",
    "    \"\"\"\n",
    "    return torch.einsum(\"ijk, ikl -> ijl\", evecs, coeffs)  ## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "f54bcb15"
   },
   "source": [
    "## 1.2 - Spectral Diffusion module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "51283b7b"
   },
   "source": [
    "The DiffusionModule in Diffusion takes as input a mesh $M$ and set of functions $\\left(f_1,\\dots, f_p\\right)$  on the mesh - that is $f_j:M\\to\\mathbb{R}$ - and outputs $\\left(g_1,\\dots, g_p\\right)$ with $g_j:M\\to\\mathbb{R}$ and where $g_j$ the the **diffused version** of $f_j$ after time $t_j$.\n",
    "\n",
    "Note that each feature function $f_j$ is diffused for time $t_j$ where the time parameter **depends on the index of the function**.\n",
    "DiffusionNet proposes to **learn each time parameter** $t_1,\\dots, t_p$. There is therefore $p$ learnable parameters in this module.\n",
    "\n",
    "\n",
    "In order to compute Diffusion efficiently, we will use the spectral diffusion approximation introduced in TD1.\n",
    "\n",
    "Recall from that TD1 that the Laplacian eigenvectors of a mesh made of $N$ vertices can stored in a matrix $\\Phi\\in\\mathbb{R}^{N\\times K}$.\n",
    "Given an initial function $f_0$, its (spectral) diffused version $f_t$ after time $t$ can be written $f_t = \\Phi \\alpha_t$ with\n",
    "$$\n",
    "(\\alpha_t)_j = \\exp(-t\\lambda_j)\\beta_j\n",
    "$$\n",
    "\n",
    "where $\\beta=\\Phi^\\top A f_0 \\ \\in\\mathbb{R}^K$ is the projection of $f_0$ in the basis, and $A$ the diagonal area matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "0fef4fab"
   },
   "source": [
    "### Question 2\n",
    "**Fill the SpectralDiffusion module which diffuses features with learnable time parameters.**\n",
    "\n",
    "**Clue:**\n",
    "1. Use [`nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html) to create learnable parameters in a module\n",
    "2. Use [`nn.init.constant_`](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.constant_) to initialize time parameters to 0\n",
    "3. Be careful about the dimension of inputs, outputs, intermediate tensors, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "id": "ae726841"
   },
   "outputs": [],
   "source": [
    "class SpectralDiffusion(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        \"\"\"\n",
    "        Initializes the module with time parameters to 0.\n",
    "\n",
    "        Parameters\n",
    "        ------------------\n",
    "        n_channels : int - number of input feature functions\n",
    "        \"\"\"\n",
    "        # This runs the __init__ function of nn.Module\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        ## TODO DEFINE AND INITIALIZE THE Diffusion times as learnable parameters.\n",
    "        self.diffusion_times = nn.Parameter(\n",
    "            torch.ones((1, n_channels)) * 1e-3, requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, evals, evecs, vertex_areas):\n",
    "        \"\"\"\n",
    "        Given input features x and information on the current meshes\n",
    "        return diffused versions of the features.\n",
    "\n",
    "        Parameters\n",
    "        ------------------------\n",
    "        x     : (B, n, p) batch of input features. p = self.n_channels\n",
    "        evals : (B, K,) batch of eigenvalues\n",
    "        evecs : (B, n, K) batch of eigenvectors\n",
    "        vertex_areas : (B, n,) batch of vertex areax\n",
    "\n",
    "\n",
    "        Output\n",
    "        ------------------------\n",
    "        x_diffuse : diffused version of each input feature\n",
    "        \"\"\"\n",
    "        # Remove negative diffusion times\n",
    "        with torch.no_grad():\n",
    "            self.diffusion_times.data = torch.clamp(self.diffusion_times, min=1e-8)\n",
    "\n",
    "        ## TODO DIFFUSE x\n",
    "        proj_features = project_to_basis(x, evecs, vertex_areas)\n",
    "        coeffs = torch.exp(-evals * self.diffusion_times)[:, :, None] * proj_features\n",
    "        x_diffused = unproject_from_basis(coeffs, evecs)\n",
    "        return x_diffused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "43e0be0d"
   },
   "source": [
    "## 1.3 Gradient Features module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "82c8be59"
   },
   "source": [
    "The gradient feature module allows to compute features from the gradient of the diffused feature. This module treats **each vertex independantly**. The batch size is here the number of vertices and not the shape as a whole.\n",
    "\n",
    "This modules takes as input **the gradient of each feature** at a vertex, and outputs **a real number for each feature** for this vertex.\n",
    "\n",
    "\n",
    "The gradient of the features is a **vector field**, so a pair of numbers at each vertex. In practice, the value at a vertex $v$ is stored as a complex number $z\\in\\mathbb{C}^n$. Because there are $p$ different features for each vertex, we concatenate the $p$ gradients (one for each feature) and obtain a per-vertex vector field $w_v\\in\\mathbb{C}^{p}$ as **input**.\n",
    "\n",
    "The gradient feature module therefore takes a vertex embedding $w_v$ as input and outputs a *real-valued* embedding $g_v\\in\\mathbb{R}^{p}$ where\n",
    "\n",
    "$$\n",
    "g_v = \\tanh\\left(\\langle w_v, B w_v\\rangle_{\\mathbb{C}}\\right)\n",
    "$$\n",
    "with $\\tanh$ and $\\langle\\rangle_{\\mathbb{C}}$ applied **element-wise**.\n",
    "\n",
    "\n",
    "Where $B$ is a **complex and learnable** matrix, and $\\langle\\rangle_{\\mathbb{C}}$ can be seen as the $\\mathbb{R}^2$ inner product when identifying $\\mathbb{C}$ with $\\mathbb{R}^2$. This means for $a,b,c,d\\in\\mathbb{R}$ that:\n",
    "$$\n",
    "\\langle a+ib, c+id\\rangle_{\\mathbb{C}} = ac + bd\n",
    "$$\n",
    "\n",
    "\n",
    "In summary, the gradient feature module:\n",
    " - Takes $w_v\\in\\mathbb{C}^{p}$ as input\n",
    " - Computes $g_v$ using the formula above\n",
    " - Has $B\\in\\mathbb{C}^p$ as a unique parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "5a3428d3"
   },
   "source": [
    "### Question 3\n",
    "**Implement the Gradient Feature Module**\n",
    "\n",
    "**Tips**:\n",
    "\n",
    "1. In practice, use $w_v\\in\\mathbb{R}^{p\\times 2}$ and $B=B_{re}+iB_{im}$ where both matrix are real values $p\\times p$ matrices. Compute separately the real and imaginary part of $Bw_v$, then apply the inner product and hyperbolic tangent element-wise.\n",
    "2. A learnable matrix multipication can be represented as a [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer\n",
    "3. You shouldn't spend too much time on this, don't hesitate to reach out for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "id": "e10fc6fe"
   },
   "outputs": [],
   "source": [
    "class SpatialGradient(nn.Module):\n",
    "    \"\"\"\n",
    "    Module which computes g_v from vertex embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        \"\"\"\n",
    "        Initializes the module.\n",
    "\n",
    "        Parameters\n",
    "        ------------------\n",
    "        n_channels : int - number of input feature functions\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        # Real and Imaginary part of B\n",
    "        self.B_re = nn.Linear(self.n_channels, self.n_channels, bias=False)\n",
    "        self.B_im = nn.Linear(self.n_channels, self.n_channels, bias=False)\n",
    "\n",
    "    def forward(self, vects):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------------------\n",
    "        Vects : (N, P, 2) per-vertex vector field (w_v)\n",
    "\n",
    "        Output\n",
    "        ---------------------\n",
    "        features : (N, P) per-vertex scalar field\n",
    "        \"\"\"\n",
    "        vects_re = vects[..., 0]  # (N,P) real part of w_v\n",
    "        vects_im = vects[..., 1]  # (N,P) imaginary part of w_v\n",
    "\n",
    "        B_mult_re = self.B_re(vects_re)\n",
    "        B_mult_re -= self.B_im(vects_im)\n",
    "\n",
    "        B_mult_im = self.B_re(vects_im)\n",
    "        B_mult_im += self.B_im(vects_re)\n",
    "        ## TODO Perform forward pass\n",
    "\n",
    "        return torch.tanh(vects_re * B_mult_re + vects_im * B_mult_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "be747853"
   },
   "source": [
    "## 1.4 MLP module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "d2b89301"
   },
   "source": [
    "The MLP module is a simple multi-layer perceptron which acts on **each vertex independantly**.\n",
    "\n",
    "This can be customizable using custom hidden layer sizes and droupout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "0248a6c1"
   },
   "source": [
    "### Question 4\n",
    "**Code the MiniMLP module (with dropout and activation)**\n",
    "\n",
    "**Tip**:\n",
    "1. Activation is not applied to the last layer\n",
    "2. Given a list `layer_list` of `torch.nn` modules, one can generate a large layer using `nn.Sequential(*layer_list)`. See the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) for more info\n",
    "3. Batch is made of per-vertex embedding, not per surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "24c96134"
   },
   "outputs": [],
   "source": [
    "class MiniMLP(nn.Sequential):\n",
    "    \"\"\"\n",
    "    A simple MLP with activation and potential dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_sizes, dropout=False, activation=nn.ReLU):\n",
    "        \"\"\"\n",
    "        Activation and dropout is applied after all layer BUT the last one\n",
    "\n",
    "        Parameters\n",
    "        ---------------------------\n",
    "        layer_size : list of ints - list of sizes of the MLP\n",
    "        dropout    : book - whether to add droupout or not\n",
    "        activation : nn.module : activation function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layer_list = []\n",
    "\n",
    "        ## TODO FILL THE LAYER LIST\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            if dropout:\n",
    "                layer_list.append(nn.Dropout())\n",
    "            layer_list.append(nn.Linear(layer_sizes[i - 1], layer_sizes[i]))\n",
    "\n",
    "            if i < len(layer_sizes) - 1:\n",
    "                layer_list.append(activation())\n",
    "        self.layer = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        --------------------\n",
    "        x : (n, p) - input features, batch size is the number of vertices !\n",
    "\n",
    "        Output\n",
    "        -------------------\n",
    "        y : (n,p') - output features\n",
    "        \"\"\"\n",
    "        # NOTHING TO DO HERE\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "7bdd46c1"
   },
   "source": [
    "## 1.5 DiffusionNet Block\n",
    "\n",
    "![](https://www.lix.polytechnique.fr/Labo/Robin.Magnet/INF631/TD3/DiffusionNet.PNG)\n",
    "\n",
    "Each diffusion block contains 3 main modules which were implemented in the preceeding three questions. Namely, (1) Learned spectral diffusion, (2) Gradient Feature module and (3) Point-wise features from the MLP module applied sequentially.\n",
    "\n",
    "Note the following points:\n",
    "1. Input and output are per-vertex scalars\n",
    "2. The width (or number of features) is the same as input and output\n",
    "3. The Diffusion module uses the input features as input.\n",
    "4. The Spatial Gradient module uses the **gradient** of the output of the Diffusion module as input\n",
    "5. The MiniMLP uses the **concatenation** of the input features, the output of the Diffusion module and the output of the Spatial Gradient module as input. Its input size is 3 times the number of features.\n",
    "6. The input features are **added** to the output using a **residual connection**.\n",
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "**Assemble the Diffusion + Gradient + MLP modules to construct diffusion block using the image and explanation above**\n",
    "\n",
    "**Tip**\n",
    " - We pre-filled the code to compute the gradient of the features\n",
    " - We only provide the size of the **hidden** layers of the MLP. You must add the input and output dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "Rs62kw02zKC5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "eb895954"
   },
   "outputs": [],
   "source": [
    "class DiffusionNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Diffusion block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, mlp_hidden_dims, dropout=True):\n",
    "        \"\"\"\n",
    "        Initializes the module.\n",
    "\n",
    "        Parameters\n",
    "        ------------------\n",
    "        n_channels      : int - number of feature functions (serves as both input and output)\n",
    "        mlp_hidden_dims : list of int - sizes of HIDDEN layers of the miniMLP.\n",
    "                          You should add the input and output dimension to it.\n",
    "        \"\"\"\n",
    "        super(DiffusionNetBlock, self).__init__()\n",
    "\n",
    "        # Specified dimensions\n",
    "        self.n_channels = n_channels\n",
    "        self.mlp_hidden_dims = mlp_hidden_dims\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Diffusion block\n",
    "        # TODO DEFINE THE 3 SUBPARTS\n",
    "        self.spectral_diffusion = SpectralDiffusion(n_channels)\n",
    "        self.spatial_gradient = SpatialGradient(n_channels)\n",
    "        self.mini_mlp = MiniMLP(\n",
    "            [3 * n_channels, n_channels, n_channels, n_channels], dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x_in, vertex_areas, evals, evecs, gradX, gradY):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -------------------\n",
    "        x_in         : (B,n,p) - Tensor of input signal.\n",
    "        vertex_areas : (B,n) - Tensor of vertex areas\n",
    "        evals        : (B, K,) batch of eigenvalues\n",
    "        evecs        : (B, n, K) batch of eigenvectors\n",
    "        gradX        : Half of gradient matrix, sparse real tensor with dimension [B,N,N]\n",
    "        gradY        : Half of gradient matrix, sparse real tensor with dimension [B,N,N]\n",
    "\n",
    "        Output\n",
    "        -------------------\n",
    "        x_out : (B,n,p) - Tensor of output signal.\n",
    "        \"\"\"\n",
    "\n",
    "        # Manage dimensions\n",
    "        B = x_in.shape[0]  # batch dimension\n",
    "\n",
    "        # Diffusion block\n",
    "        x_diffuse = self.spectral_diffusion(\n",
    "            x_in, evals, evecs, vertex_areas\n",
    "        )  # DIFFUSED X_in  # (B, N, p)\n",
    "\n",
    "        # Compute the batch of gradients\n",
    "        x_grads = []  # Manually loop over the batch\n",
    "        for b in range(B):\n",
    "            # gradient after diffusion\n",
    "            x_gradX = torch.mm(gradX[b, ...], x_diffuse[b, ...])\n",
    "            x_gradY = torch.mm(gradY[b, ...], x_diffuse[b, ...])\n",
    "\n",
    "            x_grads.append(torch.stack((x_gradX, x_gradY), dim=-1))\n",
    "\n",
    "        x_grad = torch.stack(x_grads, dim=0)  # (B, N, P, 2)\n",
    "\n",
    "        # TODO EVALUATE GRADIENT FEATURES\n",
    "        out_grad = self.spatial_gradient(x_grad)\n",
    "\n",
    "        # TODO APPLY THE MLP TO THE CONCATENATED FEATURES\n",
    "        cat_feats = torch.cat((x_diffuse, out_grad, x_in), dim=-1)\n",
    "        out_mlp = self.mini_mlp(cat_feats)\n",
    "        # TODO APPLY THE RESIDUAL CONNECTION\n",
    "\n",
    "        return out_mlp + x_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "00697162"
   },
   "source": [
    "## 1.6 - DiffusionNet\n",
    "\n",
    "Let's check again the architecture:\n",
    "![](https://www.lix.polytechnique.fr/Labo/Robin.Magnet/INF631/TD3/DiffusionNet.PNG)\n",
    "\n",
    "DiffusionNet takes as input per-vertex features of size $p_{in}$. Is first uses a linear layer to transform theses features into $p$-dimensional features, where $p$ is the width of DiffusionNet. There are then multiple blocks of DiffusionNetBlock to produce new features. Similarly to input features, a last linear layer transforms the $p$ dimensional features into vertex features of size $p_{out}$\n",
    "\n",
    "### Question 6\n",
    "\n",
    "**Build the DiffusionNet. It consists of an MLP in the first layer to reac with `N_block` of `DiffusionNetBlock` which was implemented in the previous cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "id": "55cb0acf"
   },
   "outputs": [],
   "source": [
    "class DiffusionNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        p_in,\n",
    "        p_out,\n",
    "        n_channels=128,\n",
    "        N_block=4,\n",
    "        last_activation=None,\n",
    "        mlp_hidden_dims=None,\n",
    "        dropout=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Construct a DiffusionNet.\n",
    "        Parameters\n",
    "        --------------------\n",
    "        p_in            : int - input dimension of the network\n",
    "        p_out           : int - output dimension  of the network\n",
    "        n_channels      : int - dimension of internal DiffusionNet blocks (default: 128)\n",
    "        N_block         : int - number of DiffusionNet blocks (default: 4)\n",
    "        last_activation : int - a function to apply to the final outputs of the network, such as torch.nn.functional.log_softmax\n",
    "        mlp_hidden_dims : list of int - a list of hidden layer sizes for MLPs (default: [C_width, C_width])\n",
    "        dropout         : bool - if True, internal MLPs use dropout (default: True)\n",
    "        \"\"\"\n",
    "\n",
    "        super(DiffusionNet, self).__init__()\n",
    "\n",
    "        ## Store parameters\n",
    "\n",
    "        # Basic parameters\n",
    "        self.p_in = p_in\n",
    "        self.p_out = p_out\n",
    "        self.n_channels = n_channels\n",
    "        self.N_block = N_block\n",
    "\n",
    "        # Outputs\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "        # MLP options\n",
    "        if mlp_hidden_dims == None:\n",
    "            mlp_hidden_dims = [n_channels, n_channels]\n",
    "        self.mlp_hidden_dims = mlp_hidden_dims\n",
    "        self.dropout = dropout\n",
    "\n",
    "        ## TODO SETUP THE NETWORK (LINEAR LAYERS + BLOCKS)\n",
    "\n",
    "        self.blocks = []  # TOFILL\n",
    "        self.blocks.append(nn.Linear(self.p_in, self.n_channels))\n",
    "        for i in range(N_block):\n",
    "            self.blocks.append(DiffusionNetBlock(n_channels, mlp_hidden_dims, dropout))\n",
    "        self.blocks.append(nn.Linear(self.n_channels, self.p_out))\n",
    "\n",
    "        self.net = nn.ModuleList(self.blocks)\n",
    "\n",
    "    def forward(\n",
    "        self, x_in, vertex_areas, evals=None, evecs=None, gradX=None, gradY=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Progapate a signal through the network.\n",
    "        Can handle input without batch dimension (will add a dummy dimension to set batch size to 1)\n",
    "\n",
    "        Parameters\n",
    "        --------------------\n",
    "        x_in         : (n,p) or (B,n,p) - Tensor of input signal.\n",
    "        vertex_areas : (n,) or (B,n) - Tensor of vertex areas\n",
    "        evals        : (B, K,) or (K,) batch of eigenvalues\n",
    "        evecs        : (B, n, K) or (n, K) batch of eigenvectors\n",
    "        gradX        : Half of gradient matrix, sparse real tensor with dimension [N,N] or [B,N,N]\n",
    "        gradY        : Half of gradient matrix, sparse real tensor with dimension [N,N] or [B,N,N]\n",
    "\n",
    "        Output\n",
    "        -----------------------\n",
    "        x_out (tensor):    Output with dimension [N,C_out] or [B,N,C_out]\n",
    "        \"\"\"\n",
    "\n",
    "        ## Check dimensions, and append batch dimension if not given\n",
    "        if x_in.shape[-1] != self.p_in:\n",
    "            raise ValueError(\n",
    "                f\"DiffusionNet was constructed with p_in={self.p_in}, \"\n",
    "                f\"but x_in has last dim={x_in.shape[-1]}\"\n",
    "            )\n",
    "        N = x_in.shape[-2]\n",
    "\n",
    "        if len(x_in.shape) == 2:\n",
    "            appended_batch_dim = True\n",
    "\n",
    "            # add a batch dim to all inputs\n",
    "            x_in = x_in.unsqueeze(0)  # (B, N, P)\n",
    "            vertex_areas = vertex_areas.unsqueeze(0)  # (B, N)\n",
    "            if evals != None:\n",
    "                evals = evals.unsqueeze(0)  # (B,K)\n",
    "            if evecs != None:\n",
    "                evecs = evecs.unsqueeze(0)  # (B,N,K)\n",
    "            if gradX != None:\n",
    "                gradX = gradX.unsqueeze(0)  # (B,N,N)\n",
    "            if gradY != None:\n",
    "                gradY = gradY.unsqueeze(0)  # (B,N,N)\n",
    "\n",
    "        elif len(x_in.shape) == 3:\n",
    "            appended_batch_dim = False\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"x_in should be tensor with shape (n,p) or (B,n,p)\")\n",
    "\n",
    "        ##  TODO PROCESS THE INPUTS\n",
    "        x_p = self.blocks[0](x_in)\n",
    "        for i in range(self.N_block):\n",
    "            x_p = self.blocks[i + 1](x_p, vertex_areas, evals, evecs, gradX, gradY)\n",
    "        x_out = self.blocks[-1](x_p)\n",
    "\n",
    "        # Remove batch dim if we added it\n",
    "        if appended_batch_dim:\n",
    "            x_out = x_out.squeeze(0)  # (N, p_out)\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "604ee348-5246-4af2-bc12-0f6d4f6eeba8"
   },
   "source": [
    "# Part II - RNA segmentation.\n",
    "\n",
    "**Given meshes of the molecular envelopes for RNA molecules, gathered from the PDB database, we will be segementing them. That is, each vertex is assigned a ground-truth segmentation label according to ~120 functional categories. See [1] for more details. We use this data as a representative task for 3D machine learning on surfaces, predicting the functional segmentation from only the molecule surface shape using DiffusionNet.**\n",
    "\n",
    "\n",
    "[1] https://hal.inria.fr/hal-02167454v2/document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "1f1f66cd"
   },
   "source": [
    "## 1. Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "161711e5"
   },
   "source": [
    "Here is some code to visualize the training data with segmentation of the surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "db20c6ce"
   },
   "outputs": [],
   "source": [
    "from rna_dataset import RNAMeshDataset\n",
    "from mesh_utils.mesh import TriMesh\n",
    "import matplotlib.pyplot as plt\n",
    "import plot_utils as plu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "id": "a0e53c70"
   },
   "outputs": [],
   "source": [
    "root_dir = \"./RNADataset/\"\n",
    "\n",
    "train_dataset = RNAMeshDataset(root_dir, train=True, num_eig=128, op_cache_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "id": "ad874a54"
   },
   "outputs": [],
   "source": [
    "# This loads all operators\n",
    "\n",
    "data1 = train_dataset[19]\n",
    "mesh1 = TriMesh(data1[\"vertices\"].numpy(), data1[\"faces\"].numpy())\n",
    "\n",
    "data2 = train_dataset[37]\n",
    "mesh2 = TriMesh(data2[\"vertices\"].numpy(), data2[\"faces\"].numpy())\n",
    "\n",
    "\n",
    "## You can see what kind of information is provided\n",
    "print(data1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "id": "7d571da0"
   },
   "outputs": [],
   "source": [
    "cmap1 = plt.get_cmap(\"jet\")(data1[\"labels\"].numpy() / (train_dataset.n_classes - 1))[\n",
    "    :, :3\n",
    "]\n",
    "cmap2 = plt.get_cmap(\"jet\")(data2[\"labels\"].numpy() / (train_dataset.n_classes - 1))[\n",
    "    :, :3\n",
    "]\n",
    "\n",
    "# plu.plot(mesh1, cmap1)\n",
    "plu.plot(mesh2, cmap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "x6pKXi1DJD9g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "ac96083a"
   },
   "source": [
    "### Load training and validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "5913d4fa"
   },
   "source": [
    "All operators and eigenfunctions are precomputed and saved in a cache folder. Computing the cache can take a few minutes.\n",
    "\n",
    "If you are using colab, you might want first to mount your drive and save the cache there (You can change the `op_cache_dir`) so that you don't have to recompute it everytime. Or either compute the cache once, download it and upload it again everytime. This might be faster than recomputing all along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "id": "8ef35ad0"
   },
   "outputs": [],
   "source": [
    "# WARNING: Do not change this cell\n",
    "\n",
    "from rna_dataset import RNAMeshDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root_dir = \"./RNADataset/\"\n",
    "op_cache_dir = \"./RNADataset/cache\"\n",
    "num_eig = 128\n",
    "\n",
    "train_dataset = RNAMeshDataset(\n",
    "    root_dir, train=True, num_eig=num_eig, op_cache_dir=op_cache_dir\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=None,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "valid_dataset = RNAMeshDataset(\n",
    "    root_dir, train=False, num_eig=num_eig, op_cache_dir=op_cache_dir\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "e14ebabd"
   },
   "source": [
    "## 2 -  Training the network\n",
    "\n",
    "In order to easily run multiple experiments, we will use a class `Trainer` to run the complete training and testing code.\n",
    "\n",
    "This way we can simply change parameters as arguments when building the class and run the training easily.\n",
    "\n",
    "### Question 7: Fill the trainer class\n",
    "\n",
    "This function is mostly pre-filled. Here are the parts you should implement yourself:\n",
    "\n",
    "1. In the `__init__` function, **build DiffusionNet** using the `model_cfg` dictionary you will provide as input. An example of model config is the following:\n",
    "\n",
    "```python\n",
    "model_cfg = {'inp_feat': 'xyz',  # Type of input Features (xyz, HKS, WKS)\n",
    "              'num_eig': 32,  # Number of eigenfunctions to use for Spectral Diffusion\n",
    "              'p_in': 3,  # Number of input features\n",
    "              'p_out': train_dataset.n_classes,  # Number of output features\n",
    "              'N_block': 4,  # Number of DiffusionNetBlock\n",
    "              'n_channels': 128}  # Width of the network\n",
    "```\n",
    "\n",
    "2. In the `__init__` function, **define the loss**. Note that segmentation is essentially multi-label classification. The number of classes is given by `train_dataset.n_classes`.\n",
    "\n",
    "3. Fill in the `forward_step` `train_epoch`, `valid_epoch` methods. Note this is pretty simple and depends on your choice of loss / activation function combination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "id": "74697012"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusionnet_cls,\n",
    "        model_cfg,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        device=\"cuda\",\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        num_epochs=200,\n",
    "        lr_decay_every=50,\n",
    "        lr_decay_rate=0.5,\n",
    "        log_interval=10,\n",
    "        save_dir=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        diffusionnet_cls: (nn.Module) class of the DiffusionNet model\n",
    "        model_cfg: (dict) keyword arguments for model\n",
    "        train_loader: (torch.utils.DataLoader) DataLoader for training set\n",
    "        valid_loader: (torch.utils.DataLoader) DataLoader for validation set\n",
    "        device: (str) 'cuda' or 'cpu'\n",
    "        lr: (float) learning rate\n",
    "        weight_decay: (float) weight decay for optimiser\n",
    "        num_epochs: (int) number of epochs\n",
    "        lr_decay_every: (int) decay learning rate every this many epochs\n",
    "        lr_decay_rate: (float) decay learning rate by this factor\n",
    "        log_interval: (int) print training stats every this many iterations\n",
    "        save_dir: (str) directory to save model checkpoints\n",
    "        \"\"\"\n",
    "\n",
    "        # TOD build the network from the model_cfg\n",
    "        self.model = diffusionnet_cls(\n",
    "            model_cfg[\"p_in\"],\n",
    "            model_cfg[\"p_out\"],\n",
    "            model_cfg[\"n_channels\"],\n",
    "            model_cfg[\"N_block\"],\n",
    "            model_cfg[\"dropout\"],\n",
    "        )\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss(label_smoothing=0.2)  ### USE A MEANINGFUL LOSS\n",
    "\n",
    "        ## THIS PART JUST STORES SOME OTHER PARAMETERS\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        self.lr_decay_every = lr_decay_every\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.log_interval = log_interval\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_accs = []\n",
    "        self.test_accs = []\n",
    "\n",
    "        self.inp_feat = model_cfg.get(\"inp_feat\", \"xyz\")\n",
    "        self.num_eig = model_cfg.get(\"num_eig\", 128)\n",
    "        if not self.inp_feat in [\"xyz\", \"hks\", \"wks\"]:\n",
    "            raise ValueError(\"inp_feat must be one of xyz, hks, wks\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def forward_step(\n",
    "        self, verts, faces, frames, vertex_area, L, evals, evecs, gradX, gradY\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform a forward step of the model.\n",
    "\n",
    "        Args:\n",
    "            verts (torch.Tensor): (N, 3) tensor of vertex positions\n",
    "            faces (torch.Tensor): (F, 3) tensor of face indices\n",
    "            frames (torch.Tensor): (N, 3, 3) tensor of tangent frames.\n",
    "            vertex_area (torch.Tensor): (N, N) sparse Tensor of vertex areas.\n",
    "            L (torch.Tensor): (N, N) sparse Tensor of cotangent Laplacian.\n",
    "            evals (torch.Tensor): (num_eig,) tensor of eigenvalues.\n",
    "            evecs (torch.Tensor): (N, num_eig) tensor of eigenvectors.\n",
    "            gradX (torch.Tensor): (N, N) tensor of gradient in X direction.\n",
    "            gradY (torch.Tensor): (N, N) tensor of gradient in Y direction.\n",
    "\n",
    "        Returns:\n",
    "            pred (torch.Tensor): (N, p_out) tensor of predicted labels.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.inp_feat == \"xyz\":\n",
    "            features = verts\n",
    "        elif self.inp_feat == \"hks\":\n",
    "            features = self.compute_HKS(evecs, evals, self.num_eig, n_feat=32)\n",
    "        elif self.inp_feat == \"wks\":\n",
    "            features = self.compute_WKS(evecs, evals, self.num_eig, n_feat=32)\n",
    "\n",
    "        preds = self.model(\n",
    "            features, vertex_area, evals=evals, evecs=evecs, gradX=gradX, gradY=gradY\n",
    "        )\n",
    "\n",
    "        # MAYBE ADD ACTIVATION\n",
    "        return preds\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        Train the network for one epoch\n",
    "        \"\"\"\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for i, batch in enumerate(tqdm(self.train_loader, \"Train epoch\")):\n",
    "            verts = batch[\"vertices\"].to(self.device)\n",
    "            faces = batch[\"faces\"].to(self.device)\n",
    "            frames = batch[\"frames\"].to(self.device)\n",
    "            vertex_area = batch[\"vertex_area\"].to(self.device)\n",
    "            L = batch[\"L\"].to(self.device)\n",
    "            evals = batch[\"evals\"].to(self.device)\n",
    "            evecs = batch[\"evecs\"].to(self.device)\n",
    "            gradX = batch[\"gradX\"].to(self.device)\n",
    "            gradY = batch[\"gradY\"].to(self.device)\n",
    "            labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            preds = self.forward_step(\n",
    "                verts, faces, frames, vertex_area, L, evals, evecs, gradX, gradY\n",
    "            )\n",
    "            # MAYBE DO SOMETHING TO THE PREDS\n",
    "\n",
    "            # COMPUTE THE LOSS\n",
    "            loss = self.loss(preds, labels)  # TODO\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # COMPUTE TRAINING ACCURACY\n",
    "            pred_labels = torch.argmax(preds, dim=-1)  # TODO GET PREDICTED LABELS\n",
    "\n",
    "            n_correct = (\n",
    "                pred_labels.eq(labels).sum().item()\n",
    "            )  # number of correct predictions\n",
    "            train_acc += n_correct / labels.shape[0]\n",
    "\n",
    "        return train_loss / len(self.train_loader), train_acc / len(self.train_loader)\n",
    "\n",
    "    def valid_epoch(self):\n",
    "        \"\"\"\n",
    "        Run a validation epoch\n",
    "        \"\"\"\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        print(\"Start val epoch\")\n",
    "        for i, batch in enumerate(self.valid_loader):\n",
    "            # READ BATCH\n",
    "            verts = batch[\"vertices\"].to(self.device)\n",
    "            faces = batch[\"faces\"].to(self.device)\n",
    "            frames = batch[\"frames\"].to(self.device)\n",
    "            vertex_area = batch[\"vertex_area\"].to(self.device)\n",
    "            L = batch[\"L\"].to(self.device)\n",
    "            evals = batch[\"evals\"].to(self.device)\n",
    "            evecs = batch[\"evecs\"].to(self.device)\n",
    "            gradX = batch[\"gradX\"].to(self.device)\n",
    "            gradY = batch[\"gradY\"].to(self.device)\n",
    "            labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "            # TODO PERFORM FORWARD STEP\n",
    "            preds = self.forward_step(\n",
    "                verts, faces, frames, vertex_area, L, evals, evecs, gradX, gradY\n",
    "            )\n",
    "            # MAYBE DO SOMETHING TO THE PREDS\n",
    "\n",
    "            # Compute Loss - THIS DEPENDS ON YOUR CHOICE OF LOSS\n",
    "            loss = self.loss(preds, labels)  ##\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute ACCURACCY\n",
    "            pred_labels = torch.argmax(preds, dim=-1)  ## TODO\n",
    "\n",
    "            n_correct = (\n",
    "                pred_labels.eq(labels).sum().item()\n",
    "            )  # number of correct predictions\n",
    "            val_acc += n_correct / labels.shape[0]\n",
    "        print(\"End val epoch\")\n",
    "        return val_loss / len(self.valid_loader), val_acc / len(self.valid_loader)\n",
    "\n",
    "    def run(self):\n",
    "        os.makedirs(\"./models\", exist_ok=True)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "\n",
    "            if epoch % self.lr_decay_every == 0:\n",
    "                self.adjust_lr()\n",
    "\n",
    "            train_ep_loss, train_ep_acc = self.train_epoch()\n",
    "            self.train_losses.append(train_ep_loss)\n",
    "            self.train_accs.append(train_ep_acc)\n",
    "\n",
    "            if epoch % self.log_interval == 0:\n",
    "                val_loss, val_acc = self.valid_epoch()\n",
    "                torch.save(\n",
    "                    self.model.state_dict(),\n",
    "                    os.path.join(self.save_dir, \"model_latest.pth\"),\n",
    "                )\n",
    "                print(\n",
    "                    f\"Epoch: {epoch:03d}/{self.num_epochs}, \"\n",
    "                    f\"Train Loss: {train_ep_loss:.4f}, \"\n",
    "                    f\"Train Acc: {1e2*train_ep_acc:.2f}%, \"\n",
    "                    f\"Val Loss: {val_loss:.4f}, \"\n",
    "                    f\"Val Acc: {1e2*val_acc:.2f}%\"\n",
    "                )\n",
    "        torch.save(\n",
    "            self.model.state_dict(), os.path.join(self.save_dir, \"model_final.pth\")\n",
    "        )\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        We only test the first two shapes of validation set.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        test_seg_meshes = []\n",
    "\n",
    "        for i, batch in enumerate(self.valid_loader):\n",
    "            verts = batch[\"vertices\"].to(self.device)\n",
    "            faces = batch[\"faces\"].to(self.device)\n",
    "            frames = batch[\"frames\"].to(self.device)\n",
    "            vertex_area = batch[\"vertex_area\"].to(self.device)\n",
    "            L = batch[\"L\"].to(self.device)\n",
    "            evals = batch[\"evals\"].to(self.device)\n",
    "            evecs = batch[\"evecs\"].to(self.device)\n",
    "            gradX = batch[\"gradX\"].to(self.device)\n",
    "            gradY = batch[\"gradY\"].to(self.device)\n",
    "            labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "            preds = self.forward_step(\n",
    "                verts, faces, frames, vertex_area, L, evals, evecs, gradX, gradY\n",
    "            )\n",
    "            pred_labels = torch.max(preds, dim=1).indices\n",
    "\n",
    "            test_seg_meshes.append(\n",
    "                [\n",
    "                    TriMesh(verts.cpu().numpy(), faces.cpu().numpy()),\n",
    "                    pred_labels.cpu().numpy(),\n",
    "                ]\n",
    "            )\n",
    "            if i == 1:\n",
    "                break\n",
    "\n",
    "        cmap1 = plt.get_cmap(\"jet\")(test_seg_meshes[0][-1] / (146))[:, :3]\n",
    "        cmap2 = plt.get_cmap(\"jet\")(test_seg_meshes[1][-1] / (146))[:, :3]\n",
    "\n",
    "        plu.double_plot(test_seg_meshes[0][0], test_seg_meshes[1][0], cmap1, cmap2)\n",
    "        # return plot_multi_meshes(test_seg_meshes, cmap='vert_colors')\n",
    "\n",
    "    def adjust_lr(self):\n",
    "        lr = self.lr * self.lr_decay_rate\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    def compute_HKS(self, evecs, evals, num_eig, n_feat):\n",
    "        \"\"\"\n",
    "        Compute the HKS features for each vertex in the mesh.\n",
    "        Args:\n",
    "            evecs (torch.Tensor): (N, K) tensor of eigenvectors\n",
    "            evals (torch.Tensor): (K,) tensor of eigenvectors\n",
    "            num_eig (int): number of eigenvalues to use\n",
    "            n_feat (int): number of features to compute\n",
    "\n",
    "        Returns:\n",
    "            hks (torch.Tensor): (N, n_feat) tensor of HKS features\n",
    "        \"\"\"\n",
    "        abs_ev = torch.sort(torch.abs(evals)).values[:num_eig]\n",
    "\n",
    "        t_list = np.geomspace(\n",
    "            4 * np.log(10) / abs_ev[-1], 4 * np.log(10) / abs_ev[1], n_feat\n",
    "        )\n",
    "        t_list = torch.from_tensor(t_list.astype(np.float32)).to(device=evecs.device)\n",
    "\n",
    "        evals_s = abs_ev\n",
    "\n",
    "        coefs = torch.exp(-t_list[:, None] * evals_s[None, :])  # (num_T,K)\n",
    "\n",
    "        natural_HKS = np.einsum(\"tk,nk->nt\", coefs, evecs[:, :num_eig].square())\n",
    "\n",
    "        inv_scaling = coefs.sum(1)  # (num_T)\n",
    "\n",
    "        return (1 / inv_scaling)[None, :] * natural_HKS\n",
    "\n",
    "    def compute_WKS(self, evecs, evals, num_eig, n_feat):\n",
    "        \"\"\"\n",
    "        Compute the WKS features for each vertex in the mesh.\n",
    "\n",
    "        Args:\n",
    "            evecs (torch.Tensor): (N, K) tensor of eigenvectors\n",
    "            evals (torch.Tensor): (K,) tensor of eigenvectors\n",
    "            num_eig (int): number of eigenvalues to use\n",
    "            n_feat (int): number of features to compute\n",
    "\n",
    "        Returns:\n",
    "            wks: torch.Tensor: (N, n_feat) tensor of WKS features\n",
    "        \"\"\"\n",
    "        abs_ev = torch.sort(torch.abs(evals)).values[:num_eig]\n",
    "\n",
    "        e_min, e_max = np.log(abs_ev[1]), np.log(abs_ev[-1])\n",
    "        sigma = 7 * (e_max - e_min) / n_feat\n",
    "\n",
    "        e_min += 2 * sigma\n",
    "        e_max -= 2 * sigma\n",
    "\n",
    "        energy_list = torch.linspace(e_min, e_max, n_feat)\n",
    "\n",
    "        evals_s = abs_ev\n",
    "\n",
    "        coefs = torch.exp(\n",
    "            -torch.square(energy_list[:, None] - torch.log(torch.abs(evals_s))[None, :])\n",
    "            / (2 * sigma**2)\n",
    "        )  # (num_E,K)\n",
    "\n",
    "        natural_WKS = np.einsum(\"tk,nk->nt\", coefs, evecs[:, :num_eig].square())\n",
    "\n",
    "        inv_scaling = coefs.sum(1)  # (num_E)\n",
    "        return (1 / inv_scaling)[None, :] * natural_WKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "fc715ac6"
   },
   "source": [
    "### Training and visualising segmentation results.\n",
    "\n",
    "Simply execute the next two cells. There are no TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "id": "e6ee991c"
   },
   "outputs": [],
   "source": [
    "# Let's define the trainer\n",
    "\n",
    "my_trainer = Trainer(\n",
    "    DiffusionNet,\n",
    "    model_cfg,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    save_dir=\"./models\",\n",
    ")\n",
    "# my_trainer.run()\n",
    "\n",
    "# Let's visualize the predicted segmentation at initialization\n",
    "my_trainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "id": "42270cb3"
   },
   "outputs": [],
   "source": [
    "# We can now run the training\n",
    "my_trainer.run()\n",
    "\n",
    "# And visualize\n",
    "my_trainer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "a7542796"
   },
   "source": [
    "### Question 8: Ablation studies\n",
    "\n",
    "#### This is the final question of this lab which involves several subquestions. The objective is to understand the utility of different components of DiffusionNet block built during this session. More specifically, we will make one change at a time to the above network to understand how the segmentation accuracy varies.\n",
    "\n",
    "\n",
    "1. Remove the learned diffusion module such that a Diffusion block consists of MLP + Gradient features.\n",
    "\n",
    "2. Remove the Gradient feature module such that a Diffusion block consists of MLP + Gradient features.\n",
    "\n",
    "3. Instead of feeding in the `xyz` coordinate as features, what happens if either of HKS and/or WKS is used as a feature?\n",
    "\n",
    "\n",
    "The first two question consists of reimplementing `DiffusionNet` module such that it incorporates aforementioned changes into the blocks. Either create a new class or add arguments which you'll use in the `model_cfg` dictionary.\n",
    "\n",
    "For the last question, please pay attention go `inp_feat` and `p_in`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "id": "c506f07c"
   },
   "outputs": [],
   "source": [
    "### TODO:\n",
    "my_trainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "id": "rAxiVyNIPZ2M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "597b225ada1a8e31f8ba91733f376a075451b7f2318f668a1392c07697d374c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
