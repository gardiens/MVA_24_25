{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEXKOV6-uTU8"
   },
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5XgBkCpuTU9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_dim) # 28 * 28 is the size of MNIST images\n",
    "        self.fc_mu = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 28 * 28) # 28 * 28 is still the size of MNIST images\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        x_recon = torch.sigmoid(self.fc2(h))\n",
    "        return x_recon\n",
    "\n",
    "# Define the VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_dim, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(hidden_dim, z_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim)\n",
    "\n",
    "    def parameterization_trick(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.parameterization_trick(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "def loss_function(x, x_recon, mu, logvar):\n",
    "    recon_loss = nn.functional.binary_cross_entropy(x_recon, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(-1 + logvar - mu.pow(2) - torch.exp(logvar))\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 400\n",
    "z_dim = 20\n",
    "batch_size = 128\n",
    "num_epochs = 80\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "vae = VAE(hidden_dim, z_dim).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjxN5a-RuTU_"
   },
   "source": [
    "# Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f32RMcsKuTVA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_samples(x_recon):\n",
    "    _, axs = plt.subplots(8, 8, figsize=(8, 8))\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            axs[i, j].imshow(x_recon[i * 8 + j].detach().numpy().reshape(28, 28), cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-9Z48MTtuTVA",
    "outputId": "bfc66aa5-d6fd-46ec-d1bd-e3714d6cb47e"
   },
   "outputs": [],
   "source": [
    "vae.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, logvar = vae(data)\n",
    "        if batch_idx == 0:\n",
    "            x_disp = x_recon[:64, :]\n",
    "        loss = loss_function(data, x_recon, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch in [0, 9, 79]:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {train_loss / len(train_loader.dataset)}')\n",
    "        plot_samples(x_disp.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt6GMz3AuTVA"
   },
   "source": [
    "# Question 17 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbW3RYGDuTVA"
   },
   "outputs": [],
   "source": [
    "prompt = [\"a sitting cat  \"]\n",
    "import torch\n",
    "height = 512  # default height of Stable Diffusion\n",
    "width = 512  # default width of Stable Diffusion\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "generator = torch.manual_seed(0)  # Seed generator to create the inital latent noise\n",
    "batch_size = len(prompt)\n",
    "num_inference_steps = 25  # Number of denoising steps\n",
    "list_inference_step=[5,10,25,50,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwozWYINuTVA"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGKrPJ0NuTVA"
   },
   "outputs": [],
   "source": [
    "torch_device = \"cuda\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r91FL2yvuTVB",
    "outputId": "1a9963ee-f053-41c8-d64a-defb158fd900"
   },
   "outputs": [],
   "source": [
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "2 ** (len(vae.config.block_out_channels) - 1) == 8\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ")\n",
    "latents = latents.to(torch_device)\n",
    "input_latents=latents.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "03174ee704bf41758d590bbd4ec8ce19",
      "261e2912ca8b48ecb28e5ce933b0dd42",
      "d2d489e652a94651a23d8fdd8c9818fa",
      "5883c102bea440499f6254709572ffab",
      "8b4059a26ab94ea3803640138c305822",
      "40ac445873f449caae71a89f5252cb39",
      "59f5f5e045c64d0b92d0298f7a9cf827",
      "a1a0c419a36e4425bc5407ee0a149bed",
      "507a4744540e4433ab51bdcefa3b4a73",
      "8c4af5689d5148188902db189d90b251",
      "c47b30afe9d3448399ce90cf63999bb7"
     ]
    },
    "id": "vbqg6KZOuTVB",
    "outputId": "aab5e1af-2d5d-4679-f1e6-1e7661ab2f93"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def run_pipeline(input_latents,scheduler,text_embeddings,guidance_scale,num_inference_steps):\n",
    "\n",
    "  latents = input_latents.clone() * scheduler.init_noise_sigma\n",
    "  scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "  for t in tqdm(scheduler.timesteps):\n",
    "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "      latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "      latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "      # predict the noise residual\n",
    "      with torch.no_grad():\n",
    "          noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "      # perform guidance\n",
    "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond )\n",
    "\n",
    "      # compute the previous noisy sample x_t -> x_t-1\n",
    "      latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "\n",
    "\n",
    "  # scale and decode the image latents with vae\n",
    "  latents = 1 / 0.18215 * latents\n",
    "  with torch.no_grad():\n",
    "      image = vae.decode(latents).sample\n",
    "  return image\n",
    "from diffusers import EulerDiscreteScheduler\n",
    "scheduler=EulerDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "\n",
    "image=run_pipeline(input_latents,scheduler,text_embeddings,guidance_scale,num_inference_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "mzKgMVivuTVB",
    "outputId": "71ca10e1-7955-45f0-e9d0-ee98cb28d7d6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_img(image):\n",
    "  image = (image / 2 + 0.5).clamp(0, 1)\n",
    "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "  images = (image * 255).round().astype(\"uint8\")\n",
    "  pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "  return pil_images[0]\n",
    "show_img(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iV2bjfMKuTVB"
   },
   "outputs": [],
   "source": [
    "# euler scheduler\n",
    "euler_scheduler=[]\n",
    "scheduler=EulerDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "for num_inference_steps in list_inference_step:\n",
    "  euler_scheduler.append(run_pipeline(input_latents,scheduler,text_embeddings,guidance_scale,num_inference_steps))\n",
    "\n",
    "# euler  ancestral scheduler\n",
    "euler_ancestral_scheduler=[]\n",
    "from diffusers import EulerAncestralDiscreteScheduler\n",
    "scheduler=EulerAncestralDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "for num_inference_steps in list_inference_step:\n",
    "  euler_ancestral_scheduler.append(run_pipeline(input_latents,scheduler,text_embeddings,guidance_scale,num_inference_steps))\n",
    "#DPM scheduler\n",
    "DPM_scheduler=[]\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "scheduler=DPMSolverMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "for num_inference_steps in list_inference_step:\n",
    "  DPM_scheduler.append(run_pipeline(input_latents,scheduler,text_embeddings,guidance_scale,num_inference_steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "id": "tH0FP9aJuTVB",
    "outputId": "cad97da0-7522-4dd7-b3f2-9c3d234f8ba6"
   },
   "outputs": [],
   "source": [
    "# Define scheduler names and timesteps\n",
    "scheduler_names = [ 'DPM Solver','Euler', 'Euler Ancestral']\n",
    "timesteps = [5, 10, 25, 50, 100]\n",
    "\n",
    "# Collect all results in a list of lists for easier plotting\n",
    "all_results = [euler_scheduler, euler_ancestral_scheduler, DPM_scheduler]\n",
    "all_results = [DPM_scheduler, euler_scheduler, euler_ancestral_scheduler ]\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(len(timesteps), len(all_results), figsize=(15, 10))\n",
    "\n",
    "# Adjust layout for tighter spacing\n",
    "fig.suptitle(\"Generated Images by Scheduler and Timesteps\", fontsize=16)\n",
    "plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "# Plot each image in the corresponding subplot\n",
    "for row_idx, timestep in enumerate(timesteps):\n",
    "    for col_idx, (scheduler_name, scheduler_results) in enumerate(zip(scheduler_names, all_results)):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        ax.imshow(show_img(scheduler_results[row_idx]))  # Assuming each scheduler_result is an image\n",
    "        # Remove ticks and spines but keep ylabel\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(scheduler_name, fontsize=12)  # Add scheduler name at the top\n",
    "        if col_idx == 0:\n",
    "            # Add timestep labels on the y-axis\n",
    "            ax.set_ylabel(f\"{timestep} steps\", fontsize=12, rotation=90, labelpad=10)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o53bnJPyuTVC"
   },
   "source": [
    "# Question 18: Negative Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "54d7eb81d9c2469bbbf0c96224926bc2",
      "ba858b142f8d41b396f04db100703d9a",
      "b7440ffad42d4286b4306ad172b9957f",
      "3dc8dac7656d44bfa3562f13bd34edb1",
      "e46f0554ed40417b93259e9e2f033492",
      "371b50d034f54ce8bdfffd1918262530",
      "ec329392e67c40768da7a90144cc63d5",
      "bb500be692114c05b65bc188a1a5b372",
      "d4a05485d86c4c908b3f1feba7477e44",
      "5b7b88b4795041b8a9034ba415e68cf1",
      "41bbba70196c483d896614251857b303",
      "efe699f975ad4b899ca524fed9f0d91e",
      "63b5db59489e499085658ce67caac28d",
      "3467eeb96ec9415b8bbfa27238950af1",
      "a2a76c9c1afc4175ac8c56aca16a452c",
      "b52e0258eb174505894766e7477d0be5",
      "4c0647c762d54fcf97d3d07abf86af14",
      "e654e076c9a74a4ba32461cab9929625",
      "8cdb9314ad754ac58ba21aa89332dd84",
      "5512a6a0a009440086b67e17626a2ed6",
      "61baf6e72a014dd8a3b73fb5503c401e",
      "70912225c08f418eb82716c0ee0c63ab",
      "bc43842ac92a46d8b003c5b77dac377b",
      "eee9333b6c5c470293fddb8e5d704c6d",
      "90ffef3cc3b8409baab7dbb39938c2dd",
      "2e879e8298994518931b40c6917c0aa9",
      "4dbd8035fd524445820118dc30a3fadc",
      "d1c5a3c699a54855a33c1783c44876b6",
      "763bed8c0ae444ab9717e41c611d99e0",
      "19d5993afcf84c1394280f24e8ea2077",
      "4ca2ec004acb4aecbf62626b0883dc02",
      "6ae3f3aa9d134291a2b15ba0ab6860e2",
      "c89740ba959b46b791ae1d30636a5ae3",
      "48345e60ca824dfa9b2acd157459262e",
      "7eec9c0ec10947bcacd0e1a73caee148",
      "a960feb9f32c4189bf44086499d5ca0e",
      "89b33cb43b1640688156998a75f3c6cc",
      "d90513402b3b4e8db177562da72438e5",
      "0a9bb27d1d70402da2a856c31fe0ddb7",
      "03747d1894444087a178fa8cabbf6340",
      "7a1c1abc93b94bb09190856c2ef0a3b4",
      "79fe40b6b24848778d6dc4999e289e91",
      "e161cd4939854f58a2212dc4085f09a1",
      "4c9f7de1eb5143e08000b4fabb2a7491",
      "08bb75fa55f843f7af72c8c8d53b0632",
      "01dca716f23f46eeaf9c731a7df80164",
      "e3ea523d52054f9c98167388b76f7ae0",
      "cabf91cfe4b143a5a499bcf591a2f915",
      "a108ea7fe7164221b0e0a80f5c889e12",
      "54cbd333227c45ae8468c5207180660f",
      "6a196763389045e98f2359b681018537",
      "add52a470e5146be9bce6bb5632c8548",
      "b169904512ba4584afca91faed152cbb",
      "bd1aaebba27840b1a8824e3e7b323bdf",
      "277849edbe604111ad4c048cc7e7fb32",
      "ce3bea1f029048d9bd45078718610f18",
      "aa9ed0bde21c4e60be86f5749aa872b4",
      "45653464333542ab8cf7f81c1106836f",
      "889369cdf5954a25960ff5657274dd0d",
      "c8d51be5c13e4829bb200f0d1bf2a579",
      "05b1dd8763f24e0388225a5a1962fc10",
      "836ea9702d3c40dbbb0c56fc7199dedc",
      "9b59400499ec40ee82a9abeecbb57019",
      "3cd54fe1071c471db93964c7c0366882",
      "99f88667fb1c4e638fc0a5efc0f5eb1d",
      "ef1207344e314a4a8f5ff6983ba09cc3",
      "5915c5f49a3f444892fa0868b469f78c",
      "77cd9c3cbb9c4fe78e54639029899b15",
      "043354ce2bd341d5837a7fd739601aee",
      "354e779799f648788bf9ae9e6441639c",
      "c97e61d2a7d144ac8ac0fdea9e41ff10",
      "592cfd995d744ce4960c697f4608f451",
      "f22a1b20457a440f9ef0595c80198380",
      "ee7169bf82134fc4b7e29ad85174308f",
      "ca2c439a33564c0d8901aeff170301ca",
      "0605c2dfece146baac783419de02408a",
      "79a62b5313534cf59bfc152bc728a59c",
      "9eacb98525c14100ae9d2cd71d9a7494",
      "14f1a676c59f4af385d6d7b5d4d719c3",
      "b3176e8b7c7d4ddd8205eb5da279c4a9",
      "dc062e459a7a4adc9d8a5e3ed38f4013",
      "bb8e40fe8b59470cbc5addd43bc2d216",
      "6534396753db4b3d9689717b14c12e88",
      "9eb640950e49446ca5eb5e86ae62122d",
      "fbb941b1449148bf9ef5b67b60d998d7",
      "5e64fd6419e14af9afc0bdb1bd863d0b",
      "789424002c13481a9318f8811c2fbbcc",
      "a7ab1fe7aca14adea89919b486c0a566",
      "2c1dc8e520c24493ad43fb1e47355655",
      "1b7e4c4ea13d427a8e6043d880cfeb86",
      "b9d9f876ef4944959830ed92e603d7e3",
      "7b7ec4a004414aadb0209240b652a94e",
      "c1b25c3b17e64dce9acf1c0960a7e4e9",
      "1ebb217617b04a7780be13e329638cb0",
      "31ef2246763c4ef19e0b41ab50f95b87",
      "d66d817ede3641dca747c5ef7eadc95b",
      "9dc70b4c345f4eab881bed181af918b3",
      "64dd8251b8704b1493d5496ca7526c37",
      "1d0e0e6743ae4f7da7f8cc0192175fe7",
      "a5610c5c477d49eeb460e1b8b2c9856b",
      "7b0b7458f67845268427ee9084a5264f",
      "351c81dccfb849ecac59b73c60e5e25a",
      "a81b0727273c4f21bd74a4f17a2f504e",
      "0f5f0fc0fc264904822677ab6ad703cc",
      "96be1271c6d849ee9fc67d9e7bbead37",
      "50e4161799d94b488335c72778779a60",
      "5477d981e68c47a080a3ce49ae168bae",
      "a1e0980c013f415b9fa399572016206b",
      "e0867e7bd4764d629f98c7976ef6e250",
      "33a5549ab89041589b9c179bfda5cb89",
      "ad4eb8e2854f4e67aef85b1f5c71fa50",
      "f7ebafbf73cc4e638ff7b8ef437771f7",
      "48d6688c4e7a4359b39e5f888b094339",
      "8b966073baad4722bc0cb74c3573925d",
      "c1983750dfc74f7dbbc7a1b5aaa40be4",
      "339c52c47df4476696a27362a716d465",
      "ea5ca8d4fa0a40f3afe3f13e98e45394",
      "6cb9b504ace74b118bd7fcf3359f1498",
      "657a2f2491fe4b1998ce048d55293809",
      "889010ba98c54d54b72cc6019c3858db",
      "1f336e11c9204bff98bda5eda5a9a174"
     ]
    },
    "id": "HFanXYAgvEee",
    "outputId": "dc9579c3-904b-4968-a4bc-ab45e5d0fbed"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "from diffusers import UniPCMultistepScheduler,EulerDiscreteScheduler\n",
    "\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "torch_device = \"cuda\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SXqWPpxuTVC"
   },
   "outputs": [],
   "source": [
    "prompt = [\"a photograph of an astronaut riding a horse on earth \"]\n",
    "height = 512  # default height of Stable Diffusion\n",
    "width = 512  # default width of Stable Diffusion\n",
    "num_inference_steps = 25  # Number of denoising steps\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "generator = torch.manual_seed(0)  # Seed generator to create the inital latent noise\n",
    "batch_size = len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0laYREzuTVC"
   },
   "outputs": [],
   "source": [
    "def question_18(negative_prompt,prompt):\n",
    "    text_input = tokenizer(\n",
    "        prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    text_negative_input = tokenizer(\n",
    "        negative_prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "        text_negative_embeddings = text_encoder(text_negative_input.input_ids.to(torch_device))[0]\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings,text_negative_embeddings])\n",
    "\n",
    "    2 ** (len(vae.config.block_out_channels) - 1) == 8\n",
    "    latents = torch.randn(\n",
    "        (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "        generator=generator,\n",
    "    )\n",
    "    latents = latents.to(torch_device)\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    for t in tqdm(scheduler.timesteps):\n",
    "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "        latent_model_input = torch.cat([latents] * 3)\n",
    "\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # perform guidance\n",
    "        noise_pred_uncond, noise_pred_text,noise_pred_negative = noise_pred.chunk(3,dim=0)\n",
    "\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond ) - guidance_scale *( noise_pred_negative -noise_pred_uncond)\n",
    "\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "\n",
    "\n",
    "    # scale and decode the image latents with vae\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "\n",
    "\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    plt.imshow(pil_images[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559,
     "referenced_widgets": [
      "2088ae31f1d54749ac239ce1cf32d051",
      "6d37a53bb7554b9c8f79fbe43fcedf12",
      "42a45bca2add426ca0a5921df73310a7",
      "addddc9ba1fa4008be4f53db5b061caa",
      "2b46ae9932044d20a7809607083a150d",
      "f50c65270ce3418fb53a3c2ccdc6d925",
      "2008ed75b3a84877aaf1be73687cef93",
      "40dfb0363e494e8b9219187cf7b6a7ee",
      "e4a6db8ea5c14ae2bcea73e44d43ec31",
      "93251aec849b4d84a71e2fcec2f08672",
      "d159802aad744ff784b94bf481b0735c"
     ]
    },
    "id": "Fu4aoKX1uTVC",
    "outputId": "7162f965-da41-4151-d2ac-d608e535c8f9"
   },
   "outputs": [],
   "source": [
    "negative_prompt=[\" sky\"]\n",
    "question_18(negative_prompt,prompt)\n",
    "plt.title(f\"negative prompt: {negative_prompt[0]}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559,
     "referenced_widgets": [
      "06ca26fe378e4aabaa2758d4eb07e5ea",
      "28ea0486599b4964a09a29981eba3469",
      "64a7e88717054c65955ea0bdc01829b2",
      "f133729361354893aa4828bd58a16b95",
      "78c446f415c041c9879d7d0c01672923",
      "02879b1307c74bba8dad1f367dfde64e",
      "07cdfc02076647f2b7defda2e37971ea",
      "f3622a2baded450cb81fc3dbdd24152f",
      "1f3571f5dc0a40c3861bed848faa5f5d",
      "8e025ffeaad548e3a09759b88d72b936",
      "5852967deb824d56806a1f484180d171"
     ]
    },
    "id": "F7-RgE2EuTVC",
    "outputId": "9f2c42ae-cbb6-4c84-a9c5-e39cf10f518f"
   },
   "outputs": [],
   "source": [
    "negative_prompt=[\"sky in the upper left \"]\n",
    "question_18(negative_prompt,prompt)\n",
    "plt.title(f\"negative prompt: {negative_prompt[0]}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xs8xp1EWM2QC"
   },
   "source": [
    "# Question 22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrZWEqTIM2QD"
   },
   "outputs": [],
   "source": [
    "path_to_utils = 'pyfiles'\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(path_to_utils)\n",
    "import nmt_dataset\n",
    "import nnet_models\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybRozUj2M2QD"
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "source_lang, target_lang = 'en', 'fr'\n",
    "model_dir = 'models/{}-{}'.format(source_lang, target_lang)\n",
    "! bash download-data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJ6goaSyM2QE"
   },
   "outputs": [],
   "source": [
    "def reset_seed(seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOuwjfccM2QE"
   },
   "outputs": [],
   "source": [
    "bpe_path = os.path.join(data_dir, 'bpecodes.de-en-fr')\n",
    "\n",
    "with open(bpe_path) as bpe_codes:\n",
    "    bpe_model = BPE(bpe_codes)\n",
    "\n",
    "def preprocess(line, is_source=True, source_lang=None, target_lang=None):\n",
    "    return bpe_model.segment(line.lower())\n",
    "\n",
    "def postprocess(line):\n",
    "    return line.replace('@@ ', '')\n",
    "\n",
    "def load_data(source_lang, target_lang, split='train', max_size=None):\n",
    "    # max_size: max number of sentence pairs in the training corpus (None = all)\n",
    "    path = os.path.join(data_dir, '{}.{}-{}'.format(split, *sorted([source_lang, target_lang])))\n",
    "    return nmt_dataset.load_dataset(path, source_lang, target_lang, preprocess=preprocess, max_size=max_size)   # set max_size to 10000 for fast debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbyLR-2BM2QE"
   },
   "outputs": [],
   "source": [
    "def save_model(model, checkpoint_path):\n",
    "    dirname = os.path.dirname(checkpoint_path)\n",
    "    if dirname:\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "    torch.save(model, checkpoint_path)\n",
    "\n",
    "def train_model(\n",
    "        train_iterator,\n",
    "        valid_iterators,\n",
    "        model,\n",
    "        checkpoint_path,\n",
    "        epochs=1,\n",
    "        validation_frequency=1\n",
    "    ):\n",
    "    \"\"\"\n",
    "    train_iterator: instance of nmt_dataset.BatchIterator or nmt_dataset.MultiBatchIterator\n",
    "    valid_iterators: list of nmt_dataset.BatchIterator\n",
    "    model: instance of nnet_models.EncoderDecoder\n",
    "    checkpoint_path: path of the model checkpoint\n",
    "    epochs: iterate this many times over train_iterator\n",
    "    validation_frequency: validate the model every N epochs\n",
    "    \"\"\"\n",
    "\n",
    "    reset_seed()\n",
    "\n",
    "    best_bleu = -1\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        start = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        print('Epoch: [{}/{}]'.format(epoch, epochs))\n",
    "\n",
    "        # Iterate over training batches for one epoch\n",
    "        for i, batch in tqdm(enumerate(train_iterator), total=len(train_iterator)):\n",
    "            t = time.time()\n",
    "            running_loss += model.train_step(batch)\n",
    "\n",
    "        # Average training loss for this epoch\n",
    "        # *****START CODE\n",
    "        epoch_loss = running_loss / len(train_iterator)\n",
    "        # *****END CODE\n",
    "\n",
    "        print(\"loss={:.3f}, time={:.2f}\".format(epoch_loss, time.time() - start))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Evaluate and save the model\n",
    "        if epoch % validation_frequency == 0:\n",
    "            bleu_scores = []\n",
    "\n",
    "            # Compute BLEU over all validation sets\n",
    "            for valid_iterator in valid_iterators:\n",
    "                # *****START CODE\n",
    "                src, tgt = valid_iterator.source_lang, valid_iterator.target_lang\n",
    "                translation_output = model.translate(valid_iterator, postprocess)\n",
    "                bleu_score = translation_output.score\n",
    "                output = translation_output.output\n",
    "                # *****END CODE\n",
    "\n",
    "                with open(os.path.join(model_dir, 'valid.{}-{}.{}.out'.format(src, tgt, epoch)), 'w') as f:\n",
    "                    f.writelines(line + '\\n' for line in output)\n",
    "\n",
    "                print('{}-{}: BLEU={}'.format(src, tgt, bleu_score))\n",
    "                sys.stdout.flush()\n",
    "                bleu_scores.append(bleu_score)\n",
    "\n",
    "            # Average the validation BLEU scores\n",
    "            bleu_score = round(sum(bleu_scores) / len(bleu_scores), 2)\n",
    "            if len(bleu_scores) > 1:\n",
    "                print('BLEU={}'.format(bleu_score))\n",
    "\n",
    "            # Update the model's learning rate based on current performance.\n",
    "            # This scheduler divides the learning rate by 10 if BLEU does not improve.\n",
    "            model.scheduler_step(bleu_score)\n",
    "\n",
    "            # Save a model checkpoint if it has the best validation BLEU so far\n",
    "            if bleu_score > best_bleu:\n",
    "                best_bleu = bleu_score\n",
    "                save_model(model, checkpoint_path)\n",
    "\n",
    "        print('=' * 50)\n",
    "\n",
    "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2KyHTU5M2QG"
   },
   "outputs": [],
   "source": [
    "def get_binned_bleu_scores(model, valid_iterator):\n",
    "    # Compute and plot BLEU scores according to sequence length\n",
    "    # lengths = np.arange(0, 31, 5)\n",
    "    lengths = np.arange(4, 20, 3)\n",
    "    bleu_scores = np.zeros(len(lengths))\n",
    "\n",
    "    for i in tqdm(range(1, len(lengths)), total=len(lengths) - 1):\n",
    "        min_len = lengths[i - 1]\n",
    "        max_len = lengths[i]\n",
    "\n",
    "        tmp_data = valid_data[(valid_iterator.data['source_len'] > min_len) & (valid_iterator.data['source_len'] <= max_len)]\n",
    "        tmp_iterator = nmt_dataset.BatchIterator(tmp_data, source_lang, target_lang, batch_size, max_len=max_len)\n",
    "        bleu_scores[i] = model.translate(tmp_iterator, postprocess).score\n",
    "        print(model.translate(tmp_iterator, postprocess))\n",
    "    lengths = lengths[1:]\n",
    "    bleu_scores = bleu_scores[1:]\n",
    "\n",
    "    plt.plot(lengths, bleu_scores, 'x-')\n",
    "    plt.ylim(0, np.max(bleu_scores) + 1)\n",
    "    plt.xlabel('Source length')\n",
    "    plt.ylabel('BLEU score')\n",
    "\n",
    "    return lengths, bleu_scores\n",
    "\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Plot an encoder-decoder attention matrix\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions, cmap='bone', aspect='auto')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       [nmt_dataset.EOS_TOKEN], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' ') +\n",
    "                       [nmt_dataset.EOS_TOKEN])\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def encode_as_batch(sentence, dictionary, source_lang, target_lang):\n",
    "    # Create a batch from a single sentence\n",
    "    sentence = sentence + ' ' + nmt_dataset.EOS_TOKEN\n",
    "    tensor = dictionary.txt2vec(sentence).unsqueeze(0)\n",
    "\n",
    "    return {\n",
    "        'source': tensor,\n",
    "        'source_len': torch.from_numpy(np.array([tensor.shape[-1]])),\n",
    "        'source_lang': source_lang,\n",
    "        'target_lang': target_lang\n",
    "    }\n",
    "\n",
    "\n",
    "def get_translation(model, sentence, dictionary, source_lang, target_lang, return_output=False):\n",
    "    # Translate given sentence with given model. Also show translation outputs by Google Translate for comparison.\n",
    "    print('Source:', sentence)\n",
    "    sentence_tok = preprocess(sentence, is_source=True, source_lang=source_lang, target_lang=target_lang)\n",
    "    print('Tokenized source:', sentence_tok)\n",
    "    batch = encode_as_batch(sentence_tok, dictionary, source_lang, target_lang)\n",
    "    prediction, attn_matrix, enc_self_attn = model.eval_step(batch)\n",
    "    prediction = prediction[0]\n",
    "    prediction_detok = postprocess(prediction)\n",
    "    print('Prediction:', prediction)\n",
    "    print('Detokenized prediction:', prediction_detok)\n",
    "\n",
    "    print('Google Translate ({}->{}): {}'.format(\n",
    "        source_lang,\n",
    "        target_lang,\n",
    "        translator.translate(sentence, src=source_lang, dest=target_lang).text\n",
    "    ))\n",
    "\n",
    "    print('Google Translate on prediction ({}->{}): {}'.format(\n",
    "        target_lang,\n",
    "        source_lang,\n",
    "        translator.translate(prediction_detok, src=target_lang, dest=source_lang).text\n",
    "    ))\n",
    "\n",
    "    results = {\n",
    "        'source': sentence,\n",
    "        'source_tokens': sentence_tok.split(' ') + ['<eos>'],\n",
    "        'prediction_detok': prediction_detok,\n",
    "        'prediction_tokens': prediction.split(' '),\n",
    "    }\n",
    "\n",
    "    if attn_matrix is not None:\n",
    "        attn_matrix = attn_matrix[0].detach().cpu().numpy()\n",
    "        results['attention_matrix'] = attn_matrix\n",
    "        show_attention(sentence_tok, prediction, attn_matrix)\n",
    "\n",
    "    if enc_self_attn is not None:\n",
    "        results['encoder_self_attention_list'] = enc_self_attn\n",
    "\n",
    "    if return_output:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6RrHv_vM2QG"
   },
   "outputs": [],
   "source": [
    "multi_model_dir = os.path.join(\"models\", \"de-en-fr\")\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets for all three languages\n",
    "train_data_de_en = load_data(\"de\", \"en\", \"train\", max_size=10000)\n",
    "train_data_fr_en = load_data(\"fr\", \"en\", \"train\", max_size=10000)\n",
    "train_data_de_fr = load_data(\"de\", \"fr\", \"train\", max_size=10000)\n",
    "\n",
    "\n",
    "\n",
    "# Combine the tokenized data from all datasets\n",
    "combined_source_tokenized = pd.concat(\n",
    "    [\n",
    "        train_data_de_en[\"source_tokenized\"],\n",
    "        train_data_fr_en[\"source_tokenized\"],\n",
    "        train_data_de_fr[\"source_tokenized\"],\n",
    "    ]\n",
    ")\n",
    "combined_target_tokenized = pd.concat(\n",
    "    [\n",
    "        train_data_de_en[\"target_tokenized\"],\n",
    "        train_data_fr_en[\"target_tokenized\"],\n",
    "        train_data_de_fr[\"target_tokenized\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "source_dict = nmt_dataset.load_or_create_dictionary(\n",
    "    multi_model_dir + \"/src_dict.txt\",\n",
    "    combined_source_tokenized,\n",
    "    minimum_count=10,\n",
    "    reset=True\n",
    ")\n",
    "\n",
    "target_dict = nmt_dataset.load_or_create_dictionary(\n",
    "    multi_model_dir + \"/tgt_dict.txt\",\n",
    "    combined_target_tokenized,\n",
    "    minimum_count=10,\n",
    "    reset=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cHPI-9SM2QH"
   },
   "outputs": [],
   "source": [
    "multi_transformer_encoder = nnet_models.TransformerEncoder(\n",
    "    input_size=len(source_dict), hidden_size=512, num_layers=3, dropout=0.1, heads=8\n",
    ")\n",
    "\n",
    "multi_transformer_decoder = nnet_models.TransformerDecoder(\n",
    "    output_size=len(target_dict), hidden_size=512, num_layers=1, heads=8, dropout=0.1\n",
    ")\n",
    "\n",
    "multi_transformer_model = nnet_models.EncoderDecoder(\n",
    "    multi_transformer_encoder,\n",
    "    multi_transformer_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict,\n",
    ")\n",
    "\n",
    "normed_transformer_encoder = nnet_models.TransformerEncoder(\n",
    "    input_size=len(source_dict), hidden_size=512, num_layers=3, dropout=0.1, heads=8, normalize_before=True\n",
    ")\n",
    "\n",
    "normed_transformer_decoder = nnet_models.TransformerDecoder(\n",
    "    output_size=len(target_dict), hidden_size=512, num_layers=1, heads=8, dropout=0.1, normalize_before=True\n",
    ")\n",
    "\n",
    "normed_transformer_model = nnet_models.EncoderDecoder(\n",
    "    normed_transformer_encoder,\n",
    "    normed_transformer_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "sh_multi_transformer_encoder = nnet_models.TransformerEncoder(\n",
    "    input_size=len(source_dict), hidden_size=512, num_layers=1, dropout=0.1, heads=8\n",
    ")\n",
    "\n",
    "sh_multi_transformer_model = nnet_models.EncoderDecoder(\n",
    "    sh_multi_transformer_encoder,\n",
    "    multi_transformer_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict,\n",
    ")\n",
    "\n",
    "sh_normed_transformer_encoder = nnet_models.TransformerEncoder(\n",
    "    input_size=len(source_dict), hidden_size=512, num_layers=1, dropout=0.1, heads=8, normalize_before=True\n",
    ")\n",
    "\n",
    "sh_normed_transformer_model = nnet_models.EncoderDecoder(\n",
    "    sh_normed_transformer_encoder,\n",
    "    normed_transformer_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N3HpL-qM2QH"
   },
   "source": [
    "## Multilingual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YzpPpqIM2QI"
   },
   "outputs": [],
   "source": [
    "max_len = 30       # maximum 30 tokens per sentence (longer sequences will be truncated)\n",
    "batch_size = 512   # maximum 512 tokens per batch (decrease if you get OOM errors, increase to speed up training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogdfecSOM2QI"
   },
   "outputs": [],
   "source": [
    "def preprocess(line, is_source=True, source_lang=None, target_lang=None):\n",
    "    line = bpe_model.segment(line.lower())\n",
    "    if is_source:\n",
    "        line = \"<lang:{}> {}\".format(target_lang, line)\n",
    "    return line\n",
    "\n",
    "\n",
    "test_iterators = []\n",
    "valid_iterators = []\n",
    "train_iterators = []\n",
    "\n",
    "for src, tgt in (\n",
    "    (\"en\", \"fr\"),\n",
    "    (\"fr\", \"en\"),\n",
    "    (\"en\", \"de\"),\n",
    "    (\"de\", \"en\"),\n",
    "    (\"de\", \"fr\"),\n",
    "    (\"fr\", \"de\"),\n",
    "):\n",
    "    dataset = load_data(src, tgt, \"test\")\n",
    "    nmt_dataset.binarize(\n",
    "        dataset, source_dict=source_dict, target_dict=target_dict, sort=False\n",
    "    )\n",
    "    test_iterators.append(\n",
    "        nmt_dataset.BatchIterator(\n",
    "            dataset, src, tgt, batch_size=batch_size, max_len=max_len, shuffle=True\n",
    "        )\n",
    "    )\n",
    "    train_data = load_data(src, tgt, \"train\", max_size=10000)\n",
    "    nmt_dataset.binarize(\n",
    "        train_data, source_dict=source_dict, target_dict=target_dict, sort=False\n",
    "    )\n",
    "    train_iterators.append(\n",
    "        nmt_dataset.BatchIterator(\n",
    "            train_data, src, tgt, batch_size=batch_size, max_len=max_len, shuffle=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    valid_data = load_data(src, tgt, \"valid\")\n",
    "    nmt_dataset.binarize(\n",
    "        valid_data, source_dict=source_dict, target_dict=target_dict, sort=False\n",
    "    )\n",
    "    valid_iterators.append(\n",
    "        nmt_dataset.BatchIterator(\n",
    "            valid_data, src, tgt, batch_size=batch_size, max_len=max_len, shuffle=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_iterators = nmt_dataset.MultilingualBatchIterator(train_iterators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mac1EnMlM2QJ"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def save_model(model, checkpoint_path):\n",
    "    dirname = os.path.dirname(checkpoint_path)\n",
    "    if dirname:\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "    torch.save(model, checkpoint_path)\n",
    "\n",
    "def train_model(\n",
    "        train_iterator,\n",
    "        valid_iterators,\n",
    "        model,\n",
    "        checkpoint_path,\n",
    "        epochs=1,\n",
    "        validation_frequency=1\n",
    "    ):\n",
    "    \"\"\"\n",
    "    train_iterator: instance of nmt_dataset.BatchIterator or nmt_dataset.MultiBatchIterator\n",
    "    valid_iterators: list of nmt_dataset.BatchIterator\n",
    "    model: instance of nnet_models.EncoderDecoder\n",
    "    checkpoint_path: path of the model checkpoint\n",
    "    epochs: iterate this many times over train_iterator\n",
    "    validation_frequency: validate the model every N epochs\n",
    "    \"\"\"\n",
    "\n",
    "    reset_seed()\n",
    "\n",
    "    best_bleu = -1\n",
    "    bleus = defaultdict(list)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        start = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        print('Epoch: [{}/{}]'.format(epoch, epochs))\n",
    "\n",
    "        # Iterate over training batches for one epoch\n",
    "        for i, batch in tqdm(enumerate(train_iterator), total=len(train_iterator)):\n",
    "            t = time.time()\n",
    "            running_loss += model.train_step(batch)\n",
    "\n",
    "        # Average training loss for this epoch\n",
    "        # *****START CODE\n",
    "        epoch_loss = running_loss / len(train_iterator)\n",
    "        # *****END CODE\n",
    "\n",
    "        print(\"loss={:.3f}, time={:.2f}\".format(epoch_loss, time.time() - start))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Evaluate and save the model\n",
    "        if epoch % validation_frequency == 0:\n",
    "            bleu_scores = []\n",
    "\n",
    "            # Compute BLEU over all validation sets\n",
    "            for valid_iterator in valid_iterators:\n",
    "                # *****START CODE\n",
    "                src, tgt = valid_iterator.source_lang, valid_iterator.target_lang\n",
    "                translation_output = model.translate(valid_iterator, postprocess)\n",
    "                bleu_score = translation_output.score\n",
    "                output = translation_output.output\n",
    "                print(output)\n",
    "                # *****END CODE\n",
    "\n",
    "                with open(os.path.join(model_dir, 'valid.{}-{}.{}.out'.format(src, tgt, epoch)), 'w') as f:\n",
    "                    f.writelines(line + '\\n' for line in output)\n",
    "\n",
    "                print('{}-{}: BLEU={}'.format(src, tgt, bleu_score))\n",
    "                bleus[(src, tgt)].append(bleu_score)\n",
    "                sys.stdout.flush()\n",
    "                bleu_scores.append(bleu_score)\n",
    "\n",
    "            # Average the validation BLEU scores\n",
    "            bleu_score = round(sum(bleu_scores) / len(bleu_scores), 2)\n",
    "            if len(bleu_scores) > 1:\n",
    "                print('BLEU={}'.format(bleu_score))\n",
    "\n",
    "            # Update the model's learning rate based on current performance.\n",
    "            # This scheduler divides the learning rate by 10 if BLEU does not improve.\n",
    "            model.scheduler_step(bleu_score)\n",
    "\n",
    "            # Save a model checkpoint if it has the best validation BLEU so far\n",
    "            if bleu_score > best_bleu:\n",
    "                best_bleu = bleu_score\n",
    "                save_model(model, checkpoint_path)\n",
    "\n",
    "        print('=' * 50)\n",
    "\n",
    "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))\n",
    "    return bleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1F8CLBQ6M2QK"
   },
   "outputs": [],
   "source": [
    "model_bleus = {}\n",
    "models = {\n",
    "    \"transformer\": multi_transformer_model,\n",
    "    \"norm_transformer\": normed_transformer_model,\n",
    "    \"shallow_norm\": sh_normed_transformer_model,\n",
    "    \"shallow_transformer\": sh_multi_transformer_model,\n",
    "}\n",
    "def average_coefficients(data_dict):\n",
    "    # Unpack all lists from the dictionary and compute the average for each coefficient\n",
    "    num_lists = len(data_dict)\n",
    "    list_length = len(next(iter(data_dict.values())))  # Assume all lists are the same length\n",
    "\n",
    "    # Compute the averages using a comprehension\n",
    "    averaged_list = [\n",
    "        sum(data_dict[key][i] for key in data_dict) / num_lists\n",
    "        for i in range(list_length)\n",
    "    ]\n",
    "\n",
    "    return averaged_list\n",
    "\n",
    "for name, model in models.items():\n",
    "    checkpoint_path=os.path.join(multi_model_dir,f\"{name}.pt\")\n",
    "    bleus = train_model(\n",
    "        train_iterators,\n",
    "        valid_iterators,\n",
    "        normed_transformer_model,\n",
    "        epochs=10,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "    )\n",
    "    model_bleus[name] = average_coefficients(bleus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRGgWzlrM2QK"
   },
   "source": [
    "## Results of the training w.r.t to epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjU5zqmPM2QK",
    "outputId": "c34f2f1e-2b45-46fa-f3aa-dfc9c7fe665c"
   },
   "outputs": [],
   "source": [
    "for model_name, average_bleu_scores in model_bleus.items():\n",
    "    plt.plot(average_bleu_scores, '--x', label=model_name)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('BLEU score')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgbwXAzxM2QL"
   },
   "source": [
    "## Focus on the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0N8S6J2GM2QL",
    "outputId": "64d9b37c-5f82-43ca-bae6-e71aa5e85e2d"
   },
   "outputs": [],
   "source": [
    "for model_name, average_bleu_scores in model_bleus.items():\n",
    "    if model_name != \"transformer\":\n",
    "        plt.plot(average_bleu_scores, '--x', label=model_name)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('BLEU score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoTbJZ2RM2QM"
   },
   "source": [
    "## Plot w.r.t to source lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LALVxJriM2QN"
   },
   "outputs": [],
   "source": [
    "averages = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    bleu_scores = []\n",
    "    for iterator in valid_iterators:\n",
    "        transformer_lengths, bleu_score = get_binned_bleu_scores(model, iterator)\n",
    "        bleu_scores.append(bleu_score)\n",
    "    average_bleu_scores = np.mean(bleu_scores, axis=0)\n",
    "    averages[model_name] = average_bleu_scores\n",
    "    print(f\"Average BLEU scores for {model_name}: {average_bleu_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QL7wuEdxM2QN",
    "outputId": "894e8f14-893d-40b2-df6f-bfd290f7df73"
   },
   "outputs": [],
   "source": [
    "for model_name, average_bleu_scores in averages.items():\n",
    "    plt.plot(transformer_lengths, average_bleu_scores, '--x', label=model_name)\n",
    "plt.xlabel('Source length')\n",
    "plt.ylabel('BLEU score')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HliTx7qcM2QO",
    "outputId": "6983dde5-c447-4201-9d68-32a5ae28cf61"
   },
   "outputs": [],
   "source": [
    "for model_name, average_bleu_scores in averages.items():\n",
    "    if model_name not in [\"shallow_norm\",\"shallow_transformer\"]:\n",
    "        plt.plot(transformer_lengths, average_bleu_scores, '--x', label=model_name)\n",
    "plt.xlabel('Source length')\n",
    "plt.ylabel('BLEU score')\n",
    "plt.legend()\n",
    "plt.show()²\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
