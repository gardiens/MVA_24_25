{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgkYZB8ay6UQ"
   },
   "source": [
    "# **TP 3: Uncertainty Applications**\n",
    "\n",
    "This last lab session will focus on applications based on uncertainty estimation. We will first use MC Dropout variational inference to qualitatively evaluate the most uncertain images for classification . \n",
    "Then, we'll apply the uncertainty for an important application: failure prediction.\n",
    "\n",
    "**Goal**: Take hand on applying uncertainty estimation for classification and use it for failure prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HPtHehyy6UV"
   },
   "source": [
    "## **Preliminary: download MNIST dataset**\n",
    "\n",
    "**First, we will download the MNIST dataset** (train/test images and labels): [https://thome.isir.upmc.fr/classes/MVA/mnist-data.pcl](https://thome.isir.upmc.fr/classes/MVA/mnist-data.pcl)\n",
    "\n",
    "**Then, the dataset will be loaded into memory to speed up computations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXV3DJMcy6UW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "%matplotlib inline\n",
    "import _pickle as pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {\"num_workers\": 4, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "# !wget https://thome.isir.upmc.fr/classes/MVA/mnist-data.pcl\n",
    "outfile = \"mnist-data.pcl\"\n",
    "[X_train, y_train, X_test, y_test] = pickle.load(open(outfile, \"rb\"))\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaSmWGuMy6UY"
   },
   "source": [
    "## **Part I : MC Dropout for classification on MNIST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y6vXv4by6UY"
   },
   "source": [
    "**We will design a 'LeNet-like' Convolutional neural network (ConvNet) with the following architecture:**\n",
    "\n",
    "- A conv layer with 16 5x5 filters, followed by a max pooling a size (2,2)\n",
    "- A conv layer with 32 5x5 filters, followed by a max pooling a size (2,2). From this stage, flatten the tensor\n",
    "- A dropout layer with p=0.5\n",
    "- A fully connected layer with 100 hidden units, and ReLU activation\n",
    "- A dropout layer with p=0.25\n",
    "- A fully connected layer with 10 output classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yHUfPmfy6UZ"
   },
   "source": [
    "### **I.1: Training a ConvNet with dropout on MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ei4N3knyy6Ua"
   },
   "outputs": [],
   "source": [
    "DROPOUT1_RATE = 0.5\n",
    "DROPOUT2_RATE = 0.25\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 100)\n",
    "        self.fc2 = nn.Linear(100, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layer 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Convolutional layer 2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(-1, 32 * 4 * 4)\n",
    "\n",
    "        # Dropout layer 1\n",
    "        x = F.dropout(x, p=DROPOUT1_RATE, training=self.training)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Dropout layer 2\n",
    "        x = F.dropout(x, p=DROPOUT2_RATE, training=self.training)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhSdB3hYy6Ua"
   },
   "outputs": [],
   "source": [
    "if use_cuda == True:\n",
    "    net = LeNet().cuda()\n",
    "else:\n",
    "    net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xtjr9IPty6Ua"
   },
   "source": [
    "**Now, fill the code below to train the network for 20 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tggnb-Fry6Ub"
   },
   "outputs": [],
   "source": [
    "if use_cuda == True:\n",
    "    net = LeNet().cuda()\n",
    "else:\n",
    "    net = LeNet()\n",
    "tbatch = 100\n",
    "nb_epochs = 20\n",
    "nbbatchs = int(X_train.shape[0] / 100)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(nb_epochs):  # loop over the dataset multiple times\n",
    "    for i in range(nbbatchs):\n",
    "        X_batch = X_train[i * 100 : (i + 1) * 100, :, :]\n",
    "        y_batch = y_train[i * 100 : (i + 1) * 100]\n",
    "        # Compute forward / backward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch %d/%d, Tr Loss: %.2f\" % (epoch + 1, nb_epochs, loss.item()))\n",
    "\n",
    "\n",
    "# Save your model in case the session crashes\n",
    "# TODO: push it With Cuda\n",
    "torch.save(net.state_dict(), \"lenet_final.cpkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mg-TQh1Ny6Uc"
   },
   "source": [
    "### **I.2: MC sampling on test set**\n",
    "Once your ConvNet model with dropout is trained, apply MC sampling in the test set to approximate the predictive distribution for each sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJjbZJ2Ky6Ud"
   },
   "outputs": [],
   "source": [
    "# net.load_state_dict(torch.load('./simple/TP3/lenet_final.cpkt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2vX9b64y6Ud"
   },
   "outputs": [],
   "source": [
    "# complete the following function: parse the dataset in a batch manner, run the network with dropout\n",
    "# sampling activated to get multiple predictions for each image\n",
    "def MC_sampling(X, y, net, tbatch=100, samples=200):\n",
    "    net.eval()\n",
    "    net.training = True\n",
    "    outputs = torch.zeros(samples, X.shape[0], 10)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s in range(samples):\n",
    "            for i in range(0, X.shape[0], tbatch):\n",
    "                X_batch = X[i : i + tbatch]\n",
    "                y_batch = y[i : i + tbatch]\n",
    "\n",
    "                output = net(X_batch)\n",
    "\n",
    "                outputs[s, i : i + tbatch, :] = F.softmax(output, dim=1)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7dFvgK5y6Ue",
    "outputId": "604f43c1-14a6-4c9e-fb48-328308906bbd"
   },
   "outputs": [],
   "source": [
    "#! PUT IT back\n",
    "MC_samples = MC_sampling(X_test, y_test, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OeCVnbZy6Ue"
   },
   "source": [
    "**We will now compute a classification uncertainty metric based on predictive distribution on test images.**\n",
    "\n",
    "Let us consisder an image $\\mathbf{x}$, for which we have a set of $N_S$ samples, each sample corresponding to a vector of size $N_C$ ($N_C$ being the number of classes). We compute an histogram of predictions over the $N_S$ samples: the histogram dispersion provides an indicator of the predictive uncertainty.\n",
    "Specifically, we use the following \"variation-ratio\" metric: \n",
    "\n",
    "$$ variation-ratio[\\mathbf{x}] = 1 - \\frac{f_x^{c^*}}{T}\n",
    "$$\n",
    "\n",
    "where $f_x^{c^*}$ is the number of occurences in histogram corresponding to the majority class, (*i.e.* the mode) ${c^*}$.\n",
    "\n",
    "We can compute the predictions histograms as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXJpnMf1y6Ue"
   },
   "outputs": [],
   "source": [
    "nS = MC_samples.shape[0]\n",
    "MC_samples_SM = F.softmax(MC_samples, dim=2)  # Apply soft-max\n",
    "nbtest = X_test.shape[0]\n",
    "am = np.argmax(\n",
    "    MC_samples, axis=2\n",
    ")  # Compute predicted class for each MC sampling and each example\n",
    "hists = [np.histogram(am[:, i], range=(0, 10)) for i in range(nbtest)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aW4GGnPzy6Ue"
   },
   "source": [
    "Then the model predictions for each example are computing as the majority class among MC samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNnB5XJzy6Ue"
   },
   "outputs": [],
   "source": [
    "pred_vr = [0 for i in range(10000)]\n",
    "for i in range(10000):\n",
    "    pred_vr[i] = np.argmax(hists[i][0])\n",
    "pred_vr2 = torch.tensor(np.array(pred_vr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-U8V-TCy6Ue"
   },
   "source": [
    "**From prediction histograms, compute the \"variation-ratio\" uncertainty metric:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juN8YZBdy6Uf"
   },
   "outputs": [],
   "source": [
    "fx = np.array([np.max(hist[0]) for hist in hists])\n",
    "var_ratio = 1.0 - fx / MC_samples.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98swfO_gy6Uf"
   },
   "source": [
    "We can then sort test examples by increasing confidence $f_x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUm7B9BYy6Uf"
   },
   "outputs": [],
   "source": [
    "uncertain_samples = np.argsort(fx)  # COMPLETE with your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-yMJ69Ty6Uf"
   },
   "source": [
    "**The following function can be used to draw 25 images ranked from index m regarding confidence:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjPojKvVy6Uf"
   },
   "outputs": [],
   "source": [
    "def showImages(m, var_ratio, uncertain_samples, X_test):\n",
    "    # Visualize some images\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=5, dpi=150)\n",
    "\n",
    "    for i in range(m, m + 25):\n",
    "        ist = i - m\n",
    "        title = \"VR=\" + \"{0:.3f}\".format(var_ratio[uncertain_samples[i]])\n",
    "        axes[ist // 5][ist % 5].set_title(title, fontsize=6)\n",
    "        axes[ist // 5][ist % 5].imshow(\n",
    "            X_test[uncertain_samples[i], 0, :, :].cpu(), cmap=\"gray\"\n",
    "        )\n",
    "        axes[ist // 5][ist % 5].set_xticks([])\n",
    "        axes[ist // 5][ist % 5].set_yticks([])\n",
    "    fig.set_size_inches(5, 5)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpOQ5SZ7y6Ug"
   },
   "source": [
    "**Explore image with low and high confidence. Look at images and comment the confidence metric.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pMmXgbKy6Ug"
   },
   "outputs": [],
   "source": [
    "showImages(0, var_ratio, uncertain_samples, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaZR8sHdy6Ug"
   },
   "outputs": [],
   "source": [
    "showImages(8000, var_ratio, uncertain_samples, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LFuSH_hm5Pf"
   },
   "source": [
    "**Finally, the following showUncertainty function can be used to visualize the predictions for a given input image:**\n",
    "- The mean prediction over MC samples\n",
    "- The histogram predictions\n",
    "- The histogram of predictions for the 3 leading classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W88FPI3_y6Ug"
   },
   "outputs": [],
   "source": [
    "def showUncertainty(MC_sample, hist, img, title):\n",
    "    size = 10\n",
    "\n",
    "    sSM = F.softmax(MC_sample, dim=1)\n",
    "    pred_mean = sSM.mean(axis=0)\n",
    "\n",
    "    fig = plt.figure(dpi=150)\n",
    "    fig.suptitle(title, fontsize=\"x-large\")\n",
    "    fig.set_figheight(4)\n",
    "    fig.set_figwidth(4 * 5)\n",
    "    ax = plt.subplot(161)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    ax = plt.subplot(162)\n",
    "    ax.bar(range(10), pred_mean)\n",
    "    ax.set_title(\"Mean Pred\")\n",
    "    for item in (\n",
    "        [ax.title, ax.xaxis.label, ax.yaxis.label]\n",
    "        + ax.get_xticklabels()\n",
    "        + ax.get_yticklabels()\n",
    "    ):\n",
    "        item.set_fontsize(size)\n",
    "    ax.set_xticks(range(10))\n",
    "\n",
    "    ax = plt.subplot(163)\n",
    "    ax.bar(range(10), hist)\n",
    "    ax.set_title(\"Samples Pred\")\n",
    "    ax.set_xticks(range(10))\n",
    "    for item in (\n",
    "        [ax.title, ax.xaxis.label, ax.yaxis.label]\n",
    "        + ax.get_xticklabels()\n",
    "        + ax.get_yticklabels()\n",
    "    ):\n",
    "        item.set_fontsize(size)\n",
    "\n",
    "    asorted = np.argsort(pred_mean, axis=0)\n",
    "\n",
    "    h1 = np.histogram(sSM[:, asorted[9]], range=(0.0, 1.0))\n",
    "    h2 = np.histogram(sSM[:, asorted[8]], range=(0.0, 1.0))\n",
    "    h3 = np.histogram(sSM[:, asorted[7]], range=(0.0, 1.0))\n",
    "\n",
    "    ax = plt.subplot(164)\n",
    "    ax.bar(range(10), h1[0])\n",
    "    ax.set_title(\"Class=\" + str(asorted[9].numpy()))\n",
    "    ax.set_yticks(np.arange(0, 200, 20))\n",
    "    ax.set_xticks(range(10))\n",
    "    for item in (\n",
    "        [ax.title, ax.xaxis.label, ax.yaxis.label]\n",
    "        + ax.get_xticklabels()\n",
    "        + ax.get_yticklabels()\n",
    "    ):\n",
    "        item.set_fontsize(size)\n",
    "    ax = plt.subplot(165)\n",
    "    ax.bar(range(10), h2[0])\n",
    "    ax.set_title(\"Class=\" + str(asorted[8].numpy()))\n",
    "    ax.set_yticks(np.arange(0, 200, 20))\n",
    "    ax.set_xticks(range(10))\n",
    "    for item in (\n",
    "        [ax.title, ax.xaxis.label, ax.yaxis.label]\n",
    "        + ax.get_xticklabels()\n",
    "        + ax.get_yticklabels()\n",
    "    ):\n",
    "        item.set_fontsize(size)\n",
    "    ax = plt.subplot(166)\n",
    "    ax.bar(range(10), h3[0])\n",
    "    ax.set_title(\"Class=\" + str(asorted[7].numpy()))\n",
    "    ax.set_yticks(np.arange(0, 200, 20))\n",
    "    ax.set_xticks(range(10))\n",
    "    for item in (\n",
    "        [ax.title, ax.xaxis.label, ax.yaxis.label]\n",
    "        + ax.get_xticklabels()\n",
    "        + ax.get_yticklabels()\n",
    "    ):\n",
    "        item.set_fontsize(size)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyqwhY_Sy6Uh"
   },
   "outputs": [],
   "source": [
    "min = 10\n",
    "max = 100\n",
    "\n",
    "cpt = np.random.randint(min, max)\n",
    "index = uncertain_samples[cpt]\n",
    "\n",
    "title = (\n",
    "    \"TRUE LABEL=\"\n",
    "    + str(y_test[index].cpu().numpy())\n",
    "    + \"- PRED=\"\n",
    "    + str(pred_vr2[index].cpu().numpy())\n",
    "    + \" - Uncertainty=\"\n",
    "    + str(\"{:0.2f}\".format(var_ratio[index]))\n",
    "    + \" rank=\"\n",
    "    + str(cpt)\n",
    ")\n",
    "showUncertainty(\n",
    "    MC_samples[:, index, :],\n",
    "    hists[index][0],\n",
    "    X_test[index, 0, :, :].reshape([28, 28]).cpu(),\n",
    "    title,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Comment results for investigating most uncertain vs confident samples [I.1]\n",
    "Based on the results, clears number has a really low uncertainity ratio while uncertain number have higher ratio\n",
    "#TODO: expand it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T84RQkmSy6Uh"
   },
   "source": [
    "## **Part II : Failure Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI_jsMW7y6Ui"
   },
   "source": [
    "**The objective is to provide confidence measures for model’s predictions that are reliable and whoseranking among samples enables to distinguish correct from incorrect predictions. Equipped with sucha confidence measure, a system could decide to stick to the prediction or, on the contrary, to handover to a human or a back-up system with, *e.g.* other sensors, or simply to trigger an alarm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://thome.isir.upmc.fr/classes/MVA/failure.png\" title=\"Failure prediction\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGE_RHPzo7GH"
   },
   "source": [
    "**We will introduce ConfidNet, a specific method design to address failure prediction and we will compare it to MCDropout with entropy and Maximum Class Probability (MCP).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-p4vUTPpMyZ"
   },
   "source": [
    "### **II.1 ConfidNet**\n",
    "\n",
    "By taking the largest softmax probability as confidence estimate, MCP leads to high confidence values both for correct and erroneous predictions alike. On the other hand, when the model misclassifies an example, the probability associated to the true class $y$ is lower than the maximum one and likely to be low.\n",
    "\n",
    "Based on this observation, we can consider instead the **True Class Probability** as a suitable uncertainty criterion.\n",
    "For any admissible input $\\pmb{x}\\in \\mathcal{X}$, we assume the *true* class $y(\\pmb{x})$ is known, which we denote $y$ for simplicity. The TCP of a model $F$ is defined as  \n",
    "\\begin{equation}\n",
    "    \\text{TCP}_F(\\pmb{x},y) = P(Y=y \\vert \\pmb{x}, \\hat{\\pmb{w}})\n",
    "\\end{equation}\n",
    "\n",
    "**Theoretical guarantees.** Given a properly labelled example $(\\pmb{x},y)$, then:\n",
    "- $\\text{TCP}_F(\\pmb{x},y)> 1/2$ $\\Rightarrow$ $f(\\pmb{x}) = y$, *i.e.* the example is correctly classified by the model;%the example has been correctly classified,\n",
    "- $\\text{TCP}_F(\\pmb{x},y) < 1/K$ $\\Rightarrow$ $f(\\pmb{x}) \\neq y$, *i.e.* the example is wrongly classified by the model.\n",
    "\n",
    "However, the true classes $y$ are obviously not available when estimating confidence on test inputs. Alternatively, we can **learn TCP criterion from data** with an auxiliary model called **ConfidNet**.\n",
    "\n",
    "ConfidNet is designed as a small multilayer perceptron composed of a succession of dense layers with a final sigmoid activation that outputs $C(\\pmb{x};\\pmb{\\theta})\\in[0,1]$. We use a mean-square-error (MSE) loss to train this model:\n",
    "\\begin{equation} \n",
    "\\mathcal{L}_{\\text{conf}}(\\pmb{\\theta};\\mathcal{D}) = \\frac{1}{N} \\sum_{n=1}^N \\big(C(\\pmb{x}_n;\\pmb{\\theta}) - \\text{TCP}_F(\\pmb{x}_n,y_n)\\big)^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://thome.isir.upmc.fr/classes/MVA/confidnet.jpg\" title=\"ConfidNet\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwA9VViv5Zpx"
   },
   "outputs": [],
   "source": [
    "class LeNetConfidNet(nn.Module):\n",
    "    \"\"\"A LeNet-syle model equipped with ConfidNet auxiliary branch\"\"\"\n",
    "\n",
    "    def __init__(self, n_classes=10):\n",
    "        super(LeNetConfidNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 32, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # ConfidNet Layers\n",
    "        self.uncertainty1 = nn.Linear(100, 400)\n",
    "        self.uncertainty2 = nn.Linear(400, 400)\n",
    "        self.uncertainty3 = nn.Linear(400, 400)\n",
    "        self.uncertainty4 = nn.Linear(400, 400)\n",
    "        self.uncertainty5 = nn.Linear(400, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        out = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        out = F.max_pool2d(F.relu(self.conv2(out)), 2)\n",
    "        out = out.view(-1, self.num_flat_features(out))\n",
    "        out = F.dropout(out, 0.5, training=self.training)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.dropout(out, 0.25, training=self.training)\n",
    "\n",
    "        # Uncertainty prediction\n",
    "        uncertainty = F.relu(self.uncertainty1(out))\n",
    "        uncertainty = F.relu(self.uncertainty2(uncertainty))\n",
    "        uncertainty = F.relu(self.uncertainty3(uncertainty))\n",
    "        uncertainty = F.relu(self.uncertainty4(uncertainty))\n",
    "        uncertainty = self.uncertainty5(uncertainty)\n",
    "\n",
    "        pred = self.fc2(out)\n",
    "        return pred, uncertainty\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-28ioCu8Wsb"
   },
   "outputs": [],
   "source": [
    "class SelfConfidMSELoss(nn.modules.loss._Loss):\n",
    "    \"\"\"MSE Loss for confidence learning\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, device):\n",
    "        self.nb_classes = num_classes\n",
    "        self.device = device\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        probs = F.softmax(input[0], dim=1)\n",
    "        confidence = torch.sigmoid(input[1]).squeeze()\n",
    "        # print(\"target\",target.device)\n",
    "        # print(\"device\",device)\n",
    "        labels_hot = torch.eye(10, device=device)[target.flatten()].to(device)\n",
    "        loss = (confidence - (probs * labels_hot).sum(dim=1)) ** 2\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW0YYGKI8fVs"
   },
   "source": [
    "**We train only the ConfidNet layers for 50 epochs. During confidence learning, original classification layers are fixed to keep predictions unchanged.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_confidnet = LeNetConfidNet(n_classes=10).to(device)\n",
    "lenet_confidnet.load_state_dict(torch.load(\"lenet_final.cpkt\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7U5DzHk8Zg2"
   },
   "outputs": [],
   "source": [
    "lenet_confidnet = LeNetConfidNet(n_classes=10).to(device)\n",
    "lenet_confidnet.load_state_dict(torch.load(\"lenet_final.cpkt\"), strict=False)\n",
    "lenet_confidnet.train()\n",
    "optimizer = torch.optim.Adam(lenet_confidnet.parameters(), lr=1e-4)\n",
    "criterion = SelfConfidMSELoss(10, device)\n",
    "print(\"device=\", device)\n",
    "tbatch = 100\n",
    "nb_epochs = 50\n",
    "nbbatchs = int(X_train.shape[0] / 100)\n",
    "\n",
    "# Freezing every layer except uncertainty for confidence training\n",
    "for param in lenet_confidnet.named_parameters():\n",
    "    if \"uncertainty\" in param[0]:\n",
    "        continue\n",
    "    param[1].requires_grad = False\n",
    "\n",
    "best_aupr = 0.0\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    lenet_confidnet.train()\n",
    "\n",
    "    total_loss, correct = 0.0, 0.0\n",
    "    errors, uncertainty = [], []\n",
    "\n",
    "    for i in range(nbbatchs):\n",
    "        X_batch = X_train[i * 100 : (i + 1) * 100, :, :]\n",
    "        y_batch = y_train[i * 100 : (i + 1) * 100]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = lenet_confidnet(X_batch)\n",
    "        probs = F.softmax(output[0], dim=1)\n",
    "        pred = probs.max(dim=1)[1]\n",
    "\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        correct += (pred == y_batch).sum()\n",
    "        errors.extend((pred != y_batch.view_as(pred)).detach().to(\"cpu\").numpy())\n",
    "        uncertainty.extend(output[1].squeeze().detach().to(\"cpu\").numpy())\n",
    "\n",
    "    errors_test, uncertainty_test = [], []\n",
    "    lenet_confidnet.eval()\n",
    "    nbbatchstest = int(X_test.shape[0] / tbatch)\n",
    "    for i in range(nbbatchstest):\n",
    "        X_batch = X_test[i * 100 : (i + 1) * 100, :, :]\n",
    "        y_batch = y_test[i * 100 : (i + 1) * 100]\n",
    "        with torch.no_grad():\n",
    "            output = lenet_confidnet(X_batch)\n",
    "        pred = output[0].max(dim=1)[1]\n",
    "        errors_test.extend((pred != y_batch.view_as(pred)).detach().to(\"cpu\").numpy())\n",
    "        uncertainty_test.extend(output[1].squeeze().detach().to(\"cpu\").numpy())\n",
    "\n",
    "    errors = np.reshape(errors, newshape=(len(errors), -1)).flatten()\n",
    "    uncertainty = np.reshape(uncertainty, newshape=(len(uncertainty), -1)).flatten()\n",
    "    aupr = average_precision_score(errors, -uncertainty)\n",
    "    errors_test = np.reshape(errors_test, newshape=(len(errors_test), -1)).flatten()\n",
    "    uncertainty_test = np.reshape(\n",
    "        uncertainty_test, newshape=(len(uncertainty_test), -1)\n",
    "    ).flatten()\n",
    "    print(\n",
    "        f\"[Epoch {e + 1}] loss: {total_loss/ X_train.shape[0]:.2E}\"\n",
    "        + f\"\\t accuracy_train: {correct / X_train.shape[0]:.2%}\"\n",
    "        + f\"\\t aupr_train: {aupr:.2%}\"\n",
    "        + f\"\\t aupr_test: {average_precision_score(errors_test, -uncertainty_test):.2%}\"\n",
    "    )\n",
    "    if aupr > best_aupr:\n",
    "        best_aupr = aupr\n",
    "        torch.save(lenet_confidnet.state_dict(), \"lenet_confidnet_best.cpkt\")\n",
    "\n",
    "lenet_confidnet.load_state_dict(torch.load(\"lenet_confidnet_best.cpkt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8K1mS9Qr8pc"
   },
   "outputs": [],
   "source": [
    "# If you already train your model, you can load it instead using :\n",
    "lenet_confidnet = LeNetConfidNet(n_classes=10).to(device)\n",
    "lenet_confidnet.load_state_dict(torch.load(\"lenet_confidnet_best.cpkt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s_J1yxPr96u"
   },
   "source": [
    "### **II.2 Evaluate failure prediction performances**\n",
    "\n",
    "**We compare the capacity of ConfidNet to detect failures with previous baselines (MCP and MCDropout with variation-ratio).**\n",
    "\n",
    "To measure performances, we use the *Area under the Precision-Recall* curve (AUPR). The precision-recall (PR) curve is the graph of the precision $= \\mathrm{TP}/(\\mathrm{TP} + \\mathrm{FP})$ as a function of the recall $= \\mathrm{TP}/(\\mathrm{TP} + \\mathrm{FN})$ where $\\mathrm{TP}$, $\\mathrm{TN}$, $\\mathrm{FP}$ and $\\mathrm{FN}$ are the numbers of true positives, true negatives, false positives and false negatives respectively. In our experiments, classification errors are used as the positive detection class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFzVEt6qKa7j"
   },
   "source": [
    "**We can use the following function for test set prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JwYTc92KVSz"
   },
   "outputs": [],
   "source": [
    "# @title **[CODING TASK]** Implement variational-ratio, entropy and mutual information\n",
    "\n",
    "\n",
    "def predict_test_set(\n",
    "    model, X_test, y_test, mode=\"mcp\", s=100, temp=5, epsilon=0.0006, verbose=True\n",
    "):\n",
    "    \"\"\"Predict on a test set given a model\n",
    "    # and a chosen method to compute uncertainty estimate\n",
    "    # (mcp, MC-dropout with var-ratios/entropy/mutual information\n",
    "    # ConfidNet and ODIN)\n",
    "\n",
    "    Args:\n",
    "      model: (nn.Module) a trained model\n",
    "      X_test,y_test: Data & labels\n",
    "      mode: (str) chosen uncertainty estimate method (mcp, var-ratios, entropy, mi, odin)\n",
    "      s: (int) number of samples in MCDropout\n",
    "      temp: (int, optional) value of T for temperature scaling in ODIN\n",
    "      epsilon: (float, optional) value of epsilon for inverse adversarial perturbation in ODIN\n",
    "      verbose: (bool, optional) printing progress bar when predicting\n",
    "\n",
    "    Returns:\n",
    "      pred: (tensor) class predictions by the model\n",
    "      labels: (tensor) true labels of the given dataset\n",
    "      uncertainties: (tensor) uncertainty estimates of the given dataset\n",
    "      errors: (tensor) 0/1 vector whether the model wrongly predicted a sample\n",
    "      hists : (array) number of occurences by class in each sample fo the given dataset, only with MCDropout\n",
    "      mc_samples: (tensor) prediction matrix for s=100 samples, only with MCDropout\n",
    "    \"\"\"\n",
    "    tbatch = 100\n",
    "    nbbatchs = int(X_test.shape[0] / 100)\n",
    "\n",
    "    preds, uncertainties, labels, errors = [], [], [], []\n",
    "    mc_samples, hists = [], []\n",
    "    model.eval()\n",
    "\n",
    "    # loop = tqdm(test_loader, disable=not verbose)\n",
    "    for i in range(nbbatchs):\n",
    "        images = X_test[i * 100 : (i + 1) * 100, :, :]\n",
    "        targets = y_test[i * 100 : (i + 1) * 100]\n",
    "\n",
    "        if mode in [\"mcp\", \"odin\"]:\n",
    "            model.training = False\n",
    "            if mode == \"odin\":\n",
    "                # Coding task in Section 3: implement ODIN\n",
    "                images = odin_preprocessing(model, images, epsilon).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(images)\n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]\n",
    "            if mode == \"odin\":\n",
    "                output = output / temp\n",
    "            confidence, pred = F.softmax(output, dim=1).max(dim=1, keepdim=True)\n",
    "            confidence = confidence.detach().to(\"cpu\").numpy()\n",
    "\n",
    "        elif mode in [\"var-ratios\", \"entropy\", \"mut_inf\"]:\n",
    "            model.training = True\n",
    "            outputs = torch.zeros(images.shape[0], s, 10)\n",
    "            for i in range(s):\n",
    "                with torch.no_grad():\n",
    "                    outputs[:, i] = model(images)\n",
    "            mc_probs = F.softmax(outputs, dim=2)\n",
    "            predicted_class = mc_probs.max(dim=2)[1]\n",
    "            pred = mc_probs.mean(1).max(dim=1, keepdim=True)[1]\n",
    "            mc_samples.extend(mc_probs)\n",
    "            hist = np.array(\n",
    "                [\n",
    "                    np.histogram(predicted_class[i, :], range=(0, 10))[0]\n",
    "                    for i in range(predicted_class.shape[0])\n",
    "                ]\n",
    "            )\n",
    "            hists.extend(hist)\n",
    "\n",
    "            # ============ YOUR CODE HERE ============\n",
    "            if mode == \"var-ratios\":\n",
    "                # You may want to use the hist variable here\n",
    "                confidence = 1.0 - hist.max(axis=1) / s\n",
    "            elif mode == \"entropy\":\n",
    "                confidence = -(\n",
    "                    mc_probs.mean(1) * torch.log(mc_probs.mean(1) + 1e-9)\n",
    "                ).sum(dim=1)\n",
    "            elif mode == \"mut_inf\":\n",
    "                confidence = -(\n",
    "                    mc_probs.mean(1) * torch.log(mc_probs.mean(1) + 1e-9)\n",
    "                ).sum(dim=1) + (mc_probs * torch.log(mc_probs + 1e-9)).sum(dim=2).mean(\n",
    "                    dim=1\n",
    "                )\n",
    "            # =======================================\n",
    "\n",
    "        elif mode == \"confidnet\":\n",
    "            with torch.no_grad():\n",
    "                output, confidence = model(images)\n",
    "            _, pred = F.softmax(output, dim=1).max(dim=1, keepdim=True)\n",
    "            confidence = confidence.detach().to(\"cpu\").numpy()\n",
    "        preds.extend(pred.cpu().numpy())\n",
    "        labels.extend(targets.cpu().numpy())\n",
    "        uncertainties.extend(confidence)\n",
    "        errors.extend(\n",
    "            (pred.to(device) != targets.view_as(pred)).detach().to(\"cpu\").numpy()\n",
    "        )\n",
    "\n",
    "    preds = np.reshape(preds, newshape=(len(preds), -1)).flatten()\n",
    "    labels = np.reshape(labels, newshape=(len(labels), -1)).flatten()\n",
    "    uncertainties = np.reshape(\n",
    "        uncertainties, newshape=(len(uncertainties), -1)\n",
    "    ).flatten()\n",
    "    errors = np.reshape(errors, newshape=(len(errors), -1)).flatten()\n",
    "\n",
    "    if mode in [\"var-ratios\", \"entropy\", \"mi\"]:\n",
    "        hists = np.reshape(hists, newshape=(len(hists), -1))\n",
    "\n",
    "    print(f\"Test set accuracy = {(preds == labels).sum()/len(preds):.2%}\")\n",
    "\n",
    "    return preds, labels, uncertainties, errors, hists, mc_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQYcfUi1y6Um"
   },
   "outputs": [],
   "source": [
    "# @title **[CODING TASK]** Compute precision and recall vectors along with AUPR score for ConfidNet\n",
    "\n",
    "# ============ YOUR CODE HERE ============\n",
    "# Use predict_test_set function to obtain confidence estimates\n",
    "# with previous model, choosing 'confidnet' mode.\n",
    "# Then then calculate the precision, recall and aupr\n",
    "# with sklearn functions.\n",
    "# /!\\ In failure prediction, errors are consider\n",
    "# as the positive class\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Use the predict_test_set function to obtain confidence estimates with the ConfidNet model\n",
    "pred_confidnet, labels_confidnet, uncertainties_confidnet, errors_confidnet, _, _ = (\n",
    "    predict_test_set(lenet_confidnet, X_test, y_test, mode=\"confidnet\")\n",
    ")\n",
    "\n",
    "precision_confidnet, recall_confidnet, _ = precision_recall_curve(\n",
    "    errors_confidnet, -uncertainties_confidnet\n",
    ")\n",
    "aupr_confidnet = average_precision_score(errors_confidnet, -uncertainties_confidnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLkWRtSeLnmx"
   },
   "outputs": [],
   "source": [
    "# @title **[CODING TASK]** Same with MCP\n",
    "\n",
    "# ============ YOUR CODE HERE ============\n",
    "# Mode = 'mcp'\n",
    "pred_mcp, labels_mcp, uncertainties_mcp, errors_mcp, _, _ = predict_test_set(\n",
    "    lenet_confidnet, X_test, y_test, mode=\"mcp\"\n",
    ")\n",
    "\n",
    "precision_mcp, recall_mcp, _ = precision_recall_curve(errors_mcp, -uncertainties_mcp)\n",
    "aupr_mcp = average_precision_score(errors_mcp, -uncertainties_mcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCgYBt84MPNR"
   },
   "outputs": [],
   "source": [
    "# @title **[CODING TASK]** Same with MCDropout\n",
    "\n",
    "# ============ YOUR CODE HERE ============\n",
    "# Mode = 'entropy'\n",
    "_, _, uncertainty_mc_dropout, errors_mc_dropout, _, _ = predict_test_set(\n",
    "    net, X_test, y_test, mode=\"entropy\"\n",
    ")\n",
    "\n",
    "precision_mc_dropout, recall_mc_dropout, _ = precision_recall_curve(\n",
    "    errors_mc_dropout, uncertainty_mc_dropout\n",
    ")\n",
    "aupr_mc_dropout = average_precision_score(errors_mc_dropout, uncertainty_mc_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iZZGHN3MgQS"
   },
   "source": [
    "**Let**'s look at the comparative results for failure prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ul5KNripMiA8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(recall_mcp, precision_mcp, label=f\"MCP, AUPR = {aupr_mcp:.2%}\")\n",
    "plt.plot(\n",
    "    recall_mc_dropout,\n",
    "    precision_mc_dropout,\n",
    "    label=f\"MCDropout (var-ratio), AUPR = {aupr_mc_dropout:.2%}\",\n",
    ")\n",
    "plt.plot(\n",
    "    recall_confidnet,\n",
    "    precision_confidnet,\n",
    "    label=f\"ConfidNet, AUPR = {aupr_confidnet:.2%}\",\n",
    ")\n",
    "plt.title(\"Precision-recall curves for failure prediction\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Question : \n",
    "## Explain the goal of failure prediction\n",
    "The goal of failure prediction is to  distinguish between correct and incorrect prediction. It would help on critical system  to have a better understanding in the output of the model. \n",
    "## Comment the code of the LeNetConfidNet class [II.1]\n",
    "So ConfidNet is like training with the L2 loss but instead of learning with the true label you use an output label ? You don't minimize only the maximum of every label but all the histogram. \n",
    "We have the same LeNET architecture in the beginning. Then we add another model that take the output of the first model and output an uncertainty parameters. \n",
    "\n",
    "\n",
    "## Anlyze results between MCP, MCDropout and ConfidNet [II.2]\n",
    "Observations:\n",
    "MCP (Blue Curve, AUPR = 38.94%):\n",
    "\n",
    "Performance: MCP has the highest AUPR value of 38.94%, which means it performs better at distinguishing correct predictions from incorrect ones compared to the other two methods.\n",
    "Trend:\n",
    "Starts with high precision at low recall but drops more gradually compared to the others as recall increases.\n",
    "MCP maintains better performance across a wider range of recall values.\n",
    "Implication: MCP seems more effective in identifying failures, particularly in scenarios where a higher balance of precision and recall is required.\n",
    "\n",
    "MCDropout (Orange Curve, AUPR = 27.81%):\n",
    "\n",
    "Performance: MCDropout ranks second with an AUPR of 27.81%, showing moderate failure detection performance.\n",
    "Trend:\n",
    "Precision drops more sharply at higher recall values compared to MCP.\n",
    "This suggests that while MCDropout captures uncertainty better than MCP, it does not maintain high confidence across all thresholds.\n",
    "Implication: The method adds value in scenarios with inherent uncertainty but is less reliable for higher recall thresholds.\n",
    "ConfidNet (Green Curve, AUPR = 15.28%):\n",
    "\n",
    "Performance: ConfidNet has the lowest AUPR of 15.28%, indicating the weakest performance in failure prediction.\n",
    "Trend:\n",
    "Precision starts very high at low recall but drops steeply as recall increases.\n",
    "The curve flattens out quickly, showing less effective trade-offs between precision and recall.\n",
    "Implication: Despite being designed for confidence estimation, ConfidNet underperforms in this specific failure prediction task, potentially due to limitations in training or mismatches in calibration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Out-of-distribution detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern neural networks are known to generalize well when the training and testing data are sampled from the same distribution. However, when deploying neural networks in real-world applications, there is often very little control over the testing data distribution. It is important for classifiers to be aware of uncertainty when shown new kinds of inputs, i.e., out-of- distribution examples. Therefore, being able to accurately detect out-of-distribution examples can be practically important for visual recognition tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://thome.isir.upmc.fr/classes/MVA/ood.png\" title=\"OOD detection\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use Kuzushiji-MNIST, a drop-in replacement for the MNIST dataset (28x28 grayscale, 70,000 images) containing 3832 Kanji (japanese) characters, as out-of-distribution sample to our model trained on MNIST. We will compare the methods for uncertainty estimates used previously and ODIN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load KMNIST dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "kmnist_test_dataset = datasets.KMNIST(\n",
    "    \"data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "!wget https://thome.isir.upmc.fr/classes/MVA/Kmnist-data-test.pcl\n",
    "outfile = \"Kmnist-data-test.pcl\"\n",
    "[X_testK, y_testK] = pickle.load(open(outfile, \"rb\"))\n",
    "\n",
    "X_testK = X_testK.to(device)\n",
    "y_testK = y_testK.to(device)\n",
    "\n",
    "# Visualize some images\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4)\n",
    "for i in range(16):\n",
    "    if i >= 16:\n",
    "        break\n",
    "    axes[i // 4][i % 4].imshow(X_testK.cpu()[i][0], cmap=\"gray\")\n",
    "    axes[i // 4][i % 4].set_title(f\"{kmnist_test_dataset.classes[y_testK[i]]}\")\n",
    "    axes[i // 4][i % 4].set_xticks([])\n",
    "    axes[i // 4][i % 4].set_yticks([])\n",
    "fig.set_size_inches(4, 4)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We compute the precision, recall and AUPR metric for OOD detection with MCP and MCDropout with var-ratios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions for MCP method on MNIST\n",
    "_, _, uncertainty_mcp, errors_mcp, _, _ = predict_test_set(\n",
    "    net, X_test, y_test, mode=\"mcp\"\n",
    ")\n",
    "\n",
    "# Same on KMNIST\n",
    "_, _, uncertainty_kmnist, errors_kmnist, _, _ = predict_test_set(\n",
    "    net, X_testK, y_testK, mode=\"mcp\"\n",
    ")\n",
    "\n",
    "# Concatenating predictions with MNIST, considering KMNIST samples as out-of-distributions\n",
    "tot_uncertainty = np.concatenate((uncertainty_mcp, uncertainty_kmnist))\n",
    "in_distribution = np.concatenate(\n",
    "    (np.zeros_like(uncertainty_mcp), np.ones_like(uncertainty_kmnist))\n",
    ")\n",
    "\n",
    "# Obtaining precision and recall plot vector + AUPR\n",
    "precision_ood_mcp, recall_ood_mcp, _ = precision_recall_curve(\n",
    "    in_distribution, -tot_uncertainty\n",
    ")\n",
    "aupr_ood_mcp = average_precision_score(in_distribution, -tot_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing for MCDropout with var-ratios\n",
    "_, _, uncertainty_mc_dropout, _, _, _ = predict_test_set(\n",
    "    net, X_test, y_test, mode=\"var-ratios\"\n",
    ")\n",
    "_, _, uncertainty_mc_dropout_kmnist, _, _, _ = predict_test_set(\n",
    "    net, X_testK, y_testK, mode=\"var-ratios\"\n",
    ")\n",
    "tot_uncertainty = np.concatenate(\n",
    "    (uncertainty_mc_dropout, uncertainty_mc_dropout_kmnist)\n",
    ")\n",
    "in_distribution = np.concatenate(\n",
    "    (np.zeros_like(uncertainty_mc_dropout), np.ones_like(uncertainty_mc_dropout))\n",
    ")\n",
    "\n",
    "precision_ood_mc_dropout, recall_ood_mc_dropout, _ = precision_recall_curve(\n",
    "    in_distribution, tot_uncertainty\n",
    ")\n",
    "aupr_ood_mc_dropout = average_precision_score(in_distribution, tot_uncertainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the ODIN method.\n",
    "\n",
    "ODIN [[Liang et al., ICLR 2018](https://openreview.net/pdf?id=H1VGkIxRZ)], is a threshold-based detector enhancing maximum softmax probabilities with two extensions:\n",
    "- **temperature scaling**: \n",
    "\t$ \\textit{p}(y= c \\vert \\mathbf{x}, \\mathbf{w}, T) = \\frac{\\exp(f_c( \\mathbf{x}, \\mathbf{w}) / T)}{\\sum_{k=1}^K \\exp(f_k( \\mathbf{x}, \\mathbf{w}) / T)} $\n",
    "where $T \\in \\mathbb{R}^{+}$\n",
    "- **inverse adversarial perturbation**: $ \\tilde{\\mathbf{x}} = \\mathbf{x} - \\epsilon \\mathrm{sign} \\big ( - \\nabla_x \\log (\\textit{p}(y = \\hat{y} \\vert \\mathbf{x}, \\mathbf{w}, T) \\big ) $\n",
    "\n",
    "Both technics aimed to increase in-distribution MCP higher than out-distribution MCP. Here, we set the hyperparameters $T=5$ and $\\epsilon=0.0014$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title **[CODING TASK]** Implement ODIN preprocessing\n",
    "\n",
    "\n",
    "def odin_preprocessing(model, input, epsilon):\n",
    "    # We perform the invese adversarial perturbation\n",
    "    # You can find some help in the link below:\n",
    "    # https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    # 1. Set requires_grad attribute of tensor. Important for Attack\n",
    "\n",
    "    # 2. Forward pass the data through the model\n",
    "\n",
    "    # 3. Calculate the loss w.r.t to class predictions\n",
    "\n",
    "    # 4. Zero all existing gradients\n",
    "\n",
    "    # 5. Calculate gradients of model in backward pass\n",
    "\n",
    "    # 6. Collect sign of datagrad\n",
    "\n",
    "    # 7. Normalizing the gradient to the same space of image\n",
    "\n",
    "    # 8. Apply FGSM Attack\n",
    "    model.eval()\n",
    "    input.requires_grad = True\n",
    "\n",
    "    output = model(input)\n",
    "    loss = F.cross_entropy(output, torch.argmax(output, dim=1))\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    sign_input_grad = input.grad.data.sign()\n",
    "    normalized_grad = sign_input_grad / 0.3081\n",
    "\n",
    "    perturbed_input = input + epsilon * normalized_grad\n",
    "\n",
    "    return perturbed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions for ODIN on MNIST\n",
    "_, _, uncertainty_odin, errors_odin, _, _ = predict_test_set(\n",
    "    net, X_test, y_test, mode=\"odin\", temp=5, epsilon=0.0014\n",
    ")\n",
    "\n",
    "# Compute predictions for ODIN on KMNIST\n",
    "_, _, uncertainty_kmnist, errors_kmnist, _, _ = predict_test_set(\n",
    "    net, X_testK, y_testK, mode=\"odin\", temp=5, epsilon=0.0014\n",
    ")\n",
    "\n",
    "# Concatenating predictions with MNIST, considering KMNIST samples as out-of-distributions\n",
    "tot_uncertainty = np.concatenate((uncertainty_odin, uncertainty_kmnist))\n",
    "in_distribution = np.concatenate(\n",
    "    (np.zeros_like(uncertainty_odin), np.ones_like(uncertainty_kmnist))\n",
    ")\n",
    "\n",
    "# Obtaining precision and recall plot vector + AUPR\n",
    "precision_ood_odin, recall_ood_odin, _ = precision_recall_curve(\n",
    "    in_distribution, -tot_uncertainty\n",
    ")\n",
    "aupr_ood_odin = average_precision_score(in_distribution, -tot_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.title(\"Precision-recall curve for OOD detection\")\n",
    "plt.plot(recall_ood_mcp, precision_ood_mcp, label=f\"MCP, AUPR = {aupr_ood_mcp:.2%}\")\n",
    "plt.plot(\n",
    "    recall_ood_mc_dropout,\n",
    "    precision_ood_mc_dropout,\n",
    "    label=f\"MCDropout (var-ratios), AUPR = {aupr_ood_mc_dropout:.2%}\",\n",
    ")\n",
    "plt.plot(recall_ood_odin, precision_ood_odin, label=f\"ODIN, AUPR = {aupr_ood_odin:.2%}\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Question 3.1]: Compare the precision-recall curves of each OOD method along with their AUPR values. Which method perform best and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: OOD detection: analyse results and explain the difference between the 3 methods [III.1]\n",
    "\n",
    "\n",
    "1. ODIN (AUPR = 98.59%)\n",
    "Performance:\n",
    "ODIN achieves the highest AUPR value, demonstrating the best performance in detecting OOD samples.\n",
    "The green curve remains above the other two for most of the recall range, indicating better precision-recall trade-offs.\n",
    "Strengths:\n",
    "ODIN uses temperature scaling and small input perturbations to enhance separation between in-distribution (ID) and OOD samples, which helps it excel in confidence-based OOD detection.\n",
    "Key Takeaway: ODIN outperforms MCP and MCDropout due to its explicit design for OOD detection.\n",
    "2. MCP (AUPR = 97.79%)\n",
    "Performance:\n",
    "MCP slightly outperforms MCDropout but falls behind ODIN.\n",
    "Strengths:\n",
    "MCP relies on softmax probabilities and is computationally efficient, making it competitive despite its simplicity.\n",
    "Weaknesses:\n",
    "MCP suffers from overconfidence in softmax predictions, leading to suboptimal OOD detection in complex scenarios.\n",
    "3. MCDropout (AUPR = 97.29%)\n",
    "Performance:\n",
    "MCDropout has the lowest AUPR value among the three methods.\n",
    "The orange curve starts close to the others but drops slightly faster at higher recall values, indicating weaker precision-recall trade-offs.\n",
    "Strengths:\n",
    "MCDropout captures epistemic uncertainty using Monte Carlo sampling, making it effective in certain tasks.\n",
    "Weaknesses:\n",
    "Its reliance on multiple forward passes introduces computational overhead, and it may not fully address the OOD detection problem due to reliance on softmax probabilities."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP3_uncertainty_applications.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
