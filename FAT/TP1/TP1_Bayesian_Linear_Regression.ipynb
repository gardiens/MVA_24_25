{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 1: Bayesian Linear Regression\n",
    "\n",
    "During this session, we will work with Bayesian Linear Regression models with varying basis functions (linear, polynomial and Gaussian). Datasets used are 1D toy regression samples ranging from linear datasets to more complex non-linear datasets such as increasing sinusoidal curves.\n",
    "\n",
    "**Goal**: Take hand on simple Bayesian models, understand how it works, gain finer insights on predictive distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function: plot results\n",
    "def plot_results(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    std_pred,\n",
    "    xmin=-2,\n",
    "    xmax=2,\n",
    "    ymin=-2,\n",
    "    ymax=1,\n",
    "    stdmin=0.30,\n",
    "    stdmax=0.45,\n",
    "):\n",
    "    \"\"\"Given a dataset and predictions on test set, this function draw 2 subplots:\n",
    "    - left plot compares train set, ground-truth (test set) and predictions\n",
    "    - right plot represents the predictive variance over input range\n",
    "\n",
    "    Args:\n",
    "      X_train: (array) train inputs, sized [N,]\n",
    "      y_train: (array) train labels, sized [N, ]\n",
    "      X_test: (array) test inputs, sized [N,]\n",
    "      y_test: (array) test labels, sized [N, ]\n",
    "      y_pred: (array) mean prediction, sized [N, ]\n",
    "      std_pred: (array) std prediction, sized [N, ]\n",
    "      xmin: (float) min value for x-axis on left and right plot\n",
    "      xmax: (float) max value for x-axis on left and right plot\n",
    "      ymin: (float) min value for y-axis on left plot\n",
    "      ymax: (float) max value for y-axis on left plot\n",
    "      stdmin: (float) min value for y-axis on right plot\n",
    "      stdmax: (float) max value for y-axis on right plot\n",
    "\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.xlim(xmin=xmin, xmax=xmax)\n",
    "    plt.ylim(ymin=ymin, ymax=ymax)\n",
    "    plt.plot(X_test, y_test, color=\"green\", linewidth=2, label=\"Ground Truth\")\n",
    "    plt.plot(X_train, y_train, \"o\", color=\"blue\", label=\"Training points\")\n",
    "    plt.plot(X_test, y_pred, color=\"red\", label=\"BLR Poly\")\n",
    "    plt.fill_between(\n",
    "        X_test,\n",
    "        y_pred - std_pred,\n",
    "        y_pred + std_pred,\n",
    "        color=\"indianred\",\n",
    "        label=\"1 std. int.\",\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        X_test, y_pred - std_pred * 2, y_pred - std_pred, color=\"lightcoral\"\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        X_test,\n",
    "        y_pred + std_pred * 1,\n",
    "        y_pred + std_pred * 2,\n",
    "        color=\"lightcoral\",\n",
    "        label=\"2 std. int.\",\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        X_test, y_pred - std_pred * 3, y_pred - std_pred * 2, color=\"mistyrose\"\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        X_test,\n",
    "        y_pred + std_pred * 2,\n",
    "        y_pred + std_pred * 3,\n",
    "        color=\"mistyrose\",\n",
    "        label=\"3 std. int.\",\n",
    "    )\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Predictive variance along x-axis\")\n",
    "    plt.xlim(xmin=xmin, xmax=xmax)\n",
    "    plt.ylim(ymin=stdmin, ymax=stdmax)\n",
    "    plt.plot(X_test, std_pred**2, color=\"red\", label=\"\\u03c3Â² {}\".format(\"Pred\"))\n",
    "\n",
    "    # Get training domain\n",
    "    training_domain = []\n",
    "    current_min = sorted(X_train)[0]\n",
    "    for i, elem in enumerate(sorted(X_train)):\n",
    "        if elem - sorted(X_train)[i - 1] > 1:\n",
    "            training_domain.append([current_min, sorted(X_train)[i - 1]])\n",
    "            current_min = elem\n",
    "    training_domain.append([current_min, sorted(X_train)[-1]])\n",
    "\n",
    "    # Plot domain\n",
    "    for j, (min_domain, max_domain) in enumerate(training_domain):\n",
    "        plt.axvspan(\n",
    "            min_domain,\n",
    "            max_domain,\n",
    "            alpha=0.5,\n",
    "            color=\"gray\",\n",
    "            label=\"Training area\" if j == 0 else \"\",\n",
    "        )\n",
    "    plt.axvline(X_train.mean(), linestyle=\"--\", label=\"Training barycentre\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Linear Basis function model\n",
    "\n",
    "We start with a linear dataset where we will analyze the behavior of linear basis functions in the framework of Bayesian Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linear toy dataset\n",
    "def f_linear(x, noise_amount, sigma):\n",
    "    y = -0.3 + 0.5 * x\n",
    "    noise = np.random.normal(0, sigma, len(x))\n",
    "    return y + noise_amount * noise\n",
    "\n",
    "\n",
    "# Create training and test points\n",
    "sigma = 0.2\n",
    "nbpts = 25\n",
    "dataset_linear = {}\n",
    "dataset_linear[\"X_train\"] = np.random.uniform(0, 2, nbpts)\n",
    "dataset_linear[\"y_train\"] = f_linear(\n",
    "    dataset_linear[\"X_train\"], noise_amount=1, sigma=sigma\n",
    ")\n",
    "dataset_linear[\"X_test\"] = np.linspace(-10, 10, 10 * nbpts)\n",
    "dataset_linear[\"y_test\"] = f_linear(\n",
    "    dataset_linear[\"X_test\"], noise_amount=0, sigma=sigma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.xlim(xmax=3, xmin=-1)\n",
    "plt.ylim(ymax=1.5, ymin=-1)\n",
    "plt.plot(\n",
    "    dataset_linear[\"X_test\"],\n",
    "    dataset_linear[\"y_test\"],\n",
    "    color=\"green\",\n",
    "    linewidth=2,\n",
    "    label=\"Ground Truth\",\n",
    ")\n",
    "plt.plot(\n",
    "    dataset_linear[\"X_train\"],\n",
    "    dataset_linear[\"y_train\"],\n",
    "    \"o\",\n",
    "    color=\"blue\",\n",
    "    label=\"Training points\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "dataset_linear[\"ALPHA\"] = 2.0\n",
    "dataset_linear[\"BETA\"] = 1 / (2.0 * sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the linear basis function:\n",
    "    $\\phi:x \\rightarrow (1,x)$\n",
    "\n",
    "Design matrix $\\Phi$ defined on training set is:\n",
    "$$ \\Phi=\n",
    "  \\begin{bmatrix}\n",
    "    1 & x_1 \\\\\n",
    "    ... & ...\\\\\n",
    "    1 & x_n\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Question 1.1: Code linear basis function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define basis function\n",
    "\n",
    "\n",
    "def phi_linear(x):\n",
    "    \"\"\"Linear Basis Functions\n",
    "\n",
    "    Args:\n",
    "      x: (float) 1D input\n",
    "\n",
    "    Returns:\n",
    "      (array) linear features of x\n",
    "    \"\"\"\n",
    "    # TO DO\n",
    "    return np.concatenate((np.ones((x.shape[0], 1)), x.reshape(-1, 1)), axis=1)\n",
    "\n",
    "\n",
    "# phi=phi_linear(dataset_linear['X_train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2: Recall closed form of the posterior distribution in linear case. Then, code and visualize posterior sampling. What can you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_linear.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Code and visualize posterior sampling by completing code below\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for count, n in enumerate([1, 2, 10, len(dataset_linear[\"X_train\"])]):  # removed 0\n",
    "    cur_data = dataset_linear[\"X_train\"][:n]\n",
    "    cur_lbl = dataset_linear[\"y_train\"][:n]\n",
    "    meshgrid = np.arange(-1, 1.01, 0.01)\n",
    "    w = np.zeros((2, 1))\n",
    "    posterior = np.zeros((meshgrid.shape[0], meshgrid.shape[0]))\n",
    "\n",
    "    # TO DO: code mu_n and sigma_N\n",
    "    X = dataset_linear[\"X_train\"][:n]\n",
    "    Y = dataset_linear[\"y_train\"][:n]\n",
    "    alpha = dataset_linear[\"ALPHA\"]\n",
    "    beta = dataset_linear[\"BETA\"]\n",
    "    phi = phi_linear(X)\n",
    "    sigma_N_moins1 = beta * phi.T @ phi + alpha * np.eye(phi.shape[1])\n",
    "    sigma_N = np.linalg.inv(sigma_N_moins1)\n",
    "    mu_N = beta * sigma_N @ (phi.T) @ Y  #! TODO: finish it!\n",
    "    # Compute values on meshgrid\n",
    "    for i in range(meshgrid.shape[0]):\n",
    "        for j in range(meshgrid.shape[0]):\n",
    "            w[0, 0] = meshgrid[i]\n",
    "            w[1, 0] = meshgrid[j]\n",
    "            posterior[i, j] = np.exp(\n",
    "                -0.5\n",
    "                * np.dot(\n",
    "                    np.dot((w - mu_N.reshape(2, 1)).T, np.linalg.inv(sigma_N)),\n",
    "                    (w - mu_N.reshape(2, 1)),\n",
    "                )\n",
    "            )\n",
    "    Z = 1.0 / (np.sqrt(2 * np.pi * np.linalg.det(sigma_N)))\n",
    "    posterior[:, :] /= Z\n",
    "\n",
    "    # Plot posterior with n points\n",
    "    plt.subplot(231 + count)\n",
    "    plt.imshow(posterior, extent=[-1, 1, -1, 1])\n",
    "    plt.plot(0.5, 0.3, \"+\", markeredgecolor=\"white\", markeredgewidth=3, markersize=12)\n",
    "    plt.title(\"Posterior with N={} points\".format(n))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3: Recall and code closed form of the predictive distribution in linear case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Code closed form solution according to the following requirements defined below\n",
    "\n",
    "\n",
    "def closed_form(func, X_train, y_train, alpha, beta):\n",
    "    \"\"\"Define analytical solution to Bayesian Linear Regression, with respect to the basis function chosen, the\n",
    "    training set (X_train, y_train) and the noise precision parameter beta and prior precision parameter alpha chosen.\n",
    "    It should return a function outputing both mean and std of the predictive distribution at a point x*.\n",
    "\n",
    "    Args:\n",
    "      func: (function) the basis function used\n",
    "      X_train: (array) train inputs, sized [N,]\n",
    "      y_train: (array) train labels, sized [N, ]\n",
    "      alpha: (float) prior precision parameter\n",
    "      beta: (float) noise precision parameter\n",
    "\n",
    "    Returns:\n",
    "      (function) prediction function, returning itself both mean and std\n",
    "    \"\"\"\n",
    "\n",
    "    # TO DO\n",
    "    phi = func(X_train)\n",
    "    sigma_N_moins1 = beta * phi.T @ phi + alpha * np.eye(phi.shape[1])\n",
    "    sigma_N = np.linalg.inv(sigma_N_moins1)\n",
    "    mu_N = beta * sigma_N @ (phi.T) @ y_train\n",
    "\n",
    "    def f_model(x):\n",
    "        phi_x = func(x)\n",
    "        mean = phi_x @ mu_N\n",
    "        std = np.sqrt(1 / beta + np.sum(phi_x @ sigma_N * phi_x, axis=1))\n",
    "        return mean, std\n",
    "\n",
    "    return f_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pred = closed_form(\n",
    "    phi_linear,\n",
    "    dataset_linear[\"X_train\"],\n",
    "    dataset_linear[\"y_train\"],\n",
    "    dataset_linear[\"ALPHA\"],\n",
    "    dataset_linear[\"BETA\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4: Based on previously defined ``f_pred()``, predict on the test dataset. Then visualize results using ``plot_results()`` defined at the beginning of the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO : predict on test dataset and visualize results\n",
    "X_test = dataset_linear[\"X_test\"]\n",
    "Y_test = dataset_linear[\"y_test\"]\n",
    "mean, std = f_pred(X_test)\n",
    "plot_results(\n",
    "    dataset_linear[\"X_train\"],\n",
    "    dataset_linear[\"y_train\"],\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    mean,\n",
    "    std,\n",
    "    xmin=-10,\n",
    "    xmax=10,\n",
    "    ymin=-6,\n",
    "    ymax=6,\n",
    "    stdmin=0.05,\n",
    "    stdmax=1,\n",
    ")\n",
    "# You should use the following parameters for plot_results\n",
    "# xmin=-10, xmax=10, ymin=-6, ymax=6, stdmin=0.05, stdmax=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5: Analyse these results. Why predictive variance increases far from training distribution? Prove it analytically in the case where $\\alpha=0$ and $\\beta=1$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The further we are from the training points, the more the confidence sigma^2 is increasing.  This is what we would expect to have a robust models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $\\alpha=0$ and $\\beta=1$ then $\\sigma_{-1}=\\phi^T\\phi$  then $\\sigma_{pred}^2(x)=\\phi(x*)^T (\\phi^T\\phi)^{-1} \\phi(x*)$ . Or \\phi^T\\phi is positive semi-definite, so his eigenvalues are $>= 0 $  and $\\sigma_{pred}^2(x)=\\phi(x*)^T (\\phi^T\\phi)^{-1} \\phi(x*)$ goes to +inf when x* si far from the training distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question: What happens when applying Bayesian Linear Regression on the following dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test points\n",
    "sigma = 0.2\n",
    "dataset_hole = {}\n",
    "dataset_hole[\"X_train\"] = np.concatenate(\n",
    "    ([np.random.uniform(-3, -1, 10), np.random.uniform(1, 3, 10)]), axis=0\n",
    ")\n",
    "dataset_hole[\"y_train\"] = f_linear(dataset_hole[\"X_train\"], noise_amount=1, sigma=sigma)\n",
    "dataset_hole[\"X_test\"] = np.linspace(-12, 12, 100)\n",
    "dataset_hole[\"y_test\"] = f_linear(dataset_hole[\"X_test\"], noise_amount=0, sigma=sigma)\n",
    "dataset_hole[\"ALPHA\"] = 2.0\n",
    "dataset_hole[\"BETA\"] = 1 / (2.0 * sigma**2)\n",
    "\n",
    "# Plot dataset\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.xlim(xmin=-12, xmax=12)\n",
    "plt.ylim(ymin=-7, ymax=6)\n",
    "plt.plot(\n",
    "    dataset_hole[\"X_test\"],\n",
    "    dataset_hole[\"y_test\"],\n",
    "    color=\"green\",\n",
    "    linewidth=2,\n",
    "    label=\"Ground Truth\",\n",
    ")\n",
    "plt.plot(\n",
    "    dataset_hole[\"X_train\"],\n",
    "    dataset_hole[\"y_train\"],\n",
    "    \"o\",\n",
    "    color=\"blue\",\n",
    "    label=\"Training points\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define f_pred, predict on test points and plot results\n",
    "f_pred = closed_form(\n",
    "    phi_linear,\n",
    "    dataset_hole[\"X_train\"],\n",
    "    dataset_hole[\"y_train\"],\n",
    "    dataset_hole[\"ALPHA\"],\n",
    "    dataset_hole[\"BETA\"],\n",
    ")\n",
    "# You should use the following parameters for plot_results\n",
    "# xmin=-12, xmax=12, ymin=-7, ymax=6, stdmin=0.0, stdmax=0.5\n",
    "mean, std = f_pred(dataset_hole[\"X_test\"])\n",
    "plot_results(\n",
    "    dataset_hole[\"X_train\"],\n",
    "    dataset_hole[\"y_train\"],\n",
    "    dataset_hole[\"X_test\"],\n",
    "    dataset_hole[\"y_test\"],\n",
    "    mean,\n",
    "    std,\n",
    "    xmin=-12,\n",
    "    xmax=12,\n",
    "    ymin=-7,\n",
    "    ymax=6,\n",
    "    stdmin=0.0,\n",
    "    stdmax=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Non Linear models\n",
    "\n",
    "We now introduce a more complex toy dataset, which is an increasing sinusoidal curve. The goal of this part is to get insight on the importance of the chosen basis function on the predictive variance behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sinusoidal toy dataset\n",
    "def f_sinus(x, noise_amount, sigma=0.2):\n",
    "    y = np.sin(2 * np.pi * x) + x\n",
    "    noise = np.random.normal(0, sigma, len(x))\n",
    "    return y + noise_amount * noise\n",
    "\n",
    "\n",
    "# Create training and test points\n",
    "sigma = 0.2\n",
    "nbpts = 50\n",
    "dataset_sinus = {}\n",
    "dataset_sinus[\"X_train\"] = np.random.uniform(0, 1, nbpts)\n",
    "dataset_sinus[\"y_train\"] = f_sinus(\n",
    "    dataset_sinus[\"X_train\"], noise_amount=1, sigma=sigma\n",
    ")\n",
    "dataset_sinus[\"X_test\"] = np.linspace(-1, 2, 10 * nbpts)\n",
    "dataset_sinus[\"y_test\"] = f_sinus(dataset_sinus[\"X_test\"], noise_amount=0, sigma=sigma)\n",
    "\n",
    "dataset_sinus[\"ALPHA\"] = 0.05\n",
    "dataset_sinus[\"BETA\"] = 1 / (2.0 * sigma**2)\n",
    "\n",
    "# Plot dataset\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.xlim(xmin=-1, xmax=2)\n",
    "plt.ylim(ymin=-2, ymax=3)\n",
    "plt.plot(\n",
    "    dataset_sinus[\"X_test\"],\n",
    "    dataset_sinus[\"y_test\"],\n",
    "    color=\"green\",\n",
    "    linewidth=2,\n",
    "    label=\"Ground Truth\",\n",
    ")\n",
    "plt.plot(\n",
    "    dataset_sinus[\"X_train\"],\n",
    "    dataset_sinus[\"y_train\"],\n",
    "    \"o\",\n",
    "    color=\"blue\",\n",
    "    label=\"Training points\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 Polynomial basis functions\n",
    "\n",
    "We will first use polynomial basis functions:\n",
    "$$\\phi:x \\rightarrow (\\phi_0,\\phi_1,...,\\phi_{D-1})$$\n",
    "where $\\phi_j = x^j$ for $j \\geq 0$ and $D \\geq 0$\n",
    "\n",
    "\n",
    "Design matrix $\\Phi$ defined on training set is:\n",
    "$$ \\Phi=\n",
    "  \\begin{bmatrix}\n",
    "    1 & x_1 & x_1^2 &... &x_1^{D-1} \\\\\n",
    "    ... & ... & ... & ...\\\\\n",
    "    1 & x_n & x_n^2 &... &x_n^{D-1}\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Question 2.1: Code polynomial basis function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basis function\n",
    "def phi_polynomial(x):\n",
    "    \"\"\"Polynomial Basis Functions\n",
    "\n",
    "    Args:\n",
    "      x: (float) 1D input\n",
    "\n",
    "    Returns:\n",
    "      (array) polynomial features of x\n",
    "    \"\"\"\n",
    "    D = 10\n",
    "    # TO DO\n",
    "    phi = np.zeros((x.shape[0], D))\n",
    "    for i in range(D):\n",
    "        phi[:, i] = x**i\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2 : Code and visualize results on sinusoidal dataset using polynomial basis functions. What can you say about the predictive variance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define f_pred, predict on test points and plot results\n",
    "f_pred = closed_form(\n",
    "    phi_polynomial,\n",
    "    dataset_sinus[\"X_train\"],\n",
    "    dataset_sinus[\"y_train\"],\n",
    "    dataset_sinus[\"ALPHA\"],\n",
    "    dataset_sinus[\"BETA\"],\n",
    ")\n",
    "Y = dataset_sinus[\"y_test\"]\n",
    "X = dataset_sinus[\"X_test\"]\n",
    "mean, std = f_pred(X)\n",
    "# You should use the following parameters for plot_results\n",
    "# xmin=-1, xmax=2, ymin=-3, ymax=5, stdmin=0, stdmax=10\n",
    "plot_results(\n",
    "    dataset_sinus[\"X_train\"],\n",
    "    dataset_sinus[\"y_train\"],\n",
    "    X,\n",
    "    Y,\n",
    "    mean,\n",
    "    std,\n",
    "    xmin=-1,\n",
    "    xmax=2,\n",
    "    ymin=-3,\n",
    "    ymax=5,\n",
    "    stdmin=0,\n",
    "    stdmax=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 Gaussian basis functions\n",
    "\n",
    "Now, let's consider gaussian basis functions:\n",
    "$$\\phi:x \\rightarrow (\\phi_0,\\phi_1,...,\\phi_M)$$\n",
    "where $\\phi_j = \\exp \\Big ( -\\frac{(x-\\mu_j)^2}{2s^2} \\Big )$ for $j \\geq 0$\n",
    "\n",
    "\n",
    "Design matrix $\\Phi$ defined on training set is:\n",
    "$$ \\Phi=\n",
    "  \\begin{bmatrix}\n",
    "    \\phi_0(x_1) & \\phi_1(x_1) &... &\\phi_M(x_1) \\\\\n",
    "    ... & ... & ... & ...\\\\\n",
    "    \\phi_0(x_n) & \\phi_1(x_n) &... &\\phi_M(x_n)\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Question 2.3: Code gaussian basis function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define Gaussian basis function\n",
    "MU_MIN = 0\n",
    "MU_MAX = 1\n",
    "M = 9\n",
    "\n",
    "\n",
    "def phi_gaussian(x):\n",
    "    \"\"\"Gaussian Basis Functions\n",
    "\n",
    "    Args:\n",
    "      x: (float) 1D input\n",
    "\n",
    "    Returns:\n",
    "      (array) gaussian features of x\n",
    "    \"\"\"\n",
    "    result = np.zeros((x.shape[0], M))\n",
    "    s = (MU_MAX - MU_MIN) / M\n",
    "    for i in range(M):\n",
    "        mu_i = MU_MIN + (MU_MAX - MU_MIN) * i / M\n",
    "        result[:, i] = np.exp(-((x - mu_i) ** 2) / (2 * s**2))\n",
    "    return result\n",
    "    # return np.exp(-(x - np.arange(MU_MIN, MU_MAX, s)) ** 2 / (2 * s * s))\n",
    "\n",
    "\n",
    "# create a random sample of 50 points\n",
    "x = np.random.uniform(0, 1, 50)\n",
    "# phi_gaussian(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4 : Code and visualize results on sinusoidal dataset using Gaussian basis functions. What can you say this time about the predictive variance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define f_pred, predict on test points and plot results\n",
    "f_pred = closed_form(\n",
    "    phi_gaussian,\n",
    "    dataset_sinus[\"X_train\"],\n",
    "    dataset_sinus[\"y_train\"],\n",
    "    dataset_sinus[\"ALPHA\"],\n",
    "    dataset_sinus[\"BETA\"],\n",
    ")\n",
    "Y = dataset_sinus[\"y_test\"]\n",
    "# You should use the following parameters for plot_results\n",
    "# xmin=-1, xmax=2, ymin=-2, ymax=3, stdmin=0.05, stdmax=0.2\n",
    "mean, std = f_pred(X)\n",
    "plot_results(\n",
    "    dataset_sinus[\"X_train\"],\n",
    "    dataset_sinus[\"y_train\"],\n",
    "    X,\n",
    "    Y,\n",
    "    mean,\n",
    "    std,\n",
    "    xmin=-1,\n",
    "    xmax=2,\n",
    "    ymin=-2,\n",
    "    ymax=3,\n",
    "    stdmin=0.05,\n",
    "    stdmax=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted  variance  doesn't go to +inf as we are getting away from the training area, It means this models will not be really robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5: Explain why in regions far from training distribution, the predictive variance converges to this value when using localized basis functions such as Gaussians.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As x goes to +-inf, $\\phi(x)$ converges to 0 and therefore $\\sigma_{pred}^2=1/\\beta + 0 $ In this case . "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
