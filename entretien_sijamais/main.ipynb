{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter server list\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if you have access to cuda\n",
    "import torch\n",
    "\n",
    "print(\"cuda available: \", torch.cuda.is_available())\n",
    "torch.rand(2, 3).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_data = \"PATH_TO_DATA/db_train.raw\"\n",
    "path_test_data = \"PATH_TO_DATA/db_test.raw\"\n",
    "path_train_label = \"PATH_TO_DATA/label_train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train data , those are images of size 56x56x3\n",
    "import numpy as np\n",
    "\n",
    "X_data = np.fromfile(path_train_data, dtype=np.int16)\n",
    "Y_train = np.fromfile(path_train_data, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_data = torch.tensor(np.fromfile(path_train_data, dtype=\"uint8\")) / 255\n",
    "X_data = X_data.reshape(-1, 56, 56, 3)\n",
    "X_test = torch.tensor(np.fromfile(path_test_data, dtype=\"uint8\")) / 255\n",
    "X_test = X_test.reshape(-1, 56, 56, 3)\n",
    "X_test.shape\n",
    "\n",
    "# load the txt\n",
    "Y_train = torch.tensor(np.loadtxt(path_train_label, dtype=np.int16))\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization des données \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# visualize the 10 first image of X_train and add the respective label\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nb_images = 10\n",
    "\n",
    "fig, axes = plt.subplots(1, nb_images, figsize=(20, 20))\n",
    "\n",
    "# fig.suptitle(\"train_test\")\n",
    "for i in range(nb_images):\n",
    "    axes[i].imshow(X_data[i])\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(int(Y_train[i]))\n",
    "\n",
    "print(\" Some sample from the train set \")\n",
    "# plt.title(\"train_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize image where the respective label is 0\n",
    "fig, axes = plt.subplots(1, 10, figsize=(40, 40))\n",
    "for i in range(10):\n",
    "    axes[i].imshow(X_data[Y_train == 0][i + 10])\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(0)\n",
    "print(\"some negative sample from the train set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize image where the respective label is 1\n",
    "fig, axes = plt.subplots(1, 10, figsize=(40, 40))\n",
    "for i in range(10):\n",
    "    axes[i].imshow(X_data[Y_train == 1][i + 100])\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(1)\n",
    "print(\"some positive sample from the train set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# visualize the 10 first image of X_train\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 20))\n",
    "for i in range(10):\n",
    "    axes[i].imshow(X_test[i])\n",
    "    axes[i].axis(\"off\")\n",
    "plt.title(\"test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of image for each label\n",
    "import numpy as np\n",
    "\n",
    "unique, counts = np.unique(Y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for now, you can't guess from images what is the underlying labels , but the 0 label is much more scarce than the 1.\n",
    "After training a bit and thinking about it, my intuition ( it can be rough if it is not the case) is that some image are AI-generated or not . For example in the picture below, eyes are not aligned. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"image.png\"\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "Image(filename=PATH, width=100, height=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"img_2.png\"\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "Image(filename=PATH, width=100, height=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_data = \"PATH_TO_DATA/db_train.raw\"\n",
    "path_test_data = \"PATH_TO_DATA/db_test.raw\"\n",
    "path_train_label = \"PATH_TO_DATA/label_train.txt\"\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train data , those are images of size 56x56x3\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "X_data = np.fromfile(path_train_data, dtype=np.int16)\n",
    "Y_train = np.fromfile(path_train_data, dtype=np.int16)\n",
    "\n",
    "X_data = (\n",
    "    torch.tensor(np.fromfile(path_train_data, dtype=\"uint8\"), dtype=torch.float32) / 255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"here\")\n",
    "X_data = X_data.reshape(-1, 56, 56, 3)\n",
    "X_test = (\n",
    "    torch.tensor(np.fromfile(path_test_data, dtype=\"uint8\"), dtype=torch.float32) / 255\n",
    ")\n",
    "X_test = X_test.reshape(-1, 56, 56, 3)\n",
    "X_test.shape\n",
    "\n",
    "# load the txt\n",
    "Y_train = torch.tensor(np.loadtxt(path_train_label, dtype=np.int16), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LeNet\n",
    "\n",
    "input = torch.rand(1, 56, 56, 3)\n",
    "\n",
    "model = LeNet(2, 0.5)\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X_data and Y_train into train and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_data, Y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from utils import CustomImageDataset\n",
    "\n",
    "training_data = CustomImageDataset(X_train, Y_train)\n",
    "val_data = CustomImageDataset(X_val, Y_val)\n",
    "test_data = CustomImageDataset(X_test, np.zeros(X_test.shape[0]))\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# define the training loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute metrics\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    train_loss /= size\n",
    "    train_acc /= size\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(test_loader, model, loss_fn):\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.eval()\n",
    "    size = len(test_loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            val_acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    val_loss /= size\n",
    "    val_acc /= size\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "    print(\"End val epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, loss, optimizer, epochs):\n",
    "    pbar = tqdm(total=epochs, desc=\"Training\")\n",
    "\n",
    "    log_interval = 1\n",
    "    train_acc = []\n",
    "    val_acc_l = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        train_ep_loss, train_ep_acc = train_loop(\n",
    "            train_dataloader, model, loss, optimizer\n",
    "        )\n",
    "\n",
    "        if epoch % log_interval == 0:\n",
    "            val_loss, val_acc = valid_epoch(val_dataloader, model, loss)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch}/{epochs}, \"\n",
    "                f\"Train Loss: {train_ep_loss:.4f}, \"\n",
    "                f\"Train Acc: {1e2*train_ep_acc:.2f}%, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, \"\n",
    "                f\"Val Acc: {1e2*val_acc:.2f}%\"\n",
    "            )\n",
    "            train_acc.append(train_ep_acc)\n",
    "            val_acc_l.append(val_acc)\n",
    "\n",
    "            pbar.set_postfix({\"loss\": train_ep_loss})\n",
    "        pbar.update(1)\n",
    "    print(\"Done!\")\n",
    "    pbar.close()\n",
    "    return model, train_acc, val_acc_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effective trainin\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "epochs = 20\n",
    "model = LeNet(2, 0.5).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "bs = 256\n",
    "train_dataloader = DataLoader(training_data, batch_size=bs)\n",
    "val_dataloader = DataLoader(val_data, batch_size=bs)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=bs)\n",
    "model, train_acc, val_acc = run_training(\n",
    "    train_dataloader, val_dataloader, model, loss, optimizer, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Interpretation : \n",
    " After a first training With only 20 epochs:\n",
    " - Comme la validation vient du même dataset que le dataset d'entrainement, on a pas beaucoup de différence entre le dataset d'entrainement et le dataset de validation \n",
    " - On a un plateau en validation autour de 95% des 5 à 7 epochs. On peut donc s'arrêter à ce nombre d'epoch pour nos expérimentations \n",
    " - La question est de savoir si nos résultats sont bons ou non, calculons l'accuracy sur des modèles \"naifs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On verifie que la validation accuracy est meilleur qu'un choix aléatoire\n",
    "# compute the accuracy of a random model\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random_pred = np.random.randint(0, 2, size=len(Y_val))\n",
    "print(\" accuracy si le modèle est aléatoire\", accuracy_score(Y_val, random_pred))\n",
    "\n",
    "# compute the accuracy if we only predict  1\n",
    "print(\n",
    "    \" accuracy si le modèle prédit toujours 1\",\n",
    "    accuracy_score(Y_val, np.ones(Y_val.shape)),\n",
    ")\n",
    "print(\" si le modèle prédit toujours 0\", accuracy_score(Y_val, np.zeros(Y_val.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, les bons résultats sont à compenser par le fort déséquilibre des classes. La classe 1 est ultra-majoritaire  et bien que notre modèle est meilleur que le modèle naif,le modèle naif performe déjà beaucoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the recall and precision\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, y in val_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        y_pred.extend(pred.argmax(1).cpu().numpy())\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "del pred, y\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme attendu le modèle apprend très bien la classe 1 mais arrive quand même à prédire la classe  0 ce qui est pas mal. \n",
    "On peut donc calculer le Half Total Error Rate : \n",
    "Comme il s'agit d'un taux d'erreurs on le veut aussi bas que possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de la metrique : HTER\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "# plot it\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.xlabel(\"True label\")\n",
    "plt.ylabel(\"Predicted label\")\n",
    "plt.show()\n",
    "HTER = (cm[0][1] / (cm[0][0] + cm[0][1]) + cm[1][0] / (cm[1][0] + cm[1][1])) / 2\n",
    "print(\"HTER\", HTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Improve the results :\n",
    " Pour augmenter les résultats on pourrait :\n",
    " - Changer l'architecture du modèle\n",
    " - Ajouter de la Data Augmentation.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changer l'architecture\n",
    "On va tester les modèles avec un FeedForward Network classique et une architecture type Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random FFN network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import FFN\n",
    "\n",
    "input = torch.rand(1, 56, 56, 3)\n",
    "\n",
    "model = FFN(2, dropout=0.2)\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effective training\n",
    "model = FFN(2, dropout=0.2).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "bs = 256\n",
    "train_dataloader = DataLoader(training_data, batch_size=bs)\n",
    "val_dataloader = DataLoader(val_data, batch_size=bs)\n",
    "epochs = 20\n",
    "model, train_acc, val_acc = run_training(\n",
    "    train_dataloader, val_dataloader, model, loss, optimizer, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le rajout d'un modèle de type FEED Forward network n'a pas beaucoup aidé, indiquant que les informations locales qu'extrait le CNN semblent plus efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from utils import ResidualBlock\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, layers, nb_classes, block=ResidualBlock):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, nb_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes  # Update inplanes before subsequent blocks\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten keeping batch size intact\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input = torch.rand(1, 56, 56, 3)\n",
    "\n",
    "model = ResNet([2, 2, 2], 2)\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effective training\n",
    "model = ResNet([2, 2, 2], 2).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "bs = 256\n",
    "train_dataloader = DataLoader(training_data, batch_size=bs)\n",
    "val_dataloader = DataLoader(val_data, batch_size=bs)\n",
    "epochs = 5\n",
    "model, train_acc, val_acc = run_training(\n",
    "    train_dataloader, val_dataloader, model, loss, optimizer, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has almost the same validation results has the initial CNN but overfit on training test and is more slow,  the baseline should be kept I guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation\n",
    "We add here some Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from utils import PermuteChannels, PermuteChannelsinv\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        PermuteChannels(),  # Permutes (H, W, C) -> (C, H, W) if needed\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        PermuteChannelsinv(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "input = transform_train(X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the transform to the dataset\n",
    "training_data_transform = CustomImageDataset(\n",
    "    X_train, Y_train, transform=transform_train\n",
    ")\n",
    "val_data_transform = CustomImageDataset(X_val, Y_val, transform=None)\n",
    "\n",
    "# effective training\n",
    "\n",
    "epochs = 20\n",
    "model = LeNet(2, 0.5).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "bs = 256\n",
    "train_dataloader = DataLoader(training_data_transform, batch_size=bs, num_workers=4)\n",
    "val_dataloader = DataLoader(val_data_transform, batch_size=bs, num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=bs)\n",
    "model_transform, train_acc, val_acc = run_training(\n",
    "    train_dataloader, val_dataloader, model, loss, optimizer, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a gagné environ 0.7% d'accuracy avec nos transformations. Cela semble indiqué qu'elles pourraient améliorer nos résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion des expérimentations: \n",
    "- Nous avons testé 3 modèles: un LetNet qui correspond à des CNN, un FeedForward classqiue  et une architecture type Resnet.  le Resnet et le CNN donnait des résultats similaires mais nous gardons LeNet car son architecture était plus simple \n",
    "- Concernant la Data Augmentation ils améliorent légérement le résultat du modèle. Nous pouvons le garder car il conserve un temps de calcul raisonnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul sur la métrique de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# predict the test set\n",
    "y_pred = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        y_pred.extend(pred.argmax(1).cpu().numpy())\n",
    "        y_true.extend(y.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the prediction, save as a txt format\n",
    "np.savetxt(\"prediction.txt\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prediction just in case\n",
    "y_load = np.loadtxt(\"prediction.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
