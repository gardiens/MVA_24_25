{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gardiens/MVA_24_25/blob/main/Delires/TP6/TP_texturenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d_fGA7-TCX7"
      },
      "source": [
        "## TP3 du cours DELIRES\n",
        "\n",
        "\n",
        "\n",
        "# SAMPLE GENERATOR PYRAMID 2D PERIODIC\n",
        "\n",
        "# Code for the texture synthesis method in:\n",
        "# Ulyanov et al. Texture Networks: Feed-forward Synthesis of Textures and Stylized Images\n",
        "# https://arxiv.org/abs/1603.03417\n",
        "# Generator architecture fixed to 6 scales!\n",
        "\n",
        "# Author: Jorge Gutierrez\n",
        "# Creation:  22 Jan 2019\n",
        "# Last modified: 22 Jan 2019\n",
        "# Based on https://github.com/leongatys/PytorchNeuralStyleTransfer\n",
        "# Modified by Saïd Ladjal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm6ZBqi9Lj-O"
      },
      "outputs": [],
      "source": [
        "#Download data\n",
        "!wget  https://perso.telecom-paris.fr/ladjal/TP3_DELIRES/images.tgz -O images.tgz\n",
        "!tar xvzf images.tgz\n",
        "!wget  https://perso.telecom-paris.fr/ladjal/TP3_DELIRES/texturenet_trained_models.tgz -O texturenet_trained_models.tgz\n",
        "!tar xvzf texturenet_trained_models.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plI6fL-HKatM"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from skimage import io as skio\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "#from helpers_said import *\n",
        "#from matplotlib import animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#%% quelques fonctions d'aide\n",
        "\n",
        "def egalhisto(r):\n",
        "    shapeo=r.shape\n",
        "    r2=r.reshape(-1)\n",
        "    idxs=np.argsort(r2)\n",
        "    r2[idxs]=np.arange(len(r2))/len(r2)\n",
        "    return np.float32(r2.reshape(shapeo))\n",
        "\n",
        "def redresse(t,trans=False,lam=0.5):\n",
        "    if lam>1-lam:\n",
        "        lam=1-lam\n",
        "    if (not trans) or (lam<0.01):\n",
        "        return t\n",
        "    else:\n",
        "        c=1/2*(lam)/(1-lam)\n",
        "        mask0=t>1/2\n",
        "        t[mask0]=1-t[mask0]\n",
        "        mask=(t<=lam)\n",
        "        t[mask]=1/2*(1/(1-lam))*(1/lam)*t[mask]**2\n",
        "        mask=(t>lam)*(t<1/2)\n",
        "        t[mask]=c+(1/(1-lam))*(t[mask]-lam)\n",
        "        t[mask0]=1-t[mask0]\n",
        "        return t\n",
        "def revcolors(im):\n",
        "    im2=np.zeros(im.shape,im.dtype)\n",
        "    for k in range(3):\n",
        "        im2[:,:,k]=im[:,:,2-k]\n",
        "    return im2\n",
        "\n",
        "def creeanimation(images, outimg=None, fps=5, size=None,\n",
        "               is_color=True, formatv=\"MP42\",nom='demo.avi',retour=False):\n",
        "    \"\"\"\n",
        "    Create a video from a list of images.\n",
        "\n",
        "    @param      outvid      output video\n",
        "    @param      images      list of images to use in the video\n",
        "    @param      fps         frame per second\n",
        "    @param      size        size of each frame\n",
        "    @param      is_color    color\n",
        "    @param      format      see http://www.fourcc.org/codecs.php\n",
        "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
        "\n",
        "    The function relies on http://opencv-python-tutroals.readthedocs.org/en/latest/.\n",
        "    By default, the video will have the size of the first image.\n",
        "    It will resize every image to this size before adding them to the video.\n",
        "    \"\"\"\n",
        "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "    fourcc = VideoWriter_fourcc(*formatv)\n",
        "    vid = None\n",
        "    size=images.shape[2],images.shape[1]\n",
        "    for  k in range(images.shape[0]):\n",
        "        img=revcolors(np.uint8(255*images[k]))\n",
        "        if vid is None:\n",
        "            vid = VideoWriter(nom, fourcc, float(fps), size, is_color)\n",
        "\n",
        "\n",
        "\n",
        "        vid.write(img)\n",
        "    if retour:\n",
        "        for k in range(images.shape[0]):\n",
        "            img=revcolors(np.uint8(255*images[images.shape[0]-k-1]))\n",
        "            vid.write(img)\n",
        "    vid.release()\n",
        "    return vid\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoKpnxNZKatQ"
      },
      "outputs": [],
      "source": [
        "#%% Network definition\n",
        "\n",
        "#generator's convolutional blocks 2D\n",
        "class Conv_block2D(nn.Module):\n",
        "    def __init__(self, n_ch_in, n_ch_out, m=0.1):\n",
        "        super(Conv_block2D, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(n_ch_in, n_ch_out, 3, padding=0, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(n_ch_out, momentum=m)\n",
        "        self.conv2 = nn.Conv2d(n_ch_out, n_ch_out, 3, padding=0, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(n_ch_out, momentum=m)\n",
        "        self.conv3 = nn.Conv2d(n_ch_out, n_ch_out, 1, padding=0, bias=True)\n",
        "        self.bn3 = nn.BatchNorm2d(n_ch_out, momentum=m)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat((x[:,:,-1,:].unsqueeze(2),x,x[:,:,0,:].unsqueeze(2)),2)\n",
        "        x = torch.cat((x[:,:,:,-1].unsqueeze(3),x,x[:,:,:,0].unsqueeze(3)),3)\n",
        "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.cat((x[:,:,-1,:].unsqueeze(2),x,x[:,:,0,:].unsqueeze(2)),2)\n",
        "        x = torch.cat((x[:,:,:,-1].unsqueeze(3),x,x[:,:,:,0].unsqueeze(3)),3)\n",
        "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
        "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
        "        return x\n",
        "\n",
        "#Up-sampling + batch normalization block\n",
        "class Up_Bn2D(nn.Module):\n",
        "    def __init__(self, n_ch):\n",
        "        super(Up_Bn2D, self).__init__()\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.bn = nn.BatchNorm2d(n_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(self.up(x))\n",
        "        return x\n",
        "\n",
        "class Pyramid2D(nn.Module):\n",
        "    def __init__(self, ch_in=3, ch_step=8):\n",
        "        super(Pyramid2D, self).__init__()\n",
        "\n",
        "        self.cb1_1 = Conv_block2D(ch_in,ch_step)\n",
        "        self.up1 = Up_Bn2D(ch_step)\n",
        "\n",
        "        self.cb2_1 = Conv_block2D(ch_in,ch_step)\n",
        "        self.cb2_2 = Conv_block2D(2*ch_step,2*ch_step)\n",
        "        self.up2 = Up_Bn2D(2*ch_step)\n",
        "\n",
        "        self.cb3_1 = Conv_block2D(ch_in,ch_step)\n",
        "        self.cb3_2 = Conv_block2D(3*ch_step,3*ch_step)\n",
        "        self.up3 = Up_Bn2D(3*ch_step)\n",
        "\n",
        "        self.cb4_1 = Conv_block2D(ch_in,ch_step)\n",
        "        self.cb4_2 = Conv_block2D(4*ch_step,4*ch_step)\n",
        "        self.up4 = Up_Bn2D(4*ch_step)\n",
        "\n",
        "        self.cb5_1 = Conv_block2D(ch_in,ch_step)\n",
        "        self.cb5_2 = Conv_block2D(5*ch_step,5*ch_step)\n",
        "        self.up5 = Up_Bn2D(5*ch_step)\n",
        "\n",
        "        self.cb6_1 = Conv_block2D(ch_in,ch_step)\n",
        "        self.cb6_2 = Conv_block2D(6*ch_step,6*ch_step)\n",
        "        self.last_conv = nn.Conv2d(6*ch_step, 3, 1, padding=0, bias=True)\n",
        "\n",
        "    def forward(self, z):\n",
        "\n",
        "        y = self.cb1_1(z[5])\n",
        "        y = self.up1(y)\n",
        "        y = torch.cat((y,self.cb2_1(z[4])),1)\n",
        "        y = self.cb2_2(y)\n",
        "        y = self.up2(y)\n",
        "        y = torch.cat((y,self.cb3_1(z[3])),1)\n",
        "        y = self.cb3_2(y)\n",
        "        y = self.up3(y)\n",
        "        y = torch.cat((y,self.cb4_1(z[2])),1)\n",
        "        y = self.cb4_2(y)\n",
        "        y = self.up4(y)\n",
        "        y = torch.cat((y,self.cb5_1(z[1])),1)\n",
        "        y = self.cb5_2(y)\n",
        "        y = self.up5(y)\n",
        "        y = torch.cat((y,self.cb6_1(z[0])),1)\n",
        "        y = self.cb6_2(y)\n",
        "        y = self.last_conv(y)\n",
        "        return y\n",
        "\n",
        "# post processing for images\n",
        "postpa = transforms.Compose([\n",
        "        transforms.Lambda(lambda x: x.mul_(1./255)),\n",
        "        #add imagenet mean\n",
        "        transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961],\n",
        "                            std=[1,1,1]),\n",
        "        #turn to RGB\n",
        "        transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]),\n",
        "        ])\n",
        "\n",
        "postpb = transforms.Compose([transforms.ToPILImage()])\n",
        "def postp(tensor): # to clip results in the range [0,1]\n",
        "    t = postpa(tensor)\n",
        "    t[t>1] = 1\n",
        "    t[t<0] = 0\n",
        "\n",
        "    return t.numpy().transpose((1,2,0)) # CANAUX A LA FIN (torch les met au debut)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN3WzvIuKatT"
      },
      "outputs": [],
      "source": [
        "#%% DEFINIR UNE IMAGE EXEMPLE ET CHARGER LE RESEAU PRE-ENTRAINE\n",
        "## CHOOSE your texture to replicate\n",
        "#image_exemple='BrickRound0122_1_seamless_S'\n",
        "#image_exemple='BubbleMarbel'\n",
        "#image_exemple='CRW_3241_1024'\n",
        "#image_exemple='CRW_3444_1024'\n",
        "#image_exemple='Pierzga_2006_1024'\n",
        "#image_exemple='Scrapyard0093_1_seamless_S'\n",
        "#image_exemple='TexturesCom_BrickSmallBrown0473_1_M_1024'\n",
        "#image_exemple='TexturesCom_FloorsCheckerboard0046_4_seamless_Projet d'achat: On a plusieS_1024'\n",
        "#image_exemple='TexturesCom_TilesOrnate0085_1_seamless_S'\n",
        "#image_exemple='TexturesCom_TilesOrnate0158_1_seamless_S'\n",
        "image_exemple='bubble_1024'\n",
        "#image_exemple='fabric_white_blue_1024'\n",
        "#image_exemple='glass_1024'\n",
        "#image_exemple='lego_1024'\n",
        "#image_exemple='marbre_1024'\n",
        "#image_exemple='metal_ground_1024'\n",
        "#image_exemple='rouille_1024'\n",
        "\n",
        "### SUR MON MAC\n",
        "models_folder='Texturenet_Trained_models/'\n",
        "## PC LINUX\n",
        "#models_folder='/home/said/CODE/TP3_MVA_DELIRES/Texturenet_Trained_models/'\n",
        "### sur la DSI\n",
        "#models_folder='/cal/homes/ladjal/TP_CNN/model/Texturenet'\n",
        "model_folder=models_folder+image_exemple\n",
        "\n",
        "image_originale_file='./images/'+image_exemple+'.png'\n",
        "image_originale=np.float32(skio.imread(image_originale_file))\n",
        "#viewimage(image_originale)\n",
        "\n",
        "#load model  (IF YOU DO NOT HAVE GPU change 'cuda:0' to 'cpu')\n",
        "generator = Pyramid2D(ch_step=8)\n",
        "generator.load_state_dict(torch.load(model_folder + '/params.pytorch',map_location='cuda:0'))\n",
        "#generator.cuda()\n",
        "\n",
        "generator.eval()\n",
        "for param in generator.parameters():\n",
        "    param.requires_grad = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQLir4AeKatU"
      },
      "outputs": [],
      "source": [
        "#%% GENERATION of a single shots\n",
        "#draw sample\n",
        "n_input_ch = 3\n",
        "sample_size = 1024\n",
        "n_samples = 3\n",
        "\n",
        "sz = [sample_size /1,sample_size /2,sample_size /4,sample_size /8,sample_size /16,sample_size /32]\n",
        "zk = [torch.rand(n_samples,n_input_ch,int(szk),int(szk)) for szk in sz]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "z_samples = [Variable(z) for z in zk ]\n",
        "sample = generator(z_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKBO0dUrKatW"
      },
      "outputs": [],
      "source": [
        "#%% Extract from pytorch to numpy arrays\n",
        "out_imgs=np.zeros([n_samples,sample_size,sample_size,n_input_ch])\n",
        "for n in range(n_samples):\n",
        "    single_sample = sample[n,:,:,:]\n",
        "    out_imgs[n,:,:,:] = postp(single_sample.data.squeeze())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM0wv5FeKatW"
      },
      "outputs": [],
      "source": [
        "#%% Visualisation par GIMP (In Colab only one image will be shown)\n",
        "for n in range(n_samples):\n",
        "    plt.figure()\n",
        "    plt.imshow(out_imgs[n])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRG2MLYQKatX"
      },
      "outputs": [],
      "source": [
        "#%%\n",
        "#% GENERATION de VIDEOS\n",
        "\n",
        "#draw sample\n",
        "n_input_ch = 3\n",
        "sample_size = 256\n",
        "n_samples = 30\n",
        "\n",
        "sz = [sample_size /1,sample_size /2,sample_size /4,sample_size /8,sample_size /16,sample_size /32]\n",
        "zk = [torch.rand(n_samples,n_input_ch,int(szk),int(szk)) for szk in sz]\n",
        "\n",
        "\n",
        "for k in zk:\n",
        "    for t in range(1,n_samples-1):\n",
        "        lam=t/(n_samples-1)\n",
        "        k[t,:,:,:]= lam*k[-1]+(1-lam)*k[0]\n",
        "        # ???? il faut interpoler entre  k[0,:,:,:](premier bruit)  et k[-1,:,:,:] (dernier bruit)\n",
        "\n",
        "\n",
        "# pour reduire la consommation memoire on fait par packets de N images\n",
        "Nimages=10 # pour simplifier le code prendre Nimages diviseur de n_samples\n",
        "assert n_samples % Nimages ==0 , \"Nimages pas diviseur de n_samples\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwjCy4m2KatX"
      },
      "outputs": [],
      "source": [
        "#%% boucle de génération\n",
        "ideb=0\n",
        "out_imgs=np.zeros([n_samples,sample_size,sample_size,n_input_ch])\n",
        "while ideb<n_samples:\n",
        "    print(ideb)\n",
        "    z_samples = [Variable(z[ideb:ideb+Nimages]) for z in zk ]\n",
        "    #Lancer le reseau\n",
        "    sample = generator(z_samples)\n",
        "    #% extraction de torch vers un tableau numpy\n",
        "\n",
        "    for n in range(Nimages):\n",
        "        single_sample = sample[n,:,:,:]\n",
        "        out_imgs[ideb+n,:,:,:] = postp(single_sample.data.squeeze()).copy()\n",
        "    ideb+=Nimages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rs3QLmBLKatY"
      },
      "outputs": [],
      "source": [
        "#%% Visualisation par GIMP   # Ne fonctionne pas dans colab\n",
        "for n in range(n_samples):\n",
        "    plt.figure()\n",
        "    plt.imshow(out_imgs[n])\n",
        "    #input()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPKYNs2BKatY"
      },
      "outputs": [],
      "source": [
        "#%% CREER UNE VIDEO (Attention au nom de fichier! il sera écrasé)\n",
        "creeanimation(out_imgs,nom='demo4.avi',fps=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjX3ch9CKatY"
      },
      "outputs": [],
      "source": [
        "#%%%\n",
        "#  VIDEO avec deplacement\n",
        "\n",
        "#draw sample\n",
        "n_input_ch = 3\n",
        "sample_size = 512\n",
        "n_samples = 60\n",
        "\n",
        "#sz = [sample_size /1,sample_size /2,sample_size /4,sample_size /8,sample_size /16,sample_size /32]\n",
        "zk = [torch.rand(n_samples,n_input_ch,sample_size,sample_size)]\n",
        "\n",
        "\n",
        "k=zk[0]\n",
        "\n",
        "for t in range(1,n_samples-1):\n",
        "    lam=t/(n_samples-1)\n",
        "    posx=int(lam*400)\n",
        "    posy=int(lam*400)\n",
        "    k[t,:,:,:]=k[-1,:,:,:]\n",
        "    k[t,:,posy:posy+100,posx:posx+100]=k[0,:,:100,:100]\n",
        "\n",
        "zk=[zk[0]]\n",
        "for k in range(1,6):\n",
        "    pui=2**k\n",
        "    zk.append(zk[0][:,:,::pui,::pui])\n",
        "\n",
        "# pour reduire la consommation memoire on fait par packets de N images\n",
        "Nimages=10 # pour simplifier le code prendre Nimages diviseur de n_samples\n",
        "assert n_samples % Nimages ==0 , \"Nimages pas diviseur de n_samples\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JclKPaBnKatZ"
      },
      "outputs": [],
      "source": [
        "#%%\n",
        "ideb=0\n",
        "out_imgs=np.zeros([n_samples,sample_size,sample_size,n_input_ch])\n",
        "while ideb<n_samples:\n",
        "    print(ideb)\n",
        "    z_samples = [Variable(z[ideb:ideb+Nimages]) for z in zk ]\n",
        "    #Lancer le reseau\n",
        "    sample = generator(z_samples)\n",
        "    #% extraction de torch vers un tableau numpy\n",
        "\n",
        "    for n in range(Nimages):\n",
        "        single_sample = sample[n,:,:,:].cpu()\n",
        "        out_imgs[ideb+n,:,:,:] = postp(single_sample.data.squeeze())\n",
        "    ideb+=Nimages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9_jmvqsKata"
      },
      "outputs": [],
      "source": [
        "#%%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2KF0UEnKata"
      },
      "outputs": [],
      "source": [
        "#%% Visualisation par GIMP   # utiliser plt.imshow dans colab...\n",
        "for n in range(n_samples):\n",
        "    viewimage(out_imgs[n],titre=('echan_%d'%n))\n",
        "    #input()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12e7Q6sbKata"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}