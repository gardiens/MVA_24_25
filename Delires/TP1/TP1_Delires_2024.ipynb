{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efsYlgNueSDB",
    "outputId": "afc03176-80a9-470c-9a3c-e114bf14affb"
   },
   "outputs": [],
   "source": [
    "# Downloading data\n",
    "!wget  -O donnees_TP1_Delires_2024.tgz https://perso.telecom-paris.fr/ladjal/donnees_TP1_Delires_2024.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAu-tEnLekNF",
    "outputId": "b937ed79-8280-46e5-f038-56a1283aab98"
   },
   "outputs": [],
   "source": [
    "# unpack the data.\n",
    "# everything will be in content/Donnees_TPA_Delires_2024\n",
    "\n",
    "!tar xvzf donnees_TP1_Delires_2024.tgz\n",
    "zebres = \"Donnees_TP1_Delires_2024/Data/FFDNET/zebres.png\"\n",
    "testimage1 = \"/content/Donnees_TP1_Delires_2024/Data/SR/testimages/img_043.png\"\n",
    "testimage2 = \"/content/Donnees_TP1_Delires_2024/Data/SR/testimages/119082.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGG-jV1tFDju"
   },
   "source": [
    "Through all the practical work images are to be considered as float images of values in the range [0..1]. So you will some times see some /255.\n",
    "\n",
    " This scaling is important when a network has been trained with images in a specfic intervalle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwTk7TF64eoY"
   },
   "outputs": [],
   "source": [
    "# Common functions Usage for every part of the practical work\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as pil_image\n",
    "\n",
    "\n",
    "def viewimage(im, normalize=True, titre=\"\", displayfilename=False):\n",
    "    imin = im.copy().astype(np.float32)\n",
    "    if normalize:\n",
    "        imin -= imin.min()\n",
    "        if imin.max() > 0:\n",
    "            imin /= imin.max()\n",
    "    else:\n",
    "        imin = imin.clip(0, 255) / 255\n",
    "\n",
    "    imin = (imin * 255).astype(np.uint8)\n",
    "    filename = tempfile.mktemp(titre + \".png\")\n",
    "    if displayfilename:\n",
    "        print(filename)\n",
    "    plt.imsave(filename, imin, cmap=\"gray\")\n",
    "    IPython.display.display(IPython.display.Image(filename))\n",
    "\n",
    "\n",
    "# La fonction viewimage_color est la même que viewimage. Ca a l'air de marcher\n",
    "# USE ONLY viewimage\n",
    "def viewimage_color(im, normalize=True, titre=\"\", displayfilename=False):\n",
    "    imin = im.copy().astype(np.float32)\n",
    "    if normalize:\n",
    "        imin -= imin.min()\n",
    "        if imin.max() > 0:\n",
    "            imin /= imin.max()\n",
    "    else:\n",
    "        imin = imin.clip(0, 255) / 255\n",
    "\n",
    "    imin = (imin * 255).astype(np.uint8)\n",
    "    filename = tempfile.mktemp(titre + \".png\")\n",
    "    if displayfilename:\n",
    "        print(filename)\n",
    "    plt.imsave(filename, imin, cmap=\"gray\")\n",
    "    IPython.display.display(IPython.display.Image(filename))\n",
    "\n",
    "\n",
    "def read_image_from_disk(filename):\n",
    "    \"\"\"reads an image from the disk.\"\"\"\n",
    "    image = pil_image.open(filename).convert(\"RGB\")\n",
    "    imgnp = np.array(image).astype(np.float32)\n",
    "    # (h,w,c)=imgnp.shape\n",
    "    # imgnp=imgnp[:(h//scale)*scale,:(w//scale)*scale,:]\n",
    "    return imgnp / 255.0\n",
    "\n",
    "\n",
    "# NOTE FOR Denoising, we only use gray images for simplicity.\n",
    "def read_gray_image(filename):\n",
    "    img = read_image_from_disk(filename)\n",
    "    return img.sum(axis=2) / 3\n",
    "\n",
    "\n",
    "def norm2(X):\n",
    "    return ((X**2).sum()) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhYt6ysFRsn3"
   },
   "source": [
    "# DCT denoiser\n",
    "The DCT denoiser is described in the model in the next cell and a cell that uses it is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjYG10QHRqqA",
    "outputId": "5dd30b61-7880-476b-9400-a8cf8e83a691"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import fftpack as scifft\n",
    "\n",
    "\n",
    "def To_Tensor(X):\n",
    "    return torch.from_numpy(X.copy()).to(device)\n",
    "\n",
    "\n",
    "def From_Tensor(X):\n",
    "    return X.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")  # POSSIBLE BUG choose other that cuda:0\n",
    "print(device)\n",
    "\n",
    "\n",
    "def idct2_blocks(N):\n",
    "    X = np.zeros((N, N, N * N), dtype=np.float32)\n",
    "    for k in range(N):\n",
    "        for l in range(N):\n",
    "            X[k, l, k * N + l] = 1\n",
    "    O = scifft.idct(X, type=2, axis=0, norm=\"ortho\")\n",
    "    O = scifft.idct(O, type=2, axis=1, norm=\"ortho\")\n",
    "    return O\n",
    "\n",
    "\n",
    "class DCT_denoiser(nn.Module):  # N must be odd\n",
    "    def __init__(self, N=7):\n",
    "        vects = idct2_blocks(N)\n",
    "        W1 = To_Tensor(np.expand_dims(vects, 0)).to(device).permute(3, 0, 1, 2)\n",
    "        W2 = (\n",
    "            To_Tensor(np.expand_dims(np.fliplr(np.flipud(vects)), 0))\n",
    "            .to(device)\n",
    "            .permute(0, 3, 1, 2)\n",
    "        )\n",
    "        super(DCT_denoiser, self).__init__()\n",
    "        # First convolutional layer with 7x7 kernel and 49 features\n",
    "        self.conv1 = nn.Conv2d(1, N * N, kernel_size=N, stride=1, padding=\"valid\")\n",
    "        self.conv1.weight.data = W1\n",
    "        self.conv1.bias.data = torch.zeros(N * N).to(device)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # Second convolutional layer with 1x1 kernel and 1 feature\n",
    "        self.conv2 = nn.Conv2d(N * N, 1, kernel_size=7, stride=1, padding=(N - 1))\n",
    "        self.conv2.weight.data = W2\n",
    "        self.conv2.bias.data = torch.zeros(1).to(device)\n",
    "        # self.seuil=To_Tensor(s*np.ones((1),dtype=np.float32))\n",
    "        self.N = N\n",
    "\n",
    "    def get_mask(\n",
    "        self, sh\n",
    "    ):  # renvoie le mask par lequel diviser la sortie pour obtenir la vraie restauration\n",
    "        N = self.N\n",
    "        sigy = np.concatenate(\n",
    "            (\n",
    "                np.arange(1, N),\n",
    "                N * np.ones((sh[0] - 2 * N + 2,)),\n",
    "                np.arange(N - 1, 0, -1),\n",
    "            )\n",
    "        ).reshape((sh[0], 1))\n",
    "        sigx = np.concatenate(\n",
    "            (\n",
    "                np.arange(1, N),\n",
    "                N * np.ones((sh[1] - 2 * N + 2,)),\n",
    "                np.arange(N - 1, 0, -1),\n",
    "            )\n",
    "        ).reshape((1, sh[1]))\n",
    "        return sigx * sigy\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        # Input x should have shape (batch_size, channels, height, width)\n",
    "        x = self.conv1(x)\n",
    "        seuil = To_Tensor(s * np.ones((1), dtype=np.float32))\n",
    "        x = torch.where(x.abs() < seuil, torch.tensor(0.0), x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def apply_denoiser(model, img, noiselevel, ratio):\n",
    "    \"\"\"takes an image, adds noise to it (noislevel), denoise it with the model\n",
    "    and returne (clean, noisy, denoised)\"\"\"\n",
    "    noiselevel /= 255\n",
    "    img = img.copy().astype(np.float32)\n",
    "    noisy = img + np.random.randn(*img.shape) * noiselevel\n",
    "    noisy = noisy.astype(np.float32)\n",
    "    noisynet = np.expand_dims(noisy, 0)\n",
    "    noisynet = np.expand_dims(noisynet, 0)\n",
    "    noisynet = To_Tensor(noisynet)\n",
    "    out = model(\n",
    "        noisynet, ratio * noiselevel\n",
    "    )  # with mutiply noislevel by ratio to obtain te threshold\n",
    "    out = From_Tensor(out)\n",
    "    out = out[0, 0]\n",
    "    mask = model.get_mask(img.shape)\n",
    "    return (img, noisy, out / mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "id": "5CW6ZixxWwqX",
    "outputId": "31d72a35-6599-467b-b880-03a050323c47"
   },
   "outputs": [],
   "source": [
    "# TEST DCT_denoiser.\n",
    "N = 7\n",
    "test = zebres\n",
    "img = read_gray_image(test)\n",
    "my_denoiser = DCT_denoiser(N=N)\n",
    "sigma = 0.25\n",
    "# apply a normal noise\n",
    "img = img.copy().astype(np.float32)\n",
    "noise = np.random.normal(0, sigma / 255, img.shape)\n",
    "noisy = img\n",
    "viewimage(noisy)\n",
    "(img_out, noisy, out) = apply_denoiser(my_denoiser, noisy, sigma, 25)\n",
    "viewimage(noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "id": "u6kx98Djfgc4",
    "outputId": "925c232d-6a0b-4e64-883e-7ad04ba5f7cc"
   },
   "outputs": [],
   "source": [
    "nl = 25\n",
    "ratios = np.arange(0, 10, 0.5)\n",
    "errs = 0 * ratios\n",
    "k = 0\n",
    "for ratio in ratios:\n",
    "    (CL, NOI, DENOI) = apply_denoiser(my_denoiser, noisy, nl, ratio)\n",
    "    errs[k] = norm2(CL - DENOI)\n",
    "    print(\"for ratio=\", ratio, \" we have residual error = \", norm2(CL - DENOI))\n",
    "    k += 1\n",
    "\n",
    "plt.plot(ratios, errs)\n",
    "plt.xlabel(\"ratio\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.title(\"error with respect to different threshold value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yx5TFK709sUN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "id": "bc3CEO4sPjWy",
    "outputId": "34621960-de50-476d-9212-929a70adf253"
   },
   "outputs": [],
   "source": [
    "# apply a normal noise\n",
    "img = img.copy().astype(np.float32)\n",
    "noise = np.random.normal(0, sigma / 255, img.shape)\n",
    "noisy = img + noisy\n",
    "viewimage(img)\n",
    "(img_out, noisy, out) = apply_denoiser(my_denoiser, noise, nl, 1)\n",
    "viewimage(noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4IHbvjgwu9V"
   },
   "source": [
    "The Next sections are dedicated the the study of a more complex denoising network: FFDNET\n",
    "The implementation is taken from\n",
    "https://www.ipol.im/pub/art/2019/231/\n",
    "\n",
    "The denoising is all done in grayscale, but you can easily use the provided data to have a color denoiser.\n",
    "This is done to simplify comparisons and the code of the practical work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42WKqvU9xFXT",
    "outputId": "8edc3574-b9b2-4549-a8ef-bf9a756d03e4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/content/Donnees_TP1_Delires_2024/Code/FFDNET\")\n",
    "from models import FFDNet\n",
    "from utils import (\n",
    "    batch_psnr,\n",
    "    normalize,\n",
    "    init_logger_ipol,\n",
    "    variable_to_cv2_image,\n",
    "    remove_dataparallel_wrapper,\n",
    "    is_rgb,\n",
    ")\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")  # POSSIBLE BUG choose other that cuda:0\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mYZpA4ucza4P",
    "outputId": "5a5f85d2-8e4e-44f6-ca5d-ef32e8c98661"
   },
   "outputs": [],
   "source": [
    "# CREATION OF THE DENOINSING NETWORK\n",
    "\n",
    "\n",
    "def create_ffdnet():\n",
    "    model_fn = \"/content/Donnees_TP1_Delires_2024/Data/FFDNET/Pretrained/net_gray.pth\"\n",
    "\n",
    "    in_ch = 1  # this is to be changed for rgb images\n",
    "    net = FFDNet(num_input_channels=in_ch)\n",
    "\n",
    "    if device == torch.device(\"cuda:0\"):\n",
    "        state_dict = torch.load(model_fn)\n",
    "        device_ids = [0]\n",
    "        model = nn.DataParallel(net, device_ids=device_ids).cuda()\n",
    "        # state_dict=state_dict.to(device)\n",
    "    else:\n",
    "        state_dict = torch.load(model_fn, map_location=\"cpu\")\n",
    "        # CPU mode: remove the DataParallel wrapper\n",
    "        state_dict = remove_dataparallel_wrapper(state_dict)\n",
    "        model = net\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def To_Tensor(X):\n",
    "    return torch.from_numpy(X).to(device)\n",
    "\n",
    "\n",
    "def From_Tensor(X):\n",
    "    return X.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def Apply_FFDNET_model(model, im, noiselevel=25):\n",
    "    \"\"\"Given a perfect image, we add noise and denoise with ffdnet.\n",
    "    noiselevel is given in the range 0-255\n",
    "    Returns the denoised image.\n",
    "    Remember the program adds noise to the image you gave.\n",
    "    If you do not want any added noise run the program with noiselevel=0\n",
    "    \"\"\"\n",
    "    # the input must be of even size\n",
    "    (dy, dx) = im.shape\n",
    "    ndx = dx\n",
    "    ndy = dy\n",
    "    if dy % 2 == 1:\n",
    "        ndy = dy + 1\n",
    "    if dx % 2 == 1:\n",
    "        ndx = dx + 1\n",
    "    imnew = np.zeros((ndy, ndx), dtype=np.float32)\n",
    "    imnew[:dy, :dx] = im\n",
    "    imnew[:, -1] = imnew[:, dx - 1]\n",
    "    imnew[-1, :] = imnew[dy - 1, :]\n",
    "    noiselevel /= 255.0\n",
    "    noiselevel = np.ones((1), dtype=np.float32) * noiselevel\n",
    "    noisyimage = imnew + np.random.randn(*imnew.shape) * noiselevel\n",
    "    noisyimage = np.expand_dims(noisyimage, 0)\n",
    "    noisyimage = np.expand_dims(noisyimage, 0)\n",
    "    with torch.no_grad():\n",
    "        outim = model(\n",
    "            To_Tensor(noisyimage).to(device), To_Tensor(noiselevel).to(device)\n",
    "        )\n",
    "\n",
    "    outim = From_Tensor(outim)[0, 0]\n",
    "    outim = (\n",
    "        noisyimage[0, 0] - outim\n",
    "    )  # the model estimates the noise, not the clean image\n",
    "    outim = outim[:dy, :dx]  # in case even size constraint had to be enforced\n",
    "    return outim\n",
    "\n",
    "\n",
    "ffdnet = create_ffdnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "id": "7NwFwy5s5M12",
    "outputId": "e5a56714-edd4-4b46-8bfb-cdb2a33f7f3d"
   },
   "outputs": [],
   "source": [
    "# Test FFDNET\n",
    "testimage = \"/content/Donnees_TP1_Delires_2024/Data/FFDNET/zebres.png\"\n",
    "img = read_gray_image(testimage)\n",
    "sigma = 25\n",
    "noiselevel = sigma / 255\n",
    "\n",
    "# apply a normal noise\n",
    "img = img.copy().astype(np.float32)\n",
    "noisy = img + np.random.randn(*img.shape) * noiselevel\n",
    "noisy = noisy.astype(np.float32)\n",
    "viewimage(noisy)\n",
    "toto = Apply_FFDNET_model(ffdnet, noisy, noiselevel=25)\n",
    "viewimage(toto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5x8psxtu7uc"
   },
   "source": [
    "The Next sections are dedicated to the study of a super-resolution network.\n",
    "The corresponding paper is:\n",
    "Zhang, Yulun, et al. \"Residual dense network for image super-resolution.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n",
    "\n",
    "The implementation is taken from:\n",
    "https://github.com/yjn870/RDN-pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KjgJljy23BC",
    "outputId": "6235d6f3-fd27-40f8-eae8-b1167af3ed27"
   },
   "outputs": [],
   "source": [
    "!cd /content/Donnees_TP1_Delires_2024/Code/SR\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgAsFV9_erPJ"
   },
   "outputs": [],
   "source": [
    "# RESTART THE INTERPRETER AND EXCUTE THE data download again...\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import PIL.Image as pil_image\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/content/Donnees_TP1_Delires_2024/Code/SR\")\n",
    "# cd /content/Donnees_TP1_Delires_2024/Code/SR\n",
    "from models import RDN\n",
    "from utils import convert_rgb_to_y, denormalize, calc_psnr\n",
    "\n",
    "import tempfile\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72y8UtdlixIA",
    "outputId": "ea7e7aea-ddcd-47a6-8985-fa1bf5859b19"
   },
   "outputs": [],
   "source": [
    "# tests the availability of a GPU and sets the global variable device\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\n",
    "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")  # POSSIBLE BUG choose other that cuda:0\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPnCfqz2fs1B"
   },
   "outputs": [],
   "source": [
    "def viewimage(im, normalize=True, titre=\"\", displayfilename=False):\n",
    "    imin = im.copy().astype(np.float32)\n",
    "    if normalize:\n",
    "        imin -= imin.min()\n",
    "        if imin.max() > 0:\n",
    "            imin /= imin.max()\n",
    "    else:\n",
    "        imin = imin.clip(0, 255) / 255\n",
    "\n",
    "    imin = (imin * 255).astype(np.uint8)\n",
    "    filename = tempfile.mktemp(titre + \".png\")\n",
    "    if displayfilename:\n",
    "        print(filename)\n",
    "    plt.imsave(filename, imin, cmap=\"gray\")\n",
    "    IPython.display.display(IPython.display.Image(filename))\n",
    "\n",
    "\n",
    "# La fonction viewimage_color est la même que viewimage. Ca a l'air de marcher\n",
    "# USE ONLY viewimage\n",
    "def viewimage_color(im, normalize=True, titre=\"\", displayfilename=False):\n",
    "    imin = im.copy().astype(np.float32)\n",
    "    if normalize:\n",
    "        imin -= imin.min()\n",
    "        if imin.max() > 0:\n",
    "            imin /= imin.max()\n",
    "    else:\n",
    "        imin = imin.clip(0, 255) / 255\n",
    "\n",
    "    imin = (imin * 255).astype(np.uint8)\n",
    "    filename = tempfile.mktemp(titre + \".png\")\n",
    "    if displayfilename:\n",
    "        print(filename)\n",
    "    plt.imsave(filename, imin, cmap=\"gray\")\n",
    "    IPython.display.display(IPython.display.Image(filename))\n",
    "\n",
    "\n",
    "def read_image_from_disk(filename):\n",
    "    \"\"\"reads an image from the disk.\"\"\"\n",
    "    image = pil_image.open(filename).convert(\"RGB\")\n",
    "    imgnp = np.array(image).astype(np.float32)\n",
    "    # (h,w,c)=imgnp.shape\n",
    "    # imgnp=imgnp[:(h//scale)*scale,:(w//scale)*scale,:]\n",
    "    return imgnp / 255.0\n",
    "\n",
    "\n",
    "def apply_model(model, input, scale=2, subsampling_type=\"bicubic\"):\n",
    "    \"\"\"prepares an image to be scaled up. Returns the  (O,S,ZB,ZM)\n",
    "    input is a [h,w,c] numpy array\n",
    "    O= original image with size multpile of scale so that zoomed images can be compared\n",
    "      to the original image\n",
    "    S= subsampled image subsample(O)\n",
    "    ZB= zoomed image with bicubic (zoom_bicubic(subsample(O)))\n",
    "    ZM= zoomed image with model (zoom_model(subsample(O)))\n",
    "    subsample_type: if \"bicubic\" then the subsampling of the original paper\n",
    "                   is kept. if \"decim\" one pixel each scal pixels is kept.\n",
    "    \"\"\"\n",
    "\n",
    "    (h, w, c) = input.shape\n",
    "    input = input[: (h // scale) * scale, : (w // scale) * scale, :]\n",
    "    if subsampling_type == \"decim\":\n",
    "        S = input[::scale, ::scale, :]\n",
    "    if subsampling_type == \"locmean\":\n",
    "        S = input[::scale, ::scale, :] * 0\n",
    "        for k in range(scale):\n",
    "            for l in range(scale):\n",
    "                S += input[k::scale, l::scale, :]\n",
    "        S /= scale * scale\n",
    "    if subsampling_type == \"bicubic\":\n",
    "        # convert to PILLOW image\n",
    "        pilim = pil_image.fromarray((input * 255).astype(np.uint8))\n",
    "        # subsample using PIL BICUBIC\n",
    "        lr = pilim.resize(\n",
    "            (pilim.width // scale, pilim.height // scale), resample=pil_image.BICUBIC\n",
    "        )\n",
    "\n",
    "        # back to numpy image\n",
    "        S = np.array(lr).astype(np.float32) / 255\n",
    "    ZB = pil_image.fromarray((S * 255).astype(np.uint8))\n",
    "    ZB = ZB.resize((ZB.width * scale, ZB.height * scale), resample=pil_image.BICUBIC)\n",
    "    ZB = np.array(ZB).astype(np.float32) / 255\n",
    "    lrmodel = np.expand_dims(S.transpose([2, 0, 1]), 0)\n",
    "    lrmodel = torch.from_numpy(lrmodel).to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(lrmodel).squeeze(0)\n",
    "    ZM = preds.permute(1, 2, 0).cpu().numpy()\n",
    "    return (input, S, ZB, ZM)\n",
    "\n",
    "\n",
    "def RDN_model(scale=2):\n",
    "    \"\"\"This function retruns an RDN model that scales images by\n",
    "    a factor given as a parameter.\"\"\"\n",
    "\n",
    "    if scale == 2:\n",
    "        weightsfile = \"/content/Donnees_TP1_Delires_2024/Data/SR/Pretrained/rdn_x2.pth\"\n",
    "    if scale == 3:\n",
    "        weightsfile = \"/content/Donnees_TP1_Delires_2024/Data/SR/Pretrained/rdn_x3.pth\"\n",
    "    if scale == 4:\n",
    "        weightsfile = \"/content/Donnees_TP1_Delires_2024/Data/SR/Pretrained/rdn_x4.pth\"\n",
    "\n",
    "    model = RDN(\n",
    "        scale_factor=scale,\n",
    "        num_channels=3,\n",
    "        num_features=64,\n",
    "        growth_rate=64,\n",
    "        num_blocks=16,\n",
    "        num_layers=8,\n",
    "    ).to(device)\n",
    "    state_dict = model.state_dict()\n",
    "    for n, p in torch.load(\n",
    "        weightsfile, map_location=lambda storage, loc: storage\n",
    "    ).items():\n",
    "        if n in state_dict.keys():\n",
    "            state_dict[n].copy_(p)\n",
    "        else:\n",
    "            raise KeyError(n)\n",
    "\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 764
    },
    "id": "ibXVZ_e-j24u",
    "outputId": "fa66111f-934c-41ef-d2f4-1dfad805d3c8"
   },
   "outputs": [],
   "source": [
    "##BROUILLON\n",
    "sca = 2\n",
    "model2 = RDN_model(scale=sca)\n",
    "testimage1 = \"/content/Donnees_TP1_Delires_2024/Data/SR/testimages/img_043.png\"\n",
    "testimage2 = \"/content/Donnees_TP1_Delires_2024/Data/SR/testimages/119082.png\"\n",
    "img = read_image_from_disk(testimage1)\n",
    "(h, w, c) = img.shape\n",
    "print(h, w, c)\n",
    "viewimage(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0ddKCCrS5aSt",
    "outputId": "069f63ba-faea-499b-f5c3-cdc640d790fc"
   },
   "outputs": [],
   "source": [
    "(O, S, ZB, ZM) = apply_model(model2, img, scale=sca, subsampling_type=\"decim\")\n",
    "viewimage(O)\n",
    "viewimage(S)\n",
    "viewimage(ZB)\n",
    "viewimage(ZM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "C_kYA2vs8xbz",
    "outputId": "f43be115-6371-4451-bd36-7604e374f884"
   },
   "outputs": [],
   "source": [
    "viewimage(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0co6gGOn5z6k"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
